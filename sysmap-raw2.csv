Source;Year;Title;Keywords;Abstract
ACM;2010;Requirements Reflection: Requirements As Runtime Entities;reflection, requirements, runtime, self-adaptive systems;
ACM;2012;An Eclipse Modelling Framework Alternative to Meet the Models@Runtime Requirements;EMF, Model@Runtime, adaptation;
ACM;1998;Reconciling System Requirements and Runtime Behavior;Self-adapting systems, requirements monitoring, goal-driven requirements engineering,inconsistency management, obstacles, deviation analysis, system customization.;
ACM;2009;Monitoring Probabilistic Properties;performance, probabilistic properties, reliability, runtime monitoring, safety, security, web services;
ACM;2014;The Average Sensitivity of an Intersection of Half Spaces;learning theory, linear threshold function, noise sensitivity;
ACM;2013;Requirements Models for Design- and Runtime: A Position Paper;;
ACM;2006;Selective Code/Data Migration for Reducing Communication Energy in Embedded MpSoC Architectures;MPSoC, energy, migration;
ACM;2013;Exploiting Slicing and Patterns for RTSJ Immortal Memory Optimization;RTSJ, code slicing, design patterns, immortal memory, memory consumption, real-time Java, scoped memory;
ACM;2003;View Planning for BRDF Acquisition;;
ACM;2013;Accelerating Throughput-aware Runtime Mapping for Heterogeneous MPSoCs;Multiprocessor systems-on-chip, design-space exploration, embedded systems, energy consumption, multimedia applications, runtime mapping, synchronous data-flow graphs, throughput;
ACM;2015;Instruction-Cache Locking for Improving Embedded Systems Performance;Binary rewriting, cache locking, caches, embedded systems;
ACM;2014;NoC Contention Analysis Using a Branch-and-prune Algorithm;Many-core systems, network-on-chip, real-time systems, wormhole routing;
ACM;2011;Structuring Adaptive Replicated Systems with Design Patterns and Aspects;adaptive replication control, aspect oriented programming, design patterns;
ACM;1998;A Sequential Detailed Router for Huge Grid Graphs;Detailed Routing, Shortest Paths, Rip-up And Reroute;
ACM;2004;Kaleidoscope Configurations for Reflectance Measurement;BRDF, BTF, kaleidoscope, reflectance;
ACM;1998;Parametric Polymorphism for Java: A Reflective Solution;language design and implementation, persistence, reflection;
ACM;1998;Parametric Polymorphism for Java: A Reflective Solution;language design and implementation, persistence, reflection;
ACM;2006;Dynamic Allocation for Scratch-pad Memory Using Compile-time Decisions;Memory allocation, compiler, embedded systems, scratch pad, software caching, software-managed cache;
ACM;2003;Compiler-decided Dynamic Memory Allocation for Scratch-pad Based Embedded Systems;compiler, embedded systems, memory allocation, scratch-pad;
ACM;1997;Compiling Dynamic Mappings with Array Copies;;
ACM;1997;Compiling Dynamic Mappings with Array Copies;;
ACM;2008;Runtime Adaptation in a Service-oriented Component Model;adaptation, component replacement, migration, modularity, service-oriented architecture;
ACM;2015;Quantifying Timing-Based Information Flow in Cryptographic Hardware;;
ACM;2011;Bounded Decentralised Coordination over Multiple Objectives;coordination, distributed problem solving;
ACM;2010;Bridging Models and Systems at Runtime to Build Adaptive User Interfaces;adaptive user interfaces, executable models, model-based user interface development, model-driven engineering;
ACM;2007;Interactive Presentation: A Process Splitting Transformation for Kahn Process Networks;;
ACM;2008;Construction of Concrete Verification Models from C++;C++, dynamic memory allocation, equivalence checking, formal verification, pointers;
ACM;2006;Impact of Virtual Execution Environments on Processor Energy Consumption and Hardware Adaptation;energy efficiency, hardware adaptation, power dissipation;
ACM;1994;Correctness and Composition of Software Architectures;;
ACM;1994;Correctness and Composition of Software Architectures;;
ACM;2002;Towards a Synthesis of Dynamic Architecture Event Languages;acme, architecture change events, dynamic architectures, gauge infrastructure, probe infrastructure, software architectures, xADL, xArch;
ACM;2003;About the Requisites Analysis to the Interface Project: A Subjectivist Approach for the Information Systems;Intera&#231,,&#227,,o Humano-Computador, Semi&#243,,tica Organizacional, Sistemas de Informa&#231,,&#227,,o;
ACM;2005;The Application of Compile-time Reflection to Software Fault Tolerance Using Ada 95;Ada, atomic actions, backward error recovery, conversations, recovery blocks, reflection, software fault tolerance;
ACM;2011;Rigid Structures, Independent Units, Monitoring: Organizing Patterns in Frontline Firefighting;ethnography, firefighting, pattern research, safety-critical work, ubiquitous computing;
ACM;2003;A Propositional Policy Algebra for Access Control;Access control, policy algebra, policy composition, security policy;
ACM;2000;A Comparison of Three Programming Models for Adaptive Applications on the Origin2000;;
ACM;2005;Effective Adaptive Computing Environment Management via Dynamic Optimization;;
ACM;1999;Fast Simulation of Networks of Queues with Effective and Decoupling Bandwidths;asynchronous transfer mode, fast simulation, importance sampling, rare events;
ACM;2011;Task Scheduling for GPU Accelerated OLAP Systems;;
ACM;2006;Trunk Decomposition Based Global Routing Optimization;;
ACM;2010;Automatic Feedback-directed Object Fusing;Java, cache performance, garbage collection, just-in-time compilation, object colocation, object fusing, object inlining, optimization;
ACM;2011;The Capture Calculus Toolset;;
ACM;2015;Managing Requirements Knowledge in Business Networks: A Case Study;business networks, cross-organizational requirements engineering, knowledge maturing, knowledge organization systems, process development, requirements knowledge;
ACM;2015;Achieving Performance Isolation with Lightweight Co-Kernels;exascale, operating systems, virtualization;
ACM;2014;Personal Learning Environments (PLE) in the Teaching Ecosystem;informal learning, lifelong learning, personal learning environment (PLE), self regulated learning, teaching ecosystem;
ACM;1989;Computing, Research, and War: If Knowledge is Power, Where is Responsibility?;;
Engineering Village;2015;Rationalism with a dose of empiricism: Combining goal reasoning and case-based reasoning for self-adaptive software systems;;Requirements-driven approaches provide an effective mechanism for self-adaptive systems by reasoning over their runtime requirements models to make adaptation decisions. However, such approaches usually assume that the relations among alternative system configurations, environmental parameters and requirements are clearly understood, which is often not true. Moreover, they do not consider the influence of the current configuration of an executing system on adaptation decisions. In this paper, we propose an improved requirements-driven self-adaptation approach that combines goal reasoning and case-based reasoning. In the approach, past experiences of successful adaptations are retained as adaptation cases, which are described by not only requirements violations and contexts, but also currently deployed system configurations. The approach does not depend on a set of original adaptation cases, but employs goal reasoning to provide adaptation solutions when no similar cases are available. Case-based reasoning is used to provide more precise adaptation decisions that better reflect the complex relations among requirements violations, contexts, and current system configurations by utilizing past experiences. To prevent case-based reasoning from getting trapped in suboptimal adaptation solutions, an additional case mutation mechanism is introduced to mutate existing adaptation solutions when necessary. We conduct an experimental study with an online shopping benchmark to evaluate the effectiveness of our approach. The results show that our approach outperforms both a requirements-driven approach and a case-based approach in terms of satisfaction level of quality constraints. The results also confirm the effectiveness of case mutation for producing better adaptation solutions. In addition, we empirically investigate the evolution process of adaptation solutions. The evolution analysis reveals some general evolution trends of adaptation solutions such as different evolution phases. © Springer-Verlag London 2015.
Engineering Village;2009;Situ: A situation-theoretic approach to context-aware service evolution;;Evolvability is essential for computer systems to adapt to the dynamic and changing requirements in response to instant or delayed feedback from a service environment that nowadays is becoming more and more context aware,, however, current context-aware service-centric models largely lack the capability to continuously explore human intentions that often drive system evolution. To support service requirements analysis of real-world applications for services computing, this paper presents a situation-theoretic approach to human-intention-driven service evolution in context-aware service environments. In this study, we give situation a definition that is rich in semantics and useful for modeling and reasoning human intentions, whereas the definition of intention is based on the observations of situations. A novel computational framework is described that allows us to model and infer human intentions by detecting the desires of an individual as well as capturing the corresponding context values through observations. An inference process based on Hidden Markov Model makes instant definition of individualized services at runtime possible, and significantly, shortens service evolution cycle. We illustrate the possible applications of this framework through a smart home example a imed at supporting independent living of elderly people. © 2009 IEEE.
Engineering Village;2010;An agent-based system to support assurance of security requirements;;Current approaches to evaluating security assurance either focus on the software development stage or at the end product software. However, most often, it is after the deployment or implementation phase that specified security requirements may be violated. This may be due to improper deployment of the security measures, environmental hazards or to the fact that the assumptions under which the security requirements have been specified have become invalid. As such, this paper proposes an approach (supported by a system) which will complement security requirements engineering methodologies by gathering continuous evidence to inform on whether the security requirements elucidated during system development stage have been correctly implemented and as such, they can be relied upon to effectively protect system assets at runtime. We use Secure Tropos methodology to highlight the security assurance case and elicit the features of our security assurance evaluation system. We further depict the security assurance evaluation through an example based on firewalls configurations. © 2010 IEEE.
Engineering Village;2006;Elevating interaction requirements for web service composition;;Web services are increasingly utilized to create integrated applications from existing components However, incompatible interaction expectations between Web service interfaces and/or non-Web service interfaces participating in the overall application can inhibit the integration Frequently, these conflicts are resolved on a case by case basis Advantages can be gained by understanding integration conflict resolution as formal interaction requirements This approach enables evaluation of recurring integration conflicts during requirements analysis (rather than as a testing step), thus providing a more complete resolution that is abstracted from specific runtime interaction instances.
Engineering Village;2014;On requirements representation and reasoning using answer set programming;;We describe an approach to the representation of requirements using answer set programming and how this leads to a vision for the role of artificial intelligence techniques in software engineering with a particular focus on adaptive business systems. We outline how the approach has developed over several years through a combination of commercial software development and artificial intelligence research, resulting in: (i) a metamodel that incorporates the notion of runtime requirements, (ii) a formal language for their representation and its supporting computational model (InstAL), and (iii) a software architecture that enables monitoring of distributed systems. The metamodel is the result of several years experience in the development of business systems for e-tailing, while InstAL and the runtime monitor is on-going research to support the specification, verification and application of normative frameworks in distributed intelligent systems. Our approach derives from the view that in order to build agile systems, the components need to be structured more like software that controls robots, in that it is designed to be relatively resilient in the face of a non-deterministic, dynamic, complex environment about which there is incomplete information. Thus, degrees of autonomy become a strength and an opportunity, but must somehow be constrained by informing these autonomous components what should be done in a certain situation or what system state ought to be achieved through norms as expressions of requirements. Because such a system made up of autonomous components is potentially behaviourally complex and not just complicated, it becomes essential to monitor both whether norms/requirements are being fulfilled and if not why not. Finally, because control over the system can be expressed through requirements in the form of data that can be changed, a route is opened to adjustment and dynamic re-direction of running systems. © 2014 IEEE.
Engineering Village;2011;Monitoring fuzzy temporal requirements for service compositions: Motivations, challenges and experimental results;;Service compositions are an important family of self-adaptive systems, which need to cope with the variability of the environment (e.g., heterogeneous devices, changing context), and react to unexpected events (e.g., changing components) that may take place at runtime. To this aim, it is fundamental to continuously assess requirements while the system is executing and detect partial mismatches or handle uncertainty. Detecting the entity of a violation is very helpful, since it can guide the way applications adapt at runtime. This paper is based on the FLAGS language we already proposed in our previous work to represent requirements as fuzzy temporal formulas and identify partial violations at the temporal level. The paper illustrates the advantages of using the FLAGS language to express the requirements of service compositions, and proposes a technique to monitor them at runtime. The experimental evaluation demonstrates that the monitoring technique is feasible and the overhead introduced in the running system is negligible. © 2011 IEEE.
Engineering Village;2014;Requirements-driven social adaptation: Expert survey;;[Context and motivation] Self-adaptation empowers systems with the capability to meet stakeholders' requirements in a dynamic environment. Such systems autonomously monitor changes and events which drive adaptation decisions at runtime. Social Adaptation is a recent kind of requirements-driven adaptation which enables users to give a runtime feedback on the success and quality of a system's configurations in reaching their requirements. The system analyses users' feedback, infers their collective judgement and then uses it to shape its adaptation decisions. [Question/problem] However, there is still a lack of engineering mechanisms to guarantee a correct conduction of Social Adaptation. [Principal ideas/results] In this paper, we conduct a two-phase Expert Survey to identify core benefits, domain areas and challenges for Social Adaptation. [Contribution] Our findings provide practitioners and researchers in adaptive systems engineering with insights on this emerging role of users, or the crowd, and stimulate future research to solve the open problems in this area. © 2014 Springer International Publishing Switzerland.
Engineering Village;2008;Building contingencies into specifications;;We propose an approach to runtime feature composition and conflict resolution that combines arbitration and contingencies. By arbitration we mean the resolution of conflicts between features using priorities. Contingency means having several specifications per feature, satisfying the same requirement, depending on the current state of the shared resource. Evaluation of our approach shows that combining arbitration and contingencies ensures that in the event of a conflict, requirements of the conflicting features are eventually satisfied. © 2008 IEEE.
Engineering Village;2011;Social software product lines;;Software product lines are an engineering paradigm to systematically configure software products from reusable assets so that development effort and time are minimized. Configuring a high quality product is challenging, because quality is a dynamic property and can be difficult to determine at design time. In this paper, we propose Social Software Product Lines paradigm (SSPL) which exploits users' perception in judging products quality and guiding the configuration process at runtime. The SSPL paradigm advocates two principles. First, quality has to be evaluated iteratively during the product operation so that quality evaluation is kept up-to-date. Second, users are the primary evaluators of quality and their feedback is a primitive driver of configuration. At runtime, SSPL obtains users' quality feedback and reconfigures repeatedly in order to deliver the product found to be most adequate by the users' community. We discuss motivation and foundations of SSPL, and outline a set of research challenges. © 2011 IEEE.
Engineering Village;2010;Self-tuning of software systems through goal-based feedback loop control;;Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators rather than high-level requirements. By linking the overall satisfaction to a business value indicator as feedback, we propose a control-theoretic self-tuning method that can dynamically adjust the tradeoff decisions among different quality requirements. A preference-based reasoning algorithm is involved to configure hard goals accordingly to guide the following architecture reconfiguration. © 2010 IEEE.
Engineering Village;2011;Social sensing: When users become monitors;;Adaptation requires a system to monitor its operational context to ensure that when changes occur, a suitable adaptation action is planned and taken at runtime. The ultimate goal of adaptation is that users get their dynamic requirements met efficiently and correctly. Context changes and users'judgment of the role of the system in meeting their requirements are drivers for adaptation. In many cases, these drivers are hard to identify by designers at design time and hard to monitor by the use of exclusively technological means by the system at runtime. In this paper, we propose Social Sensing as the activity performed by users who act as monitors and provide information needed for adaptation at runtime. Such information helps the system cope with technology limitations and designers'uncertainty. We discuss the motivation and foundations of Social Sensing and outline a set of research challenges to address in future work. © 2011 ACM.
Engineering Village;2013;Requirements-driven software evolution;;It is often the case that stakeholders want to strengthen/weaken or otherwise change their requirements for a system-to-be when certain conditions apply at runtime. For example, stakeholders may decide that if requirement R is violated more than N times in a week, it should be relaxed to a less demanding one R-. Such evolution requirements play an important role in the lifetime of a software system in that they define possible changes to requirements, along with the conditions under which these changes apply. In this paper we focus on this family of requirements, how to model them and how to operationalize them at runtime. In addition, we evaluate our proposal with a case study adopted from the literature. © 2012 Springer-Verlag Berlin Heidelberg.
Engineering Village;2009;Analysis of problem frame and properties materiel requirements;;In order to extropolate what are involved in materiel requirements, problem frame for materiel requirements is provided and substantive properties which belong to materiel requirements are rigorously analyzed based on key artifacts, which are distilled from the wide variations in the use of terms. Problem frame for materiel requirements focuses on the key artifacts, their attributes, and relationships at a general level. By using higher-order logical notational conventions, formal analyses are made, describing the relations that material requirements and their reflection must satisfy. Adequacy, precondition, feasibility, necessity, and hierarchy of materiel requirements are included. Adequacy, necessity, and guidablity of materiel requirements reflection are also included. Finally, an example about the presumed urban antimissile defense system shows that the approach is highly feasible, effective and promising.
Engineering Village;2007;Towards variability design as decision boundary placement;;Complex information systems have numerous design variables that are systematically decided upon during the design process. In high-variability systems, some of these decisions are left open and deferred to later stages. For example, in product line architectures, some decision variables are used to generate families of products with variations in features. In user-adaptive systems, the behavior of the system is determined at runtime, based on user characteristics and preferences. In this paper, we propose to characterize variability in terms of boundaries in design decision graphs which depict the space of alternatives. A design decision about variability, such as what choices should be left to the user and which ones should be fixed at which stage in the design process, is then a question of where to place that decision boundary along some path in the relevant decision graph.
Engineering Village;2013;Supporting decision-making for self-adaptive systems: From goal models to dynamic decision networks;;[Context/Motivation] Different modeling techniques have been used to model requirements and decision-making of self-adaptive systems (SASs). Specifically, goal models have been prolific in supporting decision-making depending on partial and total fulfilment of functional (goals) and non-functional requirements (softgoals). Different goalrealization strategies can have different effects on softgoals which are specified with weighted contribution-links. The final decision about what strategy to use is based, among other reasons, on a utility function that takes into account the weighted sum of the different effects on softgoals. [Questions/Problems] One of the main challenges about decisionmaking in self-adaptive systems is to deal with uncertainty during runtime. New techniques are needed to systematically revise the current model when empirical evidence becomes available from the deployment. [Principal ideas/results] In this paper we enrich the decision-making supported by goal models by using Dynamic Decision Networks (DDNs). Goal realization strategies and their impact on softgoals have a correspondence with decision alternatives and conditional probabilities and expected utilities in the DDNs respectively. Our novel approach allows the specification of preferences over the softgoals and supports reasoning about partial satisfaction of softgoals using probabilities. We report results of the application of the approach on two different cases. Our early results suggest the decision-making process of SASs can be improved by using DDNs. © 2013 Springer-Verlag.
Engineering Village;2012;(Requirement) evolution requirements for adaptive systems;;It is often the case that stakeholders want to strengthen/weaken or otherwise change their requirements for a system-to-be when certain conditions apply at runtime. For example, stakeholders may decide that if requirement R is violated more than N times in a week, it should be relaxed to a less demanding one R-. Such evolution requirements play an important role in the lifetime of a software system in that they define possible changes to requirements, along with the conditions under which these changes apply. In this paper we focus on this family of requirements, how to model them and how to operationalize them at runtime. In addition, we evaluate our proposal with a case study adopted from the literature. © 2012 IEEE.
Engineering Village;2009;Requirements tracing to support change in dynamically adaptive systems;;[Context and motivation] All systems are susceptible to the need for change, with the desire to operate in changeable environments driving the need for software adaptation. A Dynamically Adaptive System (DAS) adjusts its behaviour autonomously at runtime in order to accommodate changes in its operating environment, which are anticipated in the system's requirements specification. [Question/Problem] In this paper, we argue that Dynamic Adaptive Systems' requirements specifications are more susceptible to change than those of traditional static systems. We propose an extension to i*strategic rationale models to aid in changing a DAS. [Principal Ideas/Results] By selecting some of the types of tracing proposed for the most complex systems and supporting them for DAS modelling, it becomes possible to handle change to a DAS' requirements efficiently, whilst still allowing artefacts to be stored in a Requirements Management tool to mitigate additional complexity. [Contribution] The paper identifies different classes of change that a DAS' requirements may be subjected to, and illustrates with a case study how additional tracing information can support the making of each class of change. © 2009 Springer Berlin Heidelberg.
Engineering Village;2012;Market-aware requirements;;Traditionally, non-functional requirements (NFRs) are specified as measurable entities to permit evaluation satisfaction,, however, NFR specifications quickly become obsolete because (1) NFRs are expressed in numbers, (2) architects specify them using the correct values at design time, and/or (3) providers are constantly improving their offer, in terms of functionality and quality of service (QoS). The computing-with-words approach has already been proposed to replace numerical NFR specifications, where natural language words denote fuzzy quality levels,, unfortunately, current proposals provide only for design-time, stakeholder-defined translation of words as numerical ranges. We propose a mechanism to automatically and dynamically determine current numerical ranges of the fuzzy quality levels from the available data, without human intervention, whenever changes to component QoS specifications. Our main contribution is allowing architects to specify their requirements using words only once (at design time), and whenever providers change components QoS characteristics, automatically update those requirements to the new market view, enabling market-aware requirements. The approach was validated by measuring the number of times that necessarily a requirement had to be rewritten at runtime in order to get new operationalizations which replace the now older ones. We use a set of ten complex requirements, a dataset of 1500 actual Web services with precise measurements for nine QoS aspects, and a simulated offering variability. A Web-based prototype is also made available.
Engineering Village;2015;Pragmatic requirements for adaptive systems: A goal-driven modeling and analysis approach;;Goal-models (GM) have been used in adaptive systems engineering for their ability to capture the different ways to fulfill the requirements. Contextual GM (CGM) extend these models with the notion of context and context-dependent applicability of goals. In this paper, we observe that the interpretation of a goal achievement is itself contextdependent. Thus, we introduce the notion of Pragmatic Goals which have a dynamic satisfaction criteria. However, the specification of contextdependent goals’ applicability as well as their interpretations make it hard for stakeholders to decide whether the model is achievable for all possible context combinations. Thus we also developed and evaluated an algorithm to decide on the Pragmatic CGM’s achievability.We performed several experiments to evaluate our algorithm regarding correctness and performance and concluded that it can be used for deciding at runtime the tasks to execute under a given context to achieve a quality constraint as well as for pinpointing context sets in which the model is intrinsically unachievable. © Springer International Publishing Switzerland 2015.
Engineering Village;2013;Satisficing-based approach to resolve feature interactions in control systems;;To handle the complexity of modern control systems there is an urgent need to develop features as independently developed units of extension. However, when independently developed features are later composed they become coupled through the shared environment resources. As a consequence, the system requirements may no longer be entailed when independent features try to control the same shared environment. Malfunctioning behavior as a consequence of feature interference is know in the literature as the feature interaction problem. This paper present an approach that uses designtime specification of independent requirements, in combination with a runtime arbitrator that search for feature interaction free programs which entail the system requirements. In case of conflicting requirements that can't be satisfied simultaneously, the mechanism supports explanation of the interactions as a context sharing problem. We demonstrate our approach in a real-life control system for industrial pot plant cultivation in greenhouses and show that solutions are found for compatible requirements and that conflicts are identified and explained for incompatible requirements. © 2013 Springer Science+Business Media.
Engineering Village;2003;Acquiring and incorporating state-dependent timing requirements;;Some real-time systems are designed to deliver services to objects that are controlled by external sources. Their services must be delivered on a timely basis, and the system fails when some services are delivered too late. Such a system may fail if the timing requirements, which it is designed to meet are erroneous. It may under-utilize resources and, consequently, be costly or unreliable if the requirements are too stringent. In general, the timing requirements of the system may change when the states of the objects monitored by the system change. Hence, one must identify how changes in object states call for changes in system requirements and how these changes should be incorporated in the design and implementation of the system. We first describe a methodology to determine timing requirements and to take into account of requirement changes at runtime. The method is based on several timing requirement determination schemes. Simulation data show that these schemes are effective for applications such as mobile IP hand-offs. We then discuss how to incorporate this methodology in the design of such systems and in the development process. © 2003 IEEE.
Engineering Village;2014;Dealing with multiple failures in zanshin: A control-theoretic approach;;Adaptive software systems monitor the environment to ensure that their requirements are being fulfilled. When this is not the case, their adaptation mechanism proposes an adaptation (a change to the behaviour/configuration) that can lead to restored satisfaction of system requirements. Unfortunately, such adaptation mechanisms don't work very well in cases where there are multiple failures (divergence of system behaviour relative to several requirements). This paper proposes an adaptation mechanism that can handle multiple failures. The proposal consists of extending the Qualia adaptation mechanism of Zanshin enriched with features adopted from Control Theory. The proposed framework supports the definition of requirements for the adaptation process prescribing how to deal at runtime with problems such as conicting requirements and synchronization, enhancing the precision and effectiveness of the adaptation mechanism. The proposed mechanism, named Qualia+ is illustrated and evaluated with an example using the meeting scheduling exemplar.
Engineering Village;2005;Dynamic software assembly for automatic deployment-oriented adaptation;;The notion of software adaptation considered in this paper relates to the capability of making software systems adjustable to varying deployment requirements. In this context we seek for the necessary runtime infrastructure to allow software systems adapt on the fly to the particular execution requirements. The primary assumption is that the constituent components of a software system may have to be provided with alternative incarnations, each potentially addressing varying deployment needs. In this context, adaptation is treated as a runtime function of the system itself, realising a component and assembly process, since the deployment-specific parameters are only known upon execution start-up. © 2005 Elsevier B.V.
Engineering Village;2006;Efficient trace monitoring;;A wealth of recent research involves generating program monitors from declarative specifications. Doing this efficiently has proved challenging, and available implementations often produce infeasibly slow monitors. We demonstrate how to dramatically improve performance - typically reducing overheads to within an order of magnitude of the program's normal runtime.
Engineering Village;2007;Setting and evaluation of flexible points on software user interface;;In order to adapt to user requirement changes at runtime, software provides adaptable operations through user interfaces to change software functionality. We suggest the FleXible Point (FXP), flexible changes, flexible degree, flexible force, and flexible distance to evaluate the effect of such user interfaces. An approach and a case are given to illustrate the evaluation and setting of the FXPs. Quantitative relationship between flexible point on user interface and software flexibility is discussed and expressed. The approach can be used as a guide to adjust, improve, and to compare the FXPs on user interfaces. It is a direction for managers to arrange different levels of manipulators to increase the FXP efficiency and bring user interface flexibility into play. © 2007 IEEE.
Engineering Village;2001;Constructing adaptive software in distributed systems;;Adaptive software that can react to changes in the execution environment or user requirements by switching algorithms at runtime is powerful yet difficult to implement, especially in distributed systems. This paper describes a software architecture for constructing such adaptive software and a graceful adaptation protocol that allows adaptations to be made in a coordinated manner across hosts transparently to the application. A realization of the architecture based on Cactus, a system for constructing highly configurable distributed services and protocols, is also presented. The approach is illustrated by outlining examples of adaptive components from a group communication service.
Engineering Village;2013;Active and adaptive services resource provisioning with personalized customization;;Software as a service(SaaS), we are moving to the age of service-oriented software engineering(SOSE). But for the goal of services computing, namely on-demand service, it has not been able to achieved by far, especially the active provisioning approach for services resource. In view of these facts of services aggregation, i.e. The relative deficient services resource, single provision structure and passive selection mode for service requesters. Active provisioning of services resource with personalized customization will be focused. In this paper, we have established software architecture with personalized active custom for services resource. Customization of the active provisioning of services resource mainly includes two aspects: Firstly, for unmatched services resource of service composition, sliced or segmentation method should be chosen to acquire the individual needs of services resource according to the overall requirements. Secondly, a huge amount of legacy software will be comprehensively reused, namely servicelization. Through personalized customization of services resource, it will achieve on-demand active provisioning to furnish adequate material for dynamic services aggregation. It will also provide the production framework, process guidance and engineering support of CASE tools for services resource provisioning in runtime based on personalized customization. The feasibility and efficiency of the proposed approach are verified by a series of experiments. © 2013 IEEE.
Engineering Village;2006;ANIS: A negotiated integration of services in distributed environments;;The development of highly dynamic distributed environments modifies the runtime behavior of applications. Applications tend to use services available everywhere in the environment and would like to, whenever it is possible and/or needed, integrate services offered by the local environment. In particular, if no single service can satisfy the functionality required by the application, combining existing services together should be a possibility in order to fulfill the request. In this article, we propose ANIS: A Negotiated Integration System. Our system provides a framework including a set of integration management interfaces - Integrable, Negotiable, IntegrationLifeCycle - and the tools implementing these interfaces. These tools offer different techniques of integration (local/remote composition, local/remote weaving, deployment by downloading/uploading), negotiation by contracts and the capability to manage the life cycle of the integration. A prototype based on Java platform and OSGi technology is implemented as a proof-of-concept to demonstrate the potential of ANIS1. © Springer-Verlag Berlin Heidelberg 2006.
Engineering Village;2007;An approach to automated agent deployment in service-based systems;;In service-based systems, services from various providers can be integrated following specific workflows to achieve users' goals. These workflows are often executed and coordinated by software agents, which invoke appropriate services based on situation changes. These agents need to be deployed on underlying platforms with respect to various requirements, such as access permission of agents, real-time requirements of workflows, and reliability of the overall system. Deploying these agents manually is often error-prone and time-consuming. Furthermore, agents need to migrate from hosts to hosts at runtime to satisfy deployment requirements. Hence, an automated agent deployment mechanism is needed. In this paper, an approach to automated agent deployment in service-base systems is presented. In this approach, the deployment requirements are represented as deployment policies, and techniques are developed for generating agent deployment plans by solving the constraints specified in deployment policies, and for generating executable code for runtime agent deployment and migration. © 2007 IEEE.
Engineering Village;2007;A factory to design and buid tailorable and verifiable middleware;;Heterogeneous non-functional requirements of Distributed Real-Time Embedded (DRE) system put a limit on middleware engineering: the middleware must reflect application requirements, with limited runtime impact. Thus, building an application-tailored middleware is both a requirement and a challenge. In this paper, we provide an overview of our work on the construction of middleware. We focus on two complementary projects: the definition of middleware that provides strong support for both tailorability and verification of its internals,, the definition of a methodology that enables the automatizing of key steps of middleware construction. We illustrate how our current work on PolyORB, Ocarina and the use of Petri Nets allows designer to build the middleware that precisely matches its application requirements and comes with precise proof of its properties. © Springer-Verlag Berlin Heidelberg 2007.
Engineering Village;2005;Memory Requirements of Java Bytecode Verification on Limited Devices;;Bytecode verification forms the corner stone of the Java security model that ensures the integrity of the runtime environment even in the presence of untrusted code. Limited devices, like Java smart cards, lack the necessary amount of memory to verify the type-safety of Java bytecode on their own. Proof carrying code techniques compute, outside the device, tamper-proof certificates which simplify bytecode verification and pass them along with the code. Rose has developed such an approach for a small subset of the Java bytecode language. In this paper, we extend this approach to real world Java software and develop a precise model of the memory requirements on the device. We use a variant of interval graphs to model liveness of memory regions in the checking step. Based on this model, memory-optimal checking strategies are computed outside the device and attached to the certificate. The underlying type system of the bytecode verifier has been augmented with multi-dimensional arrays and recognizes references to uninitialized Java objects. Our detailed measurements, based on real world Java libraries, demonstrate that the approach offers a substantial improvement in size of certificate over the similar approach taken by the KVM verifier. Worst case memory consumption on the device is examined as well and it turns out that the refinements based on our model save a significant amount of memory. © 2005 Published by Elsevier B.V.
Engineering Village;2006;Generic architecture and mechanisms for protocol reconfiguration;;The next generation of wireless mobile communications termed beyond 3G (or 4G) will be based on a heterogeneous infrastructure that comprises different wireless networks in a complementary manner. Beyond 3G will introduce reconfiguration capabilities to flexibly and dynamically (i.e. during operation) adapt the wireless protocol stacks to better meet the ever-changing service requirements. For the dynamic reconfiguration of protocol stacks during runtime operation to become a practical capability of mobile communication systems it is necessary to establish a software architecture that functionally supports reconfiguration. In the present paper a generic architecture and respective mechanisms to achieve protocol stack and component based protocol layer reconfiguration are proposed. © Springer Science + Business Media LLC 2006.
Engineering Village;2006;Proceedings 2006 Australian Software Engineering Conference;;The proceedings contain 44 papers. The topics discussed include: an agile approach to support incremental development of requirements specifications,, requirements capture and specifications for enterprise applications: a UML based attempt,, compatibility test for coordination aspects of software components,, a framework for checking behavioral compatibility for component composition,, a runtime monitoring and validation framework for web service interactions,, checking conformance between business processes and web service contract in service oriented applications,, a coordinated architecture for the agent-based service level agreement negotiation of web service composition,, optimizing web services performance with table driven XML,, design of agent-oriented pattern templates,, a framework for evaluating alternative architectures and its application to financial business processes,, and a service-oriented architecture for software process technology,, the transparent adaptation approach to the development of awareness mechanisms for groupware.
Engineering Village;2005;Tracing cross-cutting requirements via context-based constraints;;In complex systems, it is difficult to identify which system element is involved in which requirement. In this article, we present a new approach for expressing and validating a requirement even if we don't precisely know which system elements are involved: a context-based constraint (CoCon) can identify the involved elements according to their context. CoCons support checking the system for compliance with requirements during (re-)design, during (re-)configuration or at runtime because they specify requirements on an abstract level independent of the monitored artefact type. They facilitate handling cross-cutting requirements for possibly large, overlapping or dynamically changing sets of system elements - even across different artefact types or platforms. Besides defining CoCons, we discuss algorithms for detecting violated or contradicting CoCons. © 2005 IEEE.
Engineering Village;2007;An automated approach to monitoring and diagnosing requirements;;Monitoring the satisfaction of software requirements and diagnosing what went wrong in case of failure is a hard problem that has received little attention in the Software and Requirement Engineering literature. To address this problem, we propose a framework adapted from artificial intelligence theories of action and diagnosis. Specifically, the framework monitors the satisfaction of software requirements and generates log data at a level of granularity that can be tuned adaptively at runtime depending on monitored feedback. When errors are found, the framework diagnoses the denial of the requirements and identifies problematic components. To support diagnostic reasoning, we transform the diagnostic problem into apropositional satisfiability (SAT) problem that can be solved by existing SAT solvers. We preprocess log data into a compact propositional encoding that better scales with problem size. The proposed theoretical framework has been implemented as a diagnosing component that will return sound and complete diagnoses accounting for observed aberrant system behaviors. Our solution is illustrated with two medium-sized publicly available case studies: a Web-based email client and an ATM simulation. Our experimental results demonstrate the feasibility of scaling our approach to medium-size software systems. Copyright 2007 ACM.
Engineering Village;2004;Policy-based dynamic reconfiguration of mobile-code applications;;Policy-Enabled Mobile Applications (Poema) is a policy-based approach to mobility programming that expresses and controls reconfiguration strategies at a high level of abstraction, separate from the application's functionality. It provides an integrated environment for developing applications that can change both their functionality and layout at runtime in response to environment conditions.
Engineering Village;2006;Improving exception handling by discovering change dependencies in adaptive process management systems;;Process-aware information systems should enable the flexible alignment of business processes to new requirements by supporting deviations from the predefined process model at runtime. To facilitate such dynamic process changes we have adopted techniques from case-based reasoning (CBR). In particular, our existing approach allows to capture the semantics of ad-hoc changes, to support their memorization, and to enable their reuse in upcoming exceptional situations. To further improve change reuse this paper presents an approach for discovering dependencies between ad-hoc modifications from change history. Based on this information better user assistance can be provided when dynamic process changes have to be made. © Springer-Verlag Berlin Heidelberg 2006.
Engineering Village;2007;Using Microcomponents and Design Patterns to Build Evolutionary Transaction Services;;The evolution of existing transaction services is limited because they are tightly coupled to a given transaction standard, implement a dedicated commit protocol, and support a fixed kind of applicative participants. The next challenge for transaction services will be to deal with evolution concerns. This evolution should allow developers to tune the transaction service depending on the transaction standard or the application requirements either at design time or at runtime. The contribution of this paper is to introduce the common approach that we have defined to build various evolutionary transaction services. This common approach is based on the use of microcomponents and design patterns, whose flexibility properties allow transaction services to be adapted to various execution contexts. This approach is applied in our GoTM framework that supports the construction of transaction services implementing several transaction standards and commit protocols. We argue that using fine-grained components and design patterns to build transaction services is an efficient solution to the evolution problem and our past experiences confirm that this approach does not impact the transaction service efficiency. © 2006 Elsevier B.V. All rights reserved.
Engineering Village;2005;A PES for use in highly safety-critical control;;The programmable electronic systems currently employed in safety-critical control implement either strictly periodical or task-based operation. Here, a concept combining the advantages of both principles is presented. Its most essential characteristics are task execution without the use of asynchronous interrupts, and a tightly integrated hardware scheme to detect processing failures, for non-intrusive monitoring, and forward recovery at runtime. The architecture builds up on physical separation of task execution and task administration, which is implemented in form of a digital circuitry. Time is quantised into execution intervals, and tasks are partitioned into execution blocks matching these intervals. This concept lowers the complexity of both hardware and temporal behaviour and, thus, conforms particularly well with the requirements of the safety standard IEC 61508. © 2005 IEEE.
Engineering Village;2006;A comparison of nearest neighbor search algorithms for generic object recognition;;The nearest neighbor (NN) classifier is well suited for generic object recognition. However, it requires storing the complete training data, and classification time is linear in the amount of data. There are several approaches to improve runtime and/or memory requirements of nearest neighbor methods: Thinning methods select and store only part of the training data for the classifier. Efficient query structures reduce query times. In this paper, we present an experimental comparison and analysis of such methods using the ETH-80 database. We evaluate the following algorithms. Thinning: condensed nearest neighbor, reduced nearest neighbor, Baram's algorithm, the Baram-RNN hybrid algorithm, Gabriel and GSASH thinning. Query structures: kd-tree and approximate nearest neighbor. For the first four thinning algorithms, we also present an extension to k-NN which allows tuning the trade-off between data reduction and classifier degradation. The experiments show that most of the above methods are well suited for generic object recognition. © Springer-Verlag Berlin Heidelberg 2006.
Engineering Village;2007;The interface of VISTO, a new vector image search tool;;We discuss the interface of VISTO, a Content Based Image Retrieval system for vector images in SVG (Scalable Vector Graphics) format. The system includes different engines and one common graphical interface. In fact, to support the different requirements of different application domains, the system offers a variety of engines. Most notably, due to its modular architecture, the system allows users to add engines at runtime,, the interface provides support for newly added engines, including parameter tuning. The interface is designed for two classes of users: application domain users and researchers in the field of multimedia. Application domain users can use both query-by-sketch and query-by-example to search collections,, researcher users can test, tune, and compare engines, and they can design datasets to be used in batch mode. VISTO is an open source project developed in Java,, it uses advanced features of the language, such as the Core Reflection API, to dynamically adapt the interface to the available engines. The system is being validated in different application domains, including the production of 2D animation, the Sign Writing Language, and the BLISS Language. © Springer-Verlag Berlin Heidelberg 2007.
Engineering Village;2007;Architectural adaptation addressing the criteria of multiple quality attributes in mission-critical systems;;Mission-critical software claims safe and robust adaptations that comply with rigorous criteria of multiple critical quality attributes. Existing adaptation approaches pay little attention to comprehensively capture mission goals and explicitly specify adaptation requirements. We propose an approach to using scenario-based analysis to elicit and specify the criteria of multiple quality attributes as adaptation invariants, and design corresponding architecture variants as facilities implementing adaptations. We also present how to make adaptation decisions at runtime. ©2007 IEEE.
Engineering Village;2005;Correlating features and code using a compact two-sided trace analysis approach;;Software developers are constantly required to modify and adapt application features in response to changing requirements. The problem is that just by reading the source code, it is difficult to determine how classes and methods contribute to the runtime behavior of features. Moreover, dependencies between system features are not obvious, consequently software maintenance operations often result in unintended side effects. To tackle these problems, we propose a compact feature-driven approach (i.e., summarized trace information) based on dynamic analysis to characterize features and computational units of an application. We extract execution traces to achieve an explicit mapping between features and classes using two complementary perspectives. We apply our approach to two case studies and we report our findings. © 2005 IEEE.
Engineering Village;2006;Abstract machine and structural availability;;Abstract machine is the theoretical foundation of software programming. In previous research of software quality on abstract machine, because abstract machine always works correctly without even a small mistake encountered, software availability is concerned without relevant with hardware availability and environmental factors. However, software availability is defined as the probability that software is operating according to requirements at a given point in time. Hardware error, fault or failure surely has negative impact on software operation and decreases software availability. Software runtime environment, such as Operation System and/or some platforms, also have availability lower than 100%. Users tend to consider unrepeatable and unobvious hardware or environmental errors as application errors, and decrease their satisfaction with application. Therefore, such factors should be considered in software availability. Thus, an Abstract Machine with Hardware Reliability (AM-HR) and an Abstract Machine with Environment Reliability (AM-ER) is proposed in this paper to extend the AM theory to enable the investigation of software structural availability. Though structural availability isn't mature enough now, its utility in Change Management is shown to exhibit the promising prospect.
Engineering Village;2007;The role of roles in supporting reconfigurability and fault localizations for open distributed and embedded systems;;One of the main characteristics of open distributed embedded systems is that the involved entities are often very dynamic - -different individual entities may join or leave the systems frequently. Therefore, systems built of these dynamic entities must be runtime reconfigurable. In addition, large classes of open embedded systems often have high availability and dependability requirements. However, the openness makes these requirements more difficult to achieve and the system more vulnerable to attacks. This article presents a coordination model, the Actor, Role and Coordinator (ARC) model, that aims to support reconfigurability and fault localization for open distributed embedded software systems. In particular, the actor model is used to model concurrent embedded entities, while the system's reconfigurability and dependability requirements are encapsulated within coordination objects: roles and coordinators, and are achieved through coordination among the actors. Roles, as a key thrust in the ARC model not only represent an abstraction for a set of behaviors shared by a group of actors so that reconfiguration within the roles becomes transparent to entities outside the roles, but also assume coordination responsibilities among the member actors. The article also argues from both analytical and empirical perspectives that with the support of the role, faults can be localized within actors, and actor level reconfiguration becomes transparent to the system. © 2007 ACM.
Engineering Village;2001;Evaluating meta-programming mechanisms for ORB middleware;;Distributed object computing middleware, such as CORBA, COM+, and Java RMI, shields developers from many tedious and error-prone aspects of programming distributed applications. It is hard to evolve distributed applications after they are deployed, however, without adequate middleware support for meta-programming mechanisms, such as smart proxies, interceptors, and pluggable protocols. These mechanisms can help improve the adaptability of distributed applications by allowing their behavior to be modified without changing their existing software designs and implementations significantly. This article examines and compares common meta-programming mechanisms supported by DOC middleware. These mechanisms allow applications to adapt more readily to changes in requirements and runtime environments throughout their lifecycles. Some of these meta-programming mechanisms are relatively new, whereas others have existed for decades. Until recently, however, DOC middleware has not provided all these mechanisms in a single integrated framework, so researchers and developers may not be familiar with the breadth of metaprogramming mechanisms available today. This article provides a systematic evaluation of these mechanisms to help researchers and developers determine which are best suited to their application needs.
Engineering Village;2007;Pervasive service composition in the home network;;The home environment becomes ready to host distributed devices dynamically adapting to service availability and reacting to user location and user activity. Sensors, high definition rendering systems, home gateways, wired and wireless controllable equipments are now available. Many protocols enable connectivity and interaction between devices. However, challenges remain: protocol heterogeneity, interface fragmentation and device composition static aspect make self-organization and dynamic reconfiguration hardly achievable. This paper describes attractive scenarios at home which lead to the definition of the pervasive service composition requirements. A software architecture facing the mentioned challenges is proposed over OSGi. It first enables developers to implement distributed plug-n-play applications like a local one. It also delivers a serviceoriented middleware allowing spontaneous distributed service composition to occur at runtime. © 2007 IEEE.
Engineering Village;2007;Adaptive agent model: An agent interaction and computation model;;Software systems must be capable of coping with continuous requirements changes and at the same time wisely make use of emerging components and services to remain useful in their environment. In this paper, the Adaptive Agent Model (AAM) approach is proposed. The AAM uses configurable interaction model to drive adaptive agent behaviour. The model captures user requirements and is maintained by experts at a high level of abstraction. The AAM interaction model has been discussed with regard to interaction specification and interaction coordination, in line with a coordination language for the OpenKnowledge project. A major benefit of using the approach is agents can dynamically choose disparate components and services already developed for computation via their interaction with each other at runtime, when a new interaction model has been configured for them towards an emerging business goal. A simple expert seeking scenario has been used to illustrate the approach. © 2007 IEEE.
Engineering Village;2006;Comparative evaluation of dependability characteristics for peer-to-peer architectural styles by simulation;;An important concern for the successful deployment of a dependable system is its quality of service (QoS), which is significantly influenced by its architectural style. We propose the comparative evaluation of architectural styles by simulation. Our approach integrates architectural styles and concrete architectures to enable early design-space exploration in order to predict the QoS of peer-to-peer systems. We illustrate the approach via two case studies where availability of resources and performance of peer-to-peer search methods are evaluated. Based on our experience with these simulation environments, we sketch tool support for simulating architectural changes at runtime. © 2006 Elsevier Inc. All rights reserved.
Engineering Village;2004;Model-driven business process integration and management: A case study with the Bank SinoPac regional service platform;;Business process integration and management (BPIM) is a critical element in enterprise business transformation. Small and medium-sized businesses have their own requirements for BPIM solutions: The engagement methodology should be fast and efficient,, a reusable and robust framework is required to reduce cost,, and the whole platform should be lightweight so that one can easily revise, develop, and execute solutions. We believe that model-driven technologies are the key to solving all of the challenges mentioned above. Model Blue, a set of model-driven business integration and management methods, frameworks, supporting tools, and a runtime environment, was developed by the IBM China Research Laboratory (CRL) in Beijing to study the efficacy of model-driven BPIM. To verify the technology and methodology, Model Blue was deployed with Bank SinoPac, a mid-sized bank headquartered in Taiwan. A lightweight BPIM solution platform was delivered for Bank SinoPac to design, develop, and deploy its business logic and processes. During the eight-month life span of the project, IBM teams developed four major solutions for Bank SinoPac, which also developed one solution independently. In spite of the remote working environment and the outbreak of the Severe Acute Respiratory Syndrome illness, the project was completed successfully on schedule and within budget, with up to 30% efficiency improvement compared with similar projects. Bank SinoPac was satisfied with the technology and methodology, and awarded IBM other projects. In this paper, we illustrate how each key business process integration and solution development phase was carried out and guided by business process modeling, together with major experiences gained. The following technical aspects are discussed in detail: a two-dimensional business process modeling view to integrate flow modeling and data modeling,, a lightweight processing logic automation environment with tooling support,, and the end-to-end BPIM methodology, with models and documents successfully integrated as part of (or replacement for) the deliverables defined in the existing servicing methodologies and software engineering approaches. © Copyright 2004 by International Business Machines Corporation.
Engineering Village;2002;Using reflection as a mechanism for enforcing security policies on compiled code;;Securing application resources or defining finer-grained access control for system resources using the Java security architecture requires manual changes to source code. This is error-prone and cannot be done if only compiled code is present. We show how behavioural reflection can be used to enforce security policies on compiled code. Other authors have implemented code rewriting toolkits that achieve the same effect but they either require policies to be expressed in terms of low level abstractions or require the use of new high level policy languages. Our approach allows reuseable policies to be implemented as metaobjects in a high level object oriented language (Java), and then bound to application objects at loadtime. The binding between metaobjects and objects is implemented through bytecode rewriting under the control of a declarative binding specification. We have implemented this approach using Kava which is a portable reflective Java implementation. Kava allows customisation of a rich range of runtime behaviour, and provides a non-bypassable meta level suitable for implementing security enforcement. We discuss how we have used Kava to show how to secure a third-party application, how we prevent Kava being bypassed, and compare its performance with non-reflective security enforcement.
Engineering Village;1998;Use Case Maps as architectural entities for complex systems;;This paper presents a novel, scenario-based notation called Use Case Maps (UCMs) for describing, in a high-level way, how the organizational structure of a complex system and the emergent behavior of the system are intertwined. The notation is not a behavior specification technique in the ordinary sense, but a notation for helping a person to visualize, think about, and explain the big picture. UCMs are presented as `architectural entities' that help a person stand back from the details during all phases of system development. The notation has been thoroughly exercised on systems of industrial scale and complexity and the distilled essence of what has been found to work in practice is summarized in this paper. Examples are presented that confront difficult complex-system issues directly: decentralized control, concurrency, failure, diversity, elusiveness and fluidity of runtime views of software, self-modification of system makeup, difficulty of seeing large-scale units of emergent behavior cutting across systems as coherent entities (and of seeing how such entities arise from the collective efforts of components), and large scale.
Engineering Village;2005;Early verification and validation of mission critical systems;;Our world is increasingly relying on complex software and systems. In a growing number of fields such as transportation, finance, telecommunications, medical devices, they now play a critical role and require high assurance. To achieve this, it is imperative to produce high quality requirements. The KAOS goal-oriented requirements engineering methodology provides a rich framework for requirements elicitation and management of such systems. This paper demonstrates the practical industrial application of that methodology. The non-critical parts are modelled semi-formally using a graphical language for goal-oriented requirements engineering. When and where needed (ie. for critical parts of a system) the model can be specified at formal level using a real-time temporal logic. That formal level seamlessly extends the semi-formal level which can also help hide the formality for the non-specialist. To ensure at an early stage that the right system is being built and that the requirements model is right, validation and verification tools are applied on that model. Early verification checks help to discover missing requirements, overlooked assumptions or incorrect goal refinements. State machines generation from operations provides an executable model useful for validation purposes or for deriving an initial design. Acceptance test cases and runtime behavior monitors can also be derived from the model. The process is supported by an integrated toolbox implementing the above tools by a roundtrip mapping of KAOS requirements level notations to the languages of formal technology tools such as model-checkers, SAT engines or constraint solvers. A graphical visualization framework also significantly helps validation using domain-based representations. © 2005 Elsevier B.V. All rights reserved.
Engineering Village;2013;Requirements-driven self-repairing against environmental failures;;Self-repairing approaches have been proposed to alleviate the runtime requirements satisfaction problem by switching to appropriate alternative solutions according to the feedback monitored. However, little has been done formally on analyzing the relations between specific environmental failures and corresponding repairing decisions, making it a challenge to derive a set of alternative solutions to withstand possible environmental failures at runtime. To address these challenges, we propose a requirements-driven self-repairing approach against environmental failures, which combines both development-time and runtime techniques. At the development phase, in a stepwise manner, we formally analyze the issue of self-repairing against environmental failures with the support of the model checking technique, and then design a sufficient and necessary set of alternative solutions to withstand possible environmental failures. The runtime part is a runtime self-repairing mechanism that monitors the operating environment for unsatisfiable situations, and makes self-repairing decisions among alternative solutions in response to the detected environmental failures. © 2013 IEEE.
Engineering Village;2010;Fuzzy goals for requirements-driven adaptation;;Self-adaptation is imposing as a key characteristic of many modern software systems to tackle their complexity and cope with the many environments in which they can operate. Self-adaptation is a requirement per-se, but it also impacts the other (conventional) requirements of the system,, all these new and old requirements must be elicited and represented in a coherent and homogenous way. This paper presents FLAGS, an innovative goal model that generalizes the KAOS model, adds adaptive goals to embed adaptation countermeasures, and fosters self-adaptation by considering requirements as live, runtime entities. FLAGS also distinguishes between crisp goals, whose satisfaction is boolean, and fuzzy goals, whose satisfaction is represented through fuzzy constraints. Adaptation countermeasures are triggered by violated goals and the goal model is modified accordingly to maintain a coherent view of the system and enforce adaptation directives on the running system. The main elements of the approach are demonstrated through an example application. © 2010 IEEE.
Engineering Village;1987;ARCHITECTURE OF LISP MACHINES.;;The authors discuss what takes place during typical Lisp execution. On this basis they enumerate the runtime requirements of a Lisp system and identify potential obstacles to good machine performance. They provide a classification of the Lisp machines encountered during the survey, and examine the techniques used in these machines to cater to Lisp's runtime requirements.
Engineering Village;2012;Transparent structural online test for reconfigurable systems;;FPGA-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. However, the reliability of modern FPGAs is threatened by latent defects and aging effects. Hence, it is mandatory to ensure the reliable operation of the FPGA's reconfigurable fabric. This can be achieved by periodic or on-demand online testing. In this paper, a system-integrated, transparent structural online test method for runtime reconfigurable systems is proposed. The required tests are scheduled like functional workloads, and thorough optimizations of the test overhead reduce the performance impact. The proposed scheme has been implemented on a reconfigurable system. The results demonstrate that thorough testing of the reconfigurable fabric can be achieved at negligible performance impact on the application. © 2012 IEEE.
Engineering Village;2013;An agent-based requirements monitoring framework for internetware;;Running in a complicated, open and highly-dynamic environment, Internetware systems are likely to deviate from their requirements specification. In recent years, there have been a series of researches on runtime requirements monitoring and self-repairing based on goal-oriented requirements models and goal reasoning. However, a practical implementation framework for requirements monitoring and repairing, which supports typical Internetware characteristics like distribution and sociality, is still missing. In this paper, we propose an agent-based requirements monitoring framework for Internetware. The monitoring agents in the framework are able to monitor host systems on internal goal satisfaction and cross-agent goal delegation at runtime, and perform actuate repairing actions based on customized policies when requirements deviations are detected in a non-intrusive manner. The framework organizes monitoring agents in a decentralized way and supports cross-system goal delegation, requirements monitoring and self-repairing with inter-agent communication and interaction. To evaluate the effectiveness of our framework, we've conducted a case study with an online product booking system. The results show that the framework can effectively alleviate potential system failures in various self-reparing scenarios.
Engineering Village;2013;Test strategies for reliable runtime reconfigurable architectures;;Field-programmable gate array (FPGA)-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. The reliability of FPGAs, being manufactured in latest technologies, is threatened by soft errors, as well as aging effects and latent defects. To ensure reliable reconfiguration, it is mandatory to guarantee the correct operation of the reconfigurable fabric. This can be achieved by periodic or on-demand online testing. This paper presents a reliable system architecture for runtimereconfigurable systems, which integrates two nonconcurrent online test strategies: preconfiguration online tests (PRET) and postconfiguration online tests (PORT). The PRET checks that the reconfigurable hardware is free of faults by periodic or on-demand tests. The PORT has two objectives: It tests reconfigured hardware units after reconfiguration to check that the configuration process completed correctly and it validates the expected functionality. During operation, PORT is used to periodically check the reconfigured hardware units for malfunctions in the programmable logic. Altogether, this paper presents PRET, PORT, and the system integration of such test schemes into a runtime-reconfigurable system, including the resource management and test scheduling. Experimental results show that the integration of online testing in reconfigurable systems incurs only minimum impact on performance while delivering high fault coverage and low test latency. © 2013 IEEE.
Engineering Village;2015;Iterative solutions to the steady-state density matrix for optomechanical systems;;We present a sparse matrix permutation from graph theory that gives stable incomplete lower-upper preconditioners necessary for iterative solutions to the steady-state density matrix for quantum optomechanical systems. This reordering is efficient, adding little overhead to the computation, and results in a marked reduction in both memory and runtime requirements compared to other solution methods, with performance gains increasing with system size. Either of these benchmarks can be tuned via the preconditioner accuracy and solution tolerance. This reordering optimizes the condition number of the approximate inverse and is the only method found to be stable at large Hilbert space dimensions. This allows for steady-state solutions to otherwise intractable quantum optomechanical systems. © 2015 American Physical Society.
Engineering Village;2014;Improving OLAM with cloud elasticity;;Elasticity is considered one of the fundamental properties of cloud computing, and can be seen as the ability of a system to increase or decrease the computing resources allocated in a dynamic and on demand way. This feature is suitable for dynamic applications, whose resources requirements cannot be determined exactly in advance, either due to changes in runtime requirements or in application structure. A good candidate for using cloud elasticity is the Ocean-Land-Atmosphere Model (OLAM), since it presents a significant load variation during its execution and due to online mesh refinement (OMR), that causes load unbalancing problems. In this paper, we present our efforts to adapt OLAM to use the elasticity offered in cloud environments to dynamic allocate resources according to the demands of each execution phase, and to minimize the load unbalancing caused by OMR. The results show that elasticity was successfully used to provide these features, improving the OLAM performance and providing a better use of resources. © 2014 Springer International Publishing.
Engineering Village;2013;DYNAMICO: A reference model for governing control objectives and context relevance in self-adaptive software systems;;Despite the valuable contributions on self-adaptation, most implemented approaches assume adaptation goals and monitoring infrastructures as non-mutable, thus constraining their applicability to systems whose context awareness is restricted to static monitors. Therefore, separation of concerns, dynamic monitoring, and runtime requirements variability are critical for satisfying system goals under highly changing environments. In this chapter we present DYNAMICO, a reference model for engineering adaptive software that helps guaranteeing the coherence of (i) adaptation mechanisms with respect to changes in adaptation goals,, and (ii) monitoring mechanisms with respect to changes in both adaptation goals and adaptation mechanisms. DYNAMICO improves the engineering of self-adaptive systems by addressing (i) the management of adaptation properties and goals as control objectives,, (ii) the separation of concerns among feedback loops required to address control objectives over time,, and (iii) the management of dynamic context as an independent control function to preserve context-awareness in the adaptation mechanism. © 2013 Springer-Verlag.
Engineering Village;2012;Dynamic phase-based tuning for embedded systems using phase distance mapping;;Phase-based tuning specializes a system's tunable parameters to the varying runtime requirements of an application's different phases of execution to meet optimization goals. Since the design space for tunable systems can be very large, one of the major challenges in phase-based tuning is determining the best configuration for each phase without incurring significant tuning overhead (e.g., energy and/or performance) during design space exploration. In this paper, we propose phase distance mapping, which directly determines the best configuration for a phase, thereby eliminating design space exploration. Phase distance mapping applies the correlation between a known phase's characteristics and best configuration to determine a new phase's best configuration based on the new phase's characteristics. Experimental results verify that our phase distance mapping approach determines configurations within 3% of the optimal configurations on average and yields an energy delay product savings of 26% on average. © 2012 IEEE.
Engineering Village;2014;Progress and improvement of KSTAR plasma control using model-based control simulators;;Superconducting tokamaks like KSTAR, EAST and ITER need elaborate magnetic controls mainly due to either the demanding experiment schedule or tighter hardware limitations caused by the superconducting coils. In order to reduce the operation runtime requirements, two types of plasma simulators for the KSTAR plasma control system (PCS) have been developed for improving axisymmetric magnetic controls. The first one is an open-loop type, which can reproduce the control done in an old shot by loading the corresponding diagnostics data and PCS setup. The other one, a closed-loop simulator based on a linear nonrigid plasma model, is designed to simulate dynamic responses of the plasma equilibrium and plasma current (Ip) due to changes of the axisymmetric poloidal field (PF) coil currents, poloidal beta, and internal inductance. The closed-loop simulator is the one that actually can test and enable alteration of the feedback control setup for the next shot. The simulators have been used routinely in 2012 plasma campaign, and the experimental performances of the axisymmetric shape control algorithm are enhanced. Quality of the real-time EFIT has been enhanced by utilizations of the open-loop type. Using the closed-loop type, the decoupling scheme of the plasma current control and axisymmetric shape controls are verified through both the simulations and experiments. By combining with the relay feedback tuning algorithm, the improved controls helped to maintain the shape suitable for longer H-mode (10-16 s) with the number of required commissioning shots largely reduced. © 2013 Elsevier B.V.
Engineering Village;2011;Adaptable Decentralized Service Oriented Architecture;;In the Service Oriented Architecture (SOA), BPEL specified business processes are executed by non-scalable centralized orchestration engines. In order to address the scalability issue, decentralized orchestration engines are applied, which decentralize BPEL processes into static fragments at design time without considering runtime requirements. The fragments are then encapsulated into runtime components such as agents. There are a variety of attitudes towards workflow decentralization,, however, only a few of them produce adaptable fragments with runtime environment. In this paper, producing runtime adaptable fragments is presented in two aspects. The first one is frequent-path adaptability that is equal to finding closely interrelated activities and encapsulating them in the same fragment to omit the communication cost of the activities. Another aspect is proportional-fragment adaptability, which is analogous to the proportionality of produced fragments with number of workflow engine machines. It extenuates the internal communication among the fragments on the same machine. An ever-changing runtime environment along with the mentioned adaptability aspects may result in producing a variety of process versions at runtime. Thus, an Adaptable and Decentralized Workflow Execution Framework (ADWEF) is introduced that proposes an abstraction of adaptable decentralization in the SOA orchestration layer. Furthermore, ADWEF architectures Type-1 and Type-2 are presented to support the execution of fragments created by two decentralization methods, which produce customized fragments known as Hierarchical Process Decentralization (HPD) and Hierarchical Intelligent Process Decentralization (HIPD). However, mapping the current system conditions to a suitable decentralization method is considered as future work. Evaluations of the ADWEF decentralization methods substantiate both adaptability aspects and demonstrate a range of improvements in response-time, throughput, and bandwidth-usage compared to previous methods. © 2011 Elsevier Inc.
Engineering Village;2012;OTERA: Online test strategies for reliable reconfigurable architectures - Invited paper for the AHS-2012 special session 'dependability by reconfigurable hardware';;FPGA-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. However, the reliability of FPGAs, which are manufactured in latest technologies, is threatened not only by soft errors, but also by aging effects and latent defects. To ensure reliable reconfiguration, it is mandatory to guarantee the correct operation of the underlying reconfigurable fabric. This can be achieved by periodic or on-demand online testing. The OTERA project develops and evaluates components and strategies for reconfigurable systems that feature reliable reconfiguration. The research focus ranges from structural online tests for the FPGA infrastructure and functional online tests for the configured functionality up to the resource management and test scheduling. This paper gives an overview of the project tasks and presents first results. © 2012 IEEE.
Engineering Village;2015;A programming-level approach for elasticizing parallel scientific applications;;Elasticity is considered one of the fundamental properties of cloud computing. Several mechanisms to provide the feature are offered by public cloud providers and in some academic works. We argue these solutions are inefficient in providing elasticity for scientific applications, since they cannot consider the internal structure and behavior of applications. In this paper we present an approach for exploring the elasticity in scientific applications, in which the elasticity control is embedded in application source code and constructed using elasticity primitives. This approach enables the application itself to request or to release its own resources, taking into account the execution flow and runtime requirements. To support the construction of elastic applications using the presented approach, we developed the Cloudine framework. Cloudine provides all components necessary to construct and execute elastic scientific applications. The Cloudine effectiveness is demonstrated in the experiments where the platform is successfully used to include new features to existing applications, to extend functionalities of other elasticity frameworks and to add elasticity support to parallel programming libraries. © 2015 Elsevier Inc. All rights reserved.
Engineering Village;2007;Aspectizing a web server for adaptation;;Web servers are exposed to extremely changing runtime requirements. Going offline to adjust policies and configuration parameters in order to cope with such requirements is not an available choice for long running web servers. Many of the policies that need to be adapted are crosscutting in nature. Aspect-Oriented Programming (AOP) provides mechanisms to encapsulate the crosscutting policies as aspects. This paper describes the integration of a statically configurable web server with our dynamic aspect weaving infrastructure. This integration transformed the server to a dynamically adaptable one that could adjust its policies and configuration parameters at runtime according to the changing requirements. This paper further provides a comprehensive analysis of the memory and runtime costs associated with this transformation, and explains how our dynamic aspect weaving infrastructure via its tailored support facilitates to minimise these costs. © 2007 IEEE.
Engineering Village;2010;Efficient object detection using orthogonal NMF descriptor hierarchies;;Recently descriptors based on Histograms of Oriented Gradients (HOG) and Local Binary Patterns (LBP) have shown excellent results in object detection considering the precision as well as the recall. However, since these descriptors are based on high dimensional representations such approaches suffer from enormous memory and runtime requirements. The goal of this paper is to overcome these problems by introducing hierarchies of orthogonal Non-negative Matrix Factorizations (NMF). In fact, in this way a lower dimensional feature representation can be obtained without loosing the discriminative power of the original features. Moreover, the hierarchical structure allows to represent parts of patches on different scales allowing for a more robust classification. We show the effectiveness of our approach for two publicly available datasets and compare it to existing state-of-the-art methods. In addition, we demonstrate it in context of aerial imagery, where high dimensional images have to be processed requiring efficient methods. © 2010 Springer-Verlag.
Engineering Village;2001;Software implementation of synchronous programs;;Synchronous languages allow a high level, concurrent, and deterministic description of the behavior of reactive systems. Thus, they can be used advantageously for the programming of embedded control systems. The runtime requirements of synchronous code are light, but several critical properties must be fulfilled. In this paper, we address the problem of the software implementation of synchronous programs. After a brief introduction to reactive systems, this paper formalizes the notion of 'execution machine' for synchronous code. Then, a generic architecture for centralized execution machines is introduced. Finally, several effective implementations are presented. © 2001 IEEE.
Engineering Village;2002;eModel: Addressing the need for a flexible modeling framework in autonomic computing;;The paper describes a novel, flexible framework, eModel, designed to address the runtime requirements of autonomic computing: on-line workload measurement, analysis, and prediction. The eModel architecture has been developed using platform independent technology (XML and Java) to allow for maximum portability while also allowing for ease-of-integration with existing measurement and system management tools. The eModel toolkit consists of a GUI based model builder tool, a data base deployment tool, a runtime tool, and an analysis tool. In addition to the toolkit, the eModel design provides a runtime architecture which can be deployed directly without using any interaction with the GUI. The architecture is flexible enough to allow for incorporation with models of various complexity, including modeling techniques that require a hierarchical approach to attain reasonable accuracy based upon on-line, measured data. We present examples that illustrate eModel as a capacity planning tool as well as an augmentation to autonomic system management in an effort to highlight the technological gaps that the eModel framework is capable of bridging. © 2002 IEEE.
Engineering Village;2008;Becoming responsive to service usage and performance changes by applying service feedback metrics to software maintenance;;Software vendors are unaware of how their software performs in the field. They do not know what parts of their software are used and appreciated most and have little knowledge about the behavior of the software and its environment. In this paper we present a metrics-based approach that is used by software vendors to create real-time usage reports, based on data gathered by leveraging aspect-oriented programming techniques. This approach enables software vendors to respond quickly to performance and usage changes in their service software, both at specific customers and concerning the service software in general. We show that by using this approach, vendors can make informed decisions with respect to software requirements management and maintenance. The metrics and usage reports are validated by way of a case study at a Dutch software vendor. While validation shows high potential of the approach, a successful implementation will require change management at the software vendor.
Engineering Village;2014;A catalogue of optimization techniques for Triple Graph Grammars;;Bidirectional model transformation languages are typically declarative, being able to provide unidirectional operationalizations from a common specification automatically. Declarative languages have numerous advantages, but ensuring runtime efficiency, especially without any knowledge of the underlying transformation engine, is often quite challenging. Triple Graph Grammars (TGGs) are a prominent example for a completely declarative, bidirectional language and have been successfully used in various application scenarios. Although an optimization phase based on profiling results is often a necessity to meet runtime requirements, there currently exists no systematic classification and evaluation of optimization strategies for TGGs, i.e., the optimization process is typically an ad-hoc process. In this paper, we investigate the runtime scalability of an exemplary bidirectional model-to-text transformation. While systematically optimizing the implementation, we introduce, classify and apply a series of optimization strategies. We provide in each case a quantitative measurement and qualitative discussion, establishing a catalogue of current and future optimization techniques for TGGs in particular and declarative rule-based model transformation languages in general.
Engineering Village;1998;A sequential detailed router for huge grid graphs;;Sequential routing algorithms using maze-running are very suitable for general over-the-cell-routing but suffer often from the high memory or runtime requirements of the underlying path search routine. A new algorithm for this subproblem is presented that computes shortest paths in a rectangular grid with respect to euclidean distance. It achieves performance and memory requirements similar to fast line-search algorithms while still being optimal. An additional application for the computation of minimal rip-up sets is presented. Computational results are shown for a detailed router based on these algorithms that is used for the design of high performance CMOS processors at IBM. © 1998 IEEE.
Engineering Village;2008;From monitoring templates to security monitoring and threat detection;;This paper presents our pattern-based approach to runtime requirements monitoring and threat detection being developed as part of an approach to build frameworks supporting the construction of secure and dependable systems for ambient intelligence. Our patterns infra-structure is based on templates. From templates we generate Event-Calculus formulas expressing security requirements to monitor at run-time. From these theories we generate attack signatures, describing threats or possible attacks to the system. At run-time, we evaluate the likelihood of threats from run-time observations using a probabilistic model based on Bayesian networks. © 2008 IEEE.
Engineering Village;2007;A process splitting transformation for Kahn process networks;;In this paper we present a process splitting transformation for Kahn process networks. Running applications written in this parallel program specification on a multiprocessor architecture does not guarantee that the runtime requirements are met. Therefore, it may be necessary to further analyze and optimize Kahn process networks. In this paper, we will present a four-step transformation that results in a functionally equivalent process network, but with a changed and optimized network structure. The class of networks that can be handled is not restricted to static networks. The novelty of this approach is that it can also handle processes with dynamic program statements. We will illustrate the transformation prototyped in GCC for a JPEG decoder, showing a 21% performance improvements. © 2007 EDAA.
Engineering Village;2006;Preprocessing the MAP problem;;The MAP problem for Bayesian networks is the problem of finding for a set of variables an instantiation of highest posterior probability given the available evidence. The problem is known to be computationally infeasible in general. In this paper, we present a method for preprocessing the MAP problem with the aim of reducing the runtime requirements for its solution. Our method exploits the concepts of Markov and MAP blanket for deriving partial information about a solution to the problem. We investigate the practicability of our preprocessing method in combination with an exact algorithm for solving the MAP problem for some real Bayesian networks.
Engineering Village;2014;Supporting elasticity in OpenMP applications;;Elasticity can be seen as the ability of a system to increase or decrease the computing resources allocated in a dynamic and on demand way. In order to explore this feature, several works addressed the development of frameworks and platforms focusing the construction of elastic parallel and distributed applications for IaaS clouds. However, none of these works addressed the exploration of elasticity in multithreaded applications. In this paper, we propose a mechanism to provide elasticity support for OpenMP applications, making possible the dynamic provisioning of cloud resources taking into account the program structure and runtime requirements. In our proposal, the OpenMP directives were modified to support the dynamic adjustment of resources and a set of routines were included to the user-level library in order to enable the configuration of the the elastic execution. Dynamic memory allocation support was also included in elastic OpenMP library. We also present the architecture and implementation of the proposed mechanism. The experiments validate our approach and show some possibilities to use the elastic OpenMP. © 2014 IEEE.
Engineering Village;2010;Two-staged approach for semantically annotating and brokering TV-related services;;Nowadays, more and more distributed digital TV and TV-related resources are published on the Web, such as Electronic Personal TV Guide (EPG) data. To enable applications to access these resources easily, the TV resource data is commonly provided by Web service technologies. The huge variety of data related to the TV domain and the wide range of services that provide it, raises the need to have a broker to discover, select and orchestrate services to satisfy the runtime requirements of applications that invoke these services. The variety of data and heterogeneous nature of the service capabilities makes it a challenging domain for automated web-service discovery and composition. To overcome these issues, we propose a two-stage service annotation approach, which is resolved by integrating Linked Services and IRS-III semantic web services framework, to complete the lifecycle of service annotating, publishing, deploying, discovering, orchestration and dynamic invocation. This approach satisfies both developer's and application's requirements to use Semantic Web Services (SWS) technologies manually and automatically. © 2010 IEEE.
Engineering Village;2004;A dot placement approach to stochastic screening using bitmasks;;FM or stochastic screening is a popular approach to halftoning for many applications. The error diffusion algorithm delivers extremely good screen quality but at the price of a computationally-intensive runtime process. Point processes, using either dither arrays or bitmask sets, have efficient runtime requirements but often produce halftones of much lower quality. The generation of such screens usually involves starting with a random pattern and applying some simulated annealing process to gradually improve its characteristics. This paper proposes a method for generating stochastic patterns that employs a dot placement algorithm in which each dot is placed in a position 'appropriate' for producing good stochastic output. The algorithm is then enhanced by applying a smoothing step at the end of each halftone pattern generation to adjust any dots that, due to the placement of later dots, are now in sub-optimal positions. Although the algorithm can be used to produce dither arrays, it is primarily aimed at generating bitmasks where the additional degree pattern freedom is exploited to improve pattern smoothness. The algorithm also permits second order stochastic patterns for use with imprecise print devices such as electro-photographic printers.
Engineering Village;2005;A note on an L-approach for solving the manufacturer's pallet loading problem;;An L-approach for packing (l,w)-rectangles into an (L,W)-rectangle was introduced in an earlier work by Lins, Lins and Morabito. They conjecture that the L-approach is exact and point out its runtime requirements as the main drawback. In this note it is shown that, by simply using a different data structure, the runtime is considerably reduced in spite of larger (but affordable) memory requirements. This reduction is important for practical purposes since it makes the algorithm much more acceptable for supporting actual decisions in pallet loading. Intensive numerical experiments showing the efficiency and effectiveness of the algorithm are presented © 2005 Operational Research Society Ltd. All rights reserved.
Engineering Village;2015;Evaluating heuristic optimization, bio-inspired and graph-theoretic algorithms for the generation of fault-tolerant graphs with minimal costs;;The construction of fault-tolerant graphs is a trade-off between costs and degree of fault-tolerance. Thus the construction of such graphs can be viewed as a two-criteria optimization problem. Any algorithm therefore should be able to generate a Pareto-front of graphs so that the right graph can be chosen to match the application and the user’s need. In this work algorithms from three different domains for the construction of fault-tolerant graphs are evaluated. Classical graph-theoretic algorithms, optimization and bio-inspired approaches are compared regarding the quality of the generated graphs as well as concerning the runtime requirements. As result, recommendations for the application of the right algorithm for a certain problem class can be concluded. © Springer-Verlag Berlin Heidelberg 2015.
Engineering Village;1975;PARSING ALGORITHMS ANALYSIS.;;Two bottom-up shift-and-reduce parsing algorithms, simple precedence and LR(k) (k equals 1,2), are analyzed. These two are coded into MIXAL programs. (MIXAL is the assembly language for an imaginary computer MIX). Memory and runtime requirements are measured. Runtime estimation is made in two different ways, and the two algorithms are compared.
Engineering Village;2014;Phase distance mapping: a phase-based cache tuning methodology for embedded systems;;Networked embedded systems typically leverage a collection of low-power embedded systems (nodes) to collaboratively execute applications spanning diverse application domains (e.g., video, image processing, communication, etc.) with diverse application requirements. The individual networked nodes must operate under stringent constraints (e.g., energy, memory, etc.) and should be specialized to meet varying applications’ requirements in order to adhere to these constraints. Phase-based tuning specializes a system’s tunable parameters to the varying runtime requirements of an application’s different phases of execution to meet optimization goals. Since the design space for tunable systems can be very large, one of the major challenges in phase-based tuning is determining the best configuration for each phase without incurring significant tuning overhead (e.g., energy and/or performance) during design space exploration. In this paper, we propose phase distance mapping, which directly determines the best configuration for a phase, thereby eliminating design space exploration. Phase distance mapping applies the correlation between a known phase’s characteristics and best configuration to determine a new phase’s best configuration based on the new phase’s characteristics. Experimental results verify that our phase distance mapping approach, when applied to cache tuning, determines cache configurations within 1 % of the optimal configurations on average and yields an energy delay product savings of 27 % on average. © 2014, Springer Science+Business Media New York.
Engineering Village;2012;A bounded error, anytime, parallel algorithm for exact Bayesian network structure learning;;Bayesian network structure learning is NP-hard. Several anytime structure learning algorithms have been proposed which guarantee to learn optimal networks if given enough resources. In this paper, we describe a general purpose, anytime search algorithm with bounded error that also guarantees optimality. We give an efficient, sparse representation of a key data structure for structure learning. Empirical results show our algorithm often finds better networks more quickly than state of the art methods. They also highlight accepting a small, bounded amount of suboptimality can reduce the memory and runtime requirements of structure learning by several orders of magnitude.
Engineering Village;2014;Automated assessment of pleural thickening: Towards an early diagnosis of pleuramesothelioma;;Assessment of the growth of pleural thickenings is crucial for an early diagnosis of pleuramesothelioma. The presented automatic system supports the physician in comparing two temporally consecutive CT data-sets to determine this growth. The algorithms perform the determination of the pleural contours. After surface-based smoothing, anisotropic diffusion, a model-oriented probabilistic classification specifies the thickening’s tissue. The volume of each detected thickening is determined. While doctors still have the possibility to supervise the detection results, a full automatic registration carries out the matching of the same thickenings in two consecutive datasets to fulfill the change follow-up, where manual control is still possible thereafter. All algorithms were chosen and designed to meet runtime requirements, which allow an application in the daily routine. © Springer-Verlag Berlin Heidelberg 2014.
Engineering Village;2011;Self-adaptability in secure embedded systems: An energy-performance trade-off;;Securing embedded systems is a challenging and important research topic due to limited computational and memory resources. Moreover, battery powered embedded systems introduce power constraints that make the problem of deploying security more difficult. This problem may be addressed by improving the trade-off between minimizing energy consumption and maintaining a proper security level. This paper proposes an energy-aware method to determine the security resources consistent with the requirements of the system. The proposed solution is based on a multi-criteria decision mechanism, the Weighted Product Model (WPM), used to evaluate the relations between different security solutions and to select the appropriate one based on variable runtime requirements.
Engineering Village;2007;SLA-based advance reservations with flexible and adaptive time QoS parameters;;Utility computing enables the use of computational resources and services by consumers with service obligations and expectations defined in Service Level Agreements (SLAs). Parallel applications and workflows can be executed across multiple sites to benefit from access to a wide range of resources and to respond to dynamic runtime requirements. A utility computing provider has the difficult role of ensuring that all current SLAs are provisioned, while concurrently forming new SLAs and providing multiple services to numerous consumers. Scheduling to satisfy SLAs can result in a low return from a provider's resources due to trading off Quality of Service (QoS) guarantees against utilisation. One technique is to employ advance reservations so that an SLA aware scheduler can properly manage and schedule its resources. To improve system utilisation we exploit the principle that some consumers will be more flexible than others in relation to the starting or completion time, and that we can juggle the execution schedule right up until each execution starts. In this paper we present a QoS scheduler that uses SLAs to efficiently schedule advance reservations for computation services based on their flexibility. In our SLA model users can reduce or increase the flexibility of their QoS requirements over time according to their needs and resource provider policies. We introduce our scheduling algorithms, and show experimentally that it is possible to use flexible advance reservations to meet specified QoS while improving resource utilisation. © Springer-Verlag Berlin Heidelberg 2007.
Engineering Village;2009;A comparison of metaheuristics on a practial staff scheduling problem;;A practical scenario from logistics is used in this paper to compare different variants of particle swarm optimisation (PSO) and the evolution strategy (ES). Rapid, sub-daily planning with metaheuristics, which is the focus of our research, can significantly add to the improvement of staff scheduling in practice. PSO outperformes ES on this problem. The superior performance must be attributed to the operators and parameters of PSO since the coding of PSO and ES are identical. Repairing solutions to reduce the violation of soft constraints significantly improves the quality of results for both metaheuristics, although the runtime requirements are approximately doubled.
Engineering Village;2009;Property analysis of visual behavior models to code transformation;;Nowadays, when visual modeling is becoming more and more popular, it is still an open issue how to model the runtime behavior (animation) of visual languages. We are currently working on a complete solution to this issue, we have specified visual languages that can describe the behavior of arbitrary metamodeled visual language and we have also provided a graph-rewriting-based transformation which processes these 'animation' models and generates executable source code. This paper shortly introduces previous work, and focuses on the analysis of the runtime properties of the transformation. We performed termination analysis on the transformation, and examined the runtime requirements of the algorithm, based on the size of the input models. We have also verified that the transformation processes topologically correct models only. We present generic techniques which are applicable not only in connection with this concrete case, but with arbitrary other graph-rewriting based model transformations.
Engineering Village;2007;High performance database searching with HMMer on FPGAs;;Profile Hidden Markov Models (profile HMMs) are used as a popular bioinformatics tool for sensitive database searching, e.g. a set of not annotated protein sequences is compared to a database of profile HMMs to detect functional similarities. HMMer is a commonly used package for profile HMM-based methods. However, searching large databases with HMMer suffers from long runtimes on traditional computer architectures. These runtime requirements are likely to become even more severe due to the rapid growth in size of both sequence and model databases. In this paper, we present a new reconfigurable architecture to accelerate HMMer database searching. It is described how this leads to significant runtime savings on off-the-shelf field-programmable gate arrays (FPGAs). © 2007 IEEE.
Engineering Village;2013;Semantic issues in model-driven management of information system interoperability;;The Mediation Information System Engineering (MISE Project) aims at providing collaborating organisations with a mediation information system (MIS) in charge of supporting interoperability of a collaborative network. The MISE proposes an overall MIS design method according to a model-driven approach, based on model transformations. This MIS is in charge of managing (i) information, (ii) functions and (iii) processes among the information systems (IS) of partner organisations involved in the network. Semantic issues are accompanying this triple objective: How to deal with information reconciliation? How to ensure the matching between business functions and technical services? How to identify workflows among business processes? This article aims first at presenting the MISE approach, second at defining the semantic gaps along the MISE approach and third at describing some past, current and future research works that deal with these issues. Finally and as a conclusion, the very design-oriented previous considerations are confronted with 'runtime' requirements. © 2013 Copyright Taylor and Francis Group, LLC.
Engineering Village;2005;Construction of weighted finite state transducers for very wide context-dependent acoustic models;;A previous paper by the authors described an algorithm for efficient construction of Weighted Finite State Transducers for speech recognition when high-order context-dependent models of order K > 3 (triphones) with tied state observation distributions are used, and showed practical application of the algorithm up to K = 5 (quinphones). In this paper we give additional details of the improved implementation and analyze the algorithm's practical runtime requirements and memory footprint for context-orders up to K = 13 (+/-6 phones context) when building fully cross-word capable WFSTs for large vocabulary speech recognition tasks. We show that for typical systems it is possible to use any practical context-order K &le 13 without having to fear an exponential explosion of the search space, since the necessary state ID to phone transducer (resembling a phone-loop observing all possible K-phone constraints) can be built in a few minutes at most. The paper also gives some implementation details of how we efficiently collect context statistics and build phonetic decision trees for very wide context-dependent acoustic models. © 2005 IEEE.
Engineering Village;2010;Hybrid automata, reachability, and Systems Biology;;Hybrid automata are a powerful formalism for the representation of systems evolving according to both discrete and continuous laws. Unfortunately, undecidability soon emerges when one tries to automatically verify hybrid automata properties. An important verification problem is the reachability one that demands to decide whether a set of points is reachable from a starting region. If we focus on semi-algebraic hybrid automata the reachability problem is semi-decidable. However, high computational costs have to be afforded to solve it. We analyse this problem by exploiting some existing tools and we show that even simple examples cannot be efficiently solved. It is necessary to introduce approximations to reduce the number of variables, since this is the main source of runtime requirements. We propose some standard approximation methods based on Taylor polynomials and ad hoc strategies. We implement our methods within the software SAHA-Tool and we show their effectiveness on two biological examples: the Repressilator and the Delta-Notch protein signaling. © 2010 Elsevier B.V. All rights reserved.
Engineering Village;2008;GPU-MEME: Using graphics hardware to accelerate motif finding in DNA sequences;;Discovery of motifs that are repeated in groups of biological sequences is a major task in bioinformatics. Iterative methods such as expectation maximization (EM) are used as a common approach to find such patterns. However, corresponding algorithms are highly compute-intensive due to the small size and degenerate nature of biological motifs. Runtime requirements are likely to become even more severe due to the rapid growth of available gene transcription data. In this paper we present a novel approach to accelerate motif discovery based on commodity graphics hardware (GPUs). To derive an efficient mapping onto this type of architecture, we have formulated the compute-intensive parts of the popular MEME tool as streaming algorithms. Our experimental results show that a single GPU allows speedups of one order of magnitude with respect to the sequential MEME implementation. Furthermore, parallelization on a GPU-cluster even improves the speedup to two orders of magnitude. © 2008 Springer Berlin Heidelberg.
Engineering Village;2011;Toward a data logging data interchange format: Use cases and requirements;;Many use cases for distributed simulation depend on the effective analysis of simulation data after the simulation has completed, sometimes even years later. While many proprietary data loggers exist, logs are stored in proprietary formats often tailored for the specific use case for which each tool was designed and the specific data model used in a given simulation. This paper suggests that it is both possible and desirable to exchange, archive, and reuse simulation event log data using a standardized format for data interchange. The authors propose that such a format should be developed. There are several differences in the requirements between runtime formats and interchange log formats, with long-term reusability trumping runtime requirements for performance and space efficiency. Finally some solutions are suggested for several of the identified technical challenges. These include support for arbitrary data models, simple yet expressive metadata, log size, and complexity. Some use cases for which the suggested format would be most useful are also given.
Engineering Village;2009;Robust on-line model-based object detection from range images;;A mobile robot that accomplishes high level tasks needs to be able to classify the objects in the environment and to determine their location. In this paper, we address the problem of online object detection in 3D laser range data. The object classes are represented by 3D point-clouds that can be obtained from a set of range scans. Our method relies on the extraction of point features from range images that are computed from the point-clouds. Compared to techniques that directly operate on a full 3D representation of the environment, our approach requires less computation time while retaining the robustness of full 3D matching. Experiments demonstrate that the proposed approach is even able to deal with partially occluded scenes and to fulfill the runtime requirements of online applications. © 2009 IEEE.
Engineering Village;2008;Accelerating molecular dynamics simulations using Graphics Processing Units with CUDA;;Molecular dynamics is an important computational tool to simulate and understand biochemical processes at the atomic level. However, accurate simulation of processes such as protein folding requires a large number of both atoms and time steps. This in turn leads to huge runtime requirements. Hence, finding fast solutions is of highest importance to research. In this paper we present a new approach to accelerate molecular dynamics simulations with inexpensive commodity graphics hardware. To derive an efficient mapping onto this type of computer architecture, we have used the new Compute Unified Device Architecture programming interface to implement a new parallel algorithm. Our experimental results show that the graphics card based approach allows speedups of up to factor nineteen compared to the corresponding sequential implementation. © 2008 Elsevier B.V. All rights reserved.
Engineering Village;2005;Semantic and syntactic modeling of component-based services for context-aware pervasive systems using OWL-s;;In this paper, we present a service design methodology and specification as a basis for a pervasive context-aware service infrastructure. The service specification is based on the OWL-s specification, a standard proposed to add a semantic layer on top of WSDL web service descriptions. We have defined a set of OWL-s concepts that make it possible to express various pervasive service related properties, including service adaptation, relocation, personalization, deployment and runtime requirements. Though not straightforward to parse within strict resource boundaries, OWL-s provides an open, flexible specification language for expressing syntactic and semantic pervasive service characteristics and it increases service interoperability.
Engineering Village;2008;Integrating FPGA acceleration into HMMer;;HMMer is a commonly used package for biological sequence database searching with profile hidden Markov model (HMMs). It allows researchers to compare HMMs to sequence databases or sequences to HMM databases. However, such searches often take many hours on traditional computer architectures. These runtime requirements are likely to become even more severe due to the rapid growth in size of both sequence and model databases. We present a new reconfigurable architecture to accelerate the two HMMer database search procedures hmmsearch and hmmpfam. It is described how this leads to significant runtime savings on off-the-shelf field-programmable gate arrays (FPGAs). © 2008 Elsevier B.V. All rights reserved.
Engineering Village;2014;Parallel fast multipole boundary element method for crustal dynamics;;Crustal faults and sharp material transitions in the crust are usually represented as triangulated surfaces in structural geological models. The complex range of volumes separating such surfaces is typically three-dimensionally meshed in order to solve equations that describe crustal deformation with the finite-difference (FD) or finite-element (FEM) methods. We show here how the Boundary Element Method, combined with the Multipole approach, can revolutionise the calculation of stress and strain, solving the problem of computational scalability from reservoir to basin scales. The Fast Multipole Boundary Element Method (Fast BEM) tackles the difficulty of handling the intricate volume meshes and high resolution of crustal data that has put classical Finite 3D approaches in a performance crisis. The two main performance enhancements of this method: the reduction of required mesh elements from cubic to quadratic with linear size and linear-logarithmic runtime,, achieve a reduction of memory and runtime requirements allowing the treatment of a new scale of geodynamic models. This approach was recently tested and applied in a series of papers by [1, 2, 3] for regional and global geodynamics, using KD trees for fast identification of near and far-field interacting elements, and MPI parallelised code on distributed memory architectures, and is now in active development for crustal dynamics. As the method is based on a free-surface, it allows easy data transfer to geological visualisation tools where only changes in boundaries and material properties are required as input parameters. In addition, easy volume mesh sampling of physical quantities enables direct integration with existing FD/FEM code. © 2010 IOP Publishing Ltd.
Engineering Village;2014;Computation scalable disparity estimation for delay sensitive 3D video surveillance system;;Disparity estimation is an important task in many 3D video surveillance applications. How to generate the disparity information at the front end under limited delay budget is very challenging in a real-time surveillance system. In this paper, we tackle this problem through adopting multiresolution strategy in the disparity estimation process. Our contribution is twofold. First, unlike existing coarse-to-fine strategies based on uniform sampling, we present a fast disparity estimation algorithm based on nonuniform sampling at the coarse level. The disparity values of the non-samples are initially interpolated through Delaunay Triangulation, and then are refined through bilateral filtering. The content-aware nonuniform sampling provides better disparity approximation in the triangulated interpolation, and consequently leads to faster convergence in the refinement procedure. Second, we model the computation time in the disparity estimation process for execution assistance in the delay sensitive surveillance system. Both the sampling cell size and the data resolution are considered in this model, in order to accommodate different runtime requirements. Experimental results demonstrate the efficiency of the proposed scheme. © 2014 IEEE.
Engineering Village;2011;DSX: A knowledge-based scoring function for the assessment of protein-ligand complexes;;We introduce the new knowledge-based scoring function DSX that consists of distance-dependent pair potentials, novel torsion angle potentials, and newly defined solvent accessible surface-dependent potentials. DSX pair potentials are based on the statistical formalism of DrugScore, extended by a much more specialized set of atom types. The original DrugScore-like reference state is rather unstable with respect to modifications in the used atom types. Therefore, an important method to overcome this problem and to allow for robust results when deriving pair potentials for arbitrary sets of atom types is presented. A validation based on a carefully prepared test set is shown, enabling direct comparison to the majority of other popular scoring functions. Here, DSX features superior performance with respect to docking- and ranking power and runtime requirements. Furthermore, the beneficial combination with torsion angle-dependent and desolvation-dependent potentials is demonstrated. DSX is robust, flexible, and capable of working together with special features of popular docking engines, e.g., flexible protein residues in AutoDock or GOLD. The program is freely available to the scientific community and can be downloaded from our Web site www.agklebe.de. © 2011 American Chemical Society.
Engineering Village;2006;Impact of virtual execution environments on processor energy consumption and hardware adaptation;;During recent years, microprocessor energy consumption has been surging and efforts to reduce power and energy have received a lot of attention. At the same time, virtual execution environments (VEEs), such as Java virtual machines, have grown in popularity. Hence, it is important to evaluate the impact of virtual execution environments on microprocessor energy consumption. This paper characterizes the energy and power impact of two important components of VEEs, Just-in-time (JIT) optimization and garbage collection. We find that by reducing instruction counts, JIT optimization significantly reduces energy consumption, while garbage collection incurs runtime overhead that consumes more energy. Importantly, both JIT optimization and garbage collection decrease the average power dissipated by a program. Detailed analysis reveals that both JIT optimizer and JIT optimized code dissipate less power than un-optimized code. On the other hand, being memory bound and with low ILP, the garbage collector dissipates less power than the application code, but rarely affects the average power of the latter. Adaptive microarchitectures are another recent trend for energy reduction where microarchitectural resources can be dynamically tuned to match program runtime requirements. This research reveals that both JIT optimization and garbage collection alter a program's behavior and runtime requirements, which considerably affects the adaptation of configurable hardware units, and influences the overall energy consumption. This work also demonstrates that the adaptation preferences of the two VEE services differ substantially from those of the application code. Both VEE services prefer a simple core for high energy reduction. On the other hand, the JIT optimizer usually requires larger data caches, while the garbage collector rarely benefits from large data caches. The insights gained in this paper point to novel techniques that can further reduce microprocessor energy consumption. Copyright © 2006 ACM.
Engineering Village;2004;Rapid computation of dynamic stability derivatives;;A new technique for rapid computation of dynamic stability derivatives with steady Euler CFD codes is presented. The approach is analogous to that used in linear methods. It treats the rotational velocity as a perturbation on the steady-state solution. Standard thermodynamic relations from compressible, inviscid fluid dynamics are used to derive the modified forces and moments resulting from the generalized rotation. The coefficient of the linear term in the rotation rate is related to the desired stability derivative. Additional terms necessary to alter the center of rotation and the moment reference are also derived. All calculations are performed at the same time as other force and moment accounting. Thus, there is little impact on overall runtime requirements. Results of this model are compared with time-accurate rotating solutions, linear methods, and experimental data. The method produces results that can be characterized as having accuracy similar to that claimed for the traditional linear methods. Such accuracy is satisfactory for most design efforts.
Engineering Village;2010;Parallel Fast Multipole Boundary Element Method for crustal dynamics;;Crustal faults and sharp material transitions in the crust are usually represented as triangulated surfaces in structural geological models. The complex range of volumes separating such surfaces is typically three-dimensionally meshed in order to solve equations that describe crustal deformation with the finite-difference (FD) or finite-element (FEM) methods. We show here how the Boundary Element Method, combined with the Multipole approach, can revolutionise the calculation of stress and strain, solving the problem of computational scalability from reservoir to basin scales. The Fast Multipole Boundary Element Method (Fast BEM) tackles the difficulty of handling the intricate volume meshes and high resolution of crustal data that has put classical Finite 3D approaches in a performance crisis. The two main performance enhancements of this method: the reduction of required mesh elements from cubic to quadratic with linear size and linear-logarithmic runtime,, achieve a reduction of memory and runtime requirements allowing the treatment of a new scale of geodynamic models. This approach was recently tested and applied in a series of papers by [1, 2, 3] for regional and global geodynamics, using KD trees for fast identification of near and far-field interacting elements, and MPI parallelised code on distributed memory architectures, and is now in active development for crustal dynamics. As the method is based on a free-surface, it allows easy data transfer to geological visualisation tools where only changes in boundaries and material properties are required as input parameters. In addition, easy volume mesh sampling of physical quantities enables direct integration with existing FD/FEM code. © 2010 IOP Publishing Ltd.
Engineering Village;2014;Accurate crop classification using hierarchical genetic fuzzy rule-based systems;;This paper investigates the effectiveness of an advanced classification system for accurate crop classification using very high resolution (VHR) satellite imagery. Specifically, a recently proposed genetic fuzzy rule-based classification system (GFRBCS) is employed, namely, the Hierarchical Rule-based Linguistic Classifier (HiRLiC). HiRLiC's model comprises a small set of simple IF-THEN fuzzy rules, easily interpretable by humans. One of its most important attributes is that its learning algorithm requires minimum user interaction, since the most important learning parameters affecting the classification accuracy are determined by the learning algorithm automatically. HiRLiC is applied in a challenging crop classification task, using a SPOT5 satellite image over an intensively cultivated area in a lake-wetland ecosystem in northern Greece. A rich set of higher-order spectral and textural features is derived from the initial bands of the (pan-sharpened) image, resulting in an input space comprising 119 features. The experimental analysis proves that HiRLiC compares favorably to other interpretable classifiers of the literature, both in terms of structural complexity and classification accuracy. Its testing accuracy was very close to that obtained by complex state-of-the-art classification systems, such as the support vector machines (SVM) and random forest (RF) classifiers. Nevertheless, visual inspection of the derived classification maps shows that HiRLiC is characterized by higher generalization properties, providing more homogeneous classifications that the competitors. Moreover, the runtime requirements for producing the thematic map was orders of magnitude lower than the respective for the competitors. © 2014 SPIE.
Engineering Village;2014;Optimizing multi-objective evolutionary algorithms to enable quality-aware software provisioning;;Elasticity is a key feature for cloud infrastructures to continuously align allocated computational resources to evolving hosted software needs. This is often achieved by relaxing quality criteria, for instance security or privacy because quality criteria are often conflicting with performance. As an example, software replication could improve scalability and uptime while decreasing privacy by creating more potential leakage points. The conciliation of these conflicting objectives has to be achieved by exhibiting trade-offs. Multi-Objective Evolutionary Algorithms (MOEAs) have shown to be suitable candidates to find these trade-offs and have been even applied for cloud architecture optimizations. Still though, their runtime efficiency limits the widespread adoption of such algorithms in cloud engines, and thus the consideration of quality criteria in clouds. Indeed MOEAs produce many dead-born solutions because of the Darwinian inspired natural selection, which results in a resources wastage. To tackle MOEAs efficiency issues, we apply a process similar to modern biology. We choose specific artificial mutations by anticipating the optimization effect on the solutions instead of relying on the randomness of natural selection. This paper introduces the Sputnik algorithm, which leverages the past history of actions to enhance optimization processes such as cloud elasticity engines. We integrate Sputnik in a cloud elasticity engine, dealing with performance and quality criteria, and demonstrate significant performance improvement, meeting the runtime requirements of cloud optimization.
Engineering Village;2000;Hierarchical error detection in a software implemented fault tolerance (SIFT) environment;;This paper proposes a hierarchical error detection framework for a Software Implemented Fault Tolerance (SIFT) layer of a distributed system. A four-level error detection hierarchy is proposed in the context of Chameleon, a software environment for providing adaptive fault-tolerance in an environment of commercial off-the-shelf (COTS) system components and software. The design and implementation of a software-based distributed signature monitoring scheme, which is central to the proposed four-level hierarchy, is described. Both intralevel and interlevel optimizations that minimize the overhead of detection and are capable of adapting to runtime requirements are proposed. The paper presents results from a prototype implementation of two levels of the error detection hierarchy and results of a detailed simulation of the overall environment. The results indicate a substantial increase in availability due to the detection framework and help in understanding the trade-offs between overhead and coverage for different combinations of techniques.
Engineering Village;2007;Delay optimization for airspace capacity management with runtime and equity considerations;;En route air traffic management is difficult and can benefit greatly from decision support tools. This paper presents a study of the efficiency and effectiveness of two practical approaches to real-time scheduling algorithms: a simple greedy scheduler and a well-studied optimal scheduler. A subset (region) of the National Airspace System is isolated to perform optimization on a manageable portion of the airspace. The schedulers are tested on realistic data sets representing traffic and conditions in the corridor between Chicago and New York area airports. In particular, the optimal scheduling of flights with both origin and destination in that corridor is considered, while reserving sufficient airspace for other air traffic. In a majority of cases, the greedy method provides sufficient (often optimal) results, while under difficult traffic and weather conditions, the optimal scheduler is worth the runtime requirements due to the inability of the greedy version to find satisfactory solutions. Further benefits of an optimal scheduler are demonstrated by incorporating the concept of equity or 'fairness' into the scheduling decision. Design choices in implementing equity amongst the various airlines are discussed and results demonstrating the utility of these choices are provided. Equity is easily implemented for the optimal scheduler but not the greedy, and does not require significant additional run time. Ultimately, it is shown that an equity-aware decision support tool for delay optimization can be developed to run in real-time and can benefit from incorporating more than one approach depending on the complexity of the scenario.
Engineering Village;2000;An initial comparison of National Combustor Code simulations using various chemistry modules with experimental gas turbine Combustor Data;;The National Combustion Code (NCC) is a state of the art computational fluid dynamics code specifically designed to simulate gas turbine combustors. The NCC was given the geometry and boundary conditions from an Allied Signal gas turbine combustor. The Allied Signal combustor was an annular, mixed flow, swirler type configuration that was fueled by natural gas. The NCC was run to convergence using a variety of combustion chemistry modules that used. The chemistry models that were used were the Magnussen eddy dissipation model and the intrinsic low.dimensional manifolds (ILDM) model. The computational results were compared to experimental results for the Allied Signal combustor. For each chemistry model used, NCC computer runtime requirements and ease of use issues are discussed.
Engineering Village;2004;A configurable XForms implementation;;XForms is a new language for defining dynamic forms and user interfaces for the World Wide Web. In order to take advantage of the user interaction related features in the language, a client side processor is needed. This paper describes a configurable open source software implementation of XForms. The main goal of the implementation is to conform to the World Wide Web Consortium's XForms Recommendation. The other goals are external to the XForms specification and are related to the portability and configurability of the processor. The important questions are related to implementing an XForms processor for diverse environments, and the integration of XForms and other XML languages with different layout models. In the paper, more detailed requirements are gathered from these goals. Also, the design and implementation are presented in detail, in order to give insight to the more difficult and non-obvious parts of the software. The results of the paper cover the runtime requirements of the XForms processor. © 2004 IEEE.
Engineering Village;2009;Frequent itemsets hiding: A performance evaluation framework;;Sensitive knowledge hiding is an essential requirement to prevent disclosure of any sensitive knowledge holding in shared databases. The security of a database may be risked when it is made public as is: because the data mining tools are so sophisticated that the sensitive knowledge can easily be surfaced by receivers. This gives rise to a sanitization process which transforms the original database into another database, the released one, which does not hold the sensitive knowledge but can substitute the original otherwise. In case the sensitive knowledge is of the form frequent itemsets, the resulting concrete problem is called frequent itemsets hiding. A number of algorithms, exploiting different approaches and techniques, for frequent itemsets hiding problem is proposed in the literature. Since finding optimal solutions is NP-Hard, algorithms resort to certain heuristics having different levels of sophistication, complexity, efficiency and effectiveness. This paper presents an evaluation framework which implements recent algorithms belonging to different approaches and a set of metrics to gauge the performance and problem difficulties. The current work also presents an experimental study and its results where four algorithms and seven datasets are involved. Our results indicate that data distortion levels and runtime requirements are quite high, especially for difficult problem instances. Our conclusion is that there are new rooms for more sophisticated and tuneable (w.r.t. effectiveness/efficiency tradeoff) algorithms. © 2009 IEEE.
Engineering Village;2011;CleGo: A package for automated computation of Clebsch-Gordan coefficients in tensor product representations for Lie algebras A-G;;We present a program that allows for the computation of tensor products of irreducible representations of Lie algebras A-G based on the explicit construction of weight states. This straightforward approach (which is slower and more memory-consumptive than the standard methods to just calculate dimensions of the tensor product decomposition) produces Clebsch-Gordan coefficients that are of interest for instance in discussing symmetry breaking in model building for grand unified theories. For that purpose, multiple tensor products have been implemented as well as means for analyzing the resulting effective operators in particle physics. Program summary: Program title: CleGo Catalogue identifier: AEIQ-v1-0 Program summary URL: http://cpc.cs.qub.ac.uk/ summaries/AEIQ-v1-0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 3641 No. of bytes in distributed program, including test data, etc.: 34 536 Distribution format: tar.gz Programming language: O'Caml Computer: i386-i686, x86-64 Operating system: Cross-platform, for definiteness though we assume some UNIX system. RAM: ≥4 GB commendable, though in general memory requirements depend on the size of the Lie algebras and the representations involved. Classification: 4.2, 11.1 Nature of problem: Clebsch-Gordan coefficients are widely used in physics. This program has been written as a means to analyze symmetry breaking in the context of grand unified theories in particle physics. As an example, we computed the singlets appearing in higher-dimensional operators 27⊗ 27⊗ 27⊗ 78 and 27⊗ 27⊗ 27⊗ 650 in an E6-symmetric GUT. Solution method: In contrast to very efficient algorithms that also produce tensor product decompositions (as far as outer multiplicities/Littlewood-Richardson coefficients are concerned) we proceed straightforwardly by constructing all the weight states, i.e. the Clebsch-Gordan coefficients. This obviously comes at the expense of high memory and CPU-time demands. Applying Dynkin arithmetic in weight space, the algorithm is an extension of the one for the addition of angular momenta in su(2) ≈ A1, for reference see [1]. Note that, in general, Clebsch-Gordan coefficients are basis-dependent and therefore need to be understood with respect to the chosen basis. However, singlets appearing in (multiple) tensor products are less basis-dependent. Restrictions: Generically, only tensor products of non-degenerate or adjoint representations can be computed. However, the irreps appearing therein can subsequently be used as new input irreps for further tensor product decomposition so in principle there is no restriction on the irreps the tensor product is taken of. In practice, though, it is by the very nature of the explicit algorithm that input is restricted by memory and CPU runtime requirements. Unusual features: Analytic computation instead of float numerics. Additional comments: The program can be used in 'notebook style' using a suitable O'Caml toplevel. Alternatively, an O'Caml input file can be compiled which results in processing that is approximately a factor of five faster. The latter mode is commendable when large irreps need to be constructed. Running time: Varies depending on the input from parts of seconds to weeks for very large representations (because of memory exhaustion). © 2011 Elsevier B.V. All rights reserved.
Engineering Village;2004;On the advantages of approximate vs. complete verification: Bigger models, faster, less memory, usually accurate;;We have been exploring LURCH, an approximate (not necessarily complete) alternative to traditional model checking based on a randomized search algorithm. Randomized algorithms like LURCH have been known to outperform their deterministic counterparts for search problems representing a wide range of applications. The cost of an approximate strategy is the potential for inaccuracy. If complete algorithms terminate, they find all the features they are searching for. On the other hand, by its very nature, randomized search can miss important features. Our experiments suggest that this inaccuracy problem is not too serious. In the case studies presented here and elsewhere, LURCHS random search usually found the correct results. Also, these case studies strongly suggest that LURCH can scale to much larger models than standard model checkers like NuSMV and SPIN. The two case studies presented in this paper are selected for their simplicity and their complexity. The simple problem of the dining philosophers has been widely studied. By making the dinner more crowded, we can compare the memory and runtimes of standard methods (SPIN) and LURCH. When hundreds of philosophers sit down to eat, both LURCH and SPIN can find the deadlock case. However, SPINS memory and runtime requirements can grow exponentially while LURCHS requirements stay quite low. Success with highly symmetric, automatically generated problems says little about the generality of a technique. Hence, our second example is far more complex: a real-world flight guidance system from Rockwell Collins. Compared to NuSMV, LURCH performed very well on this model. Our random search finds the vast majority of faults (close to 90%),, runs much faster (seconds and minutes as opposed to hours),, and uses very little memory (single digits to 10s of megabytes as opposed to 10s to 100s of megabytes). The rest of this paper is structured as follows. We begin with a theoretical rationale for why random search methods like LURCH can be incomplete, yet still successful. Next, we note that for a class of problems, the complete search of standard model checkers can be overkill. LURCH is then briefly introduced and our two case studies are presented. © 2004 IEEE.
Engineering Village;2008;Non-uniform yield optimization for integrated circuit layout considering global interactions;;In a previous work we have shown a yield optimization metric and a technique that considers the effects of several types of yield enhancement methods for a given layout. Those findings suggested that it is important to consider two types of yield tradeoffs, local tradeoffs where addressing one yield loss mechanism degrades others in the immediate vicinity of the correction (local optimization window), and global tradeoffs where the net effect of the correction can be fully accounted only when considering neighboring optimization windows. Such conclusion was derived from the fact that the locally optimized layouts did not completely realize the theoretically optimal yield, which was obtained from the assumption that global tradeoffs could be fully resolved. This work focuses in the contribution that such global tradeoffs have on the final yield score when accounted properly during the optimization. While the previous work focused only in selecting the corrections that locally improved the yield score1, this work evaluates the global interactions before and after a change, and the correction is only accepted if it improves the global score. While the global optimization requires a more expensive computational process, the intention of this work is to determine how close the optimal layout can be from its theoretical limit. Since the optimization is performed and evaluated under four different types of processes in which the failure mechanisms vary in relative importance, it is possible to derive conclusions as to the need of considering global effects when trading off runtime requirements with quality of the correction. © 2008 SPIE.
Engineering Village;2008;PEM fuel cells versus diesel generators which solution to pick?;;Hurricane Katrina and the pursuant ruling by the FCC has caused many Telecom operators to review and find solutions to provide extended runtime in critical areas of their network. In the past, lead acid batteries have dominated the backup power market for these sites and the power grid quality was considered robust. Katrina forced operators to look past this and it revealed that lead acid batteries typically only provide up to eight hours of backup for a site. For those sites where operators require extended runtime, a diesel generator was typically the only option. This paper will provide an overview of not only the diesel solution but also for a new solution that utilizes a Proton Exchange Membrane Fuel Cell (PEMFC) and premium lead acid batteries to provide extended runtime at the site Using a 5kW base station plant, we will examine the architecture and design solutions that will give operators well over 48 hours of backup and in some cases over 160 hours. The PEMFC solution can utilize advanced lead acid front terminal batteries to ease maintenance as well as a robust and cost effective PEMFC engine. Critical areas for the mainstream success of fuel cells include total solution costs, reliability, fuel supply, maintenance and Hydrogen storage. The comparative diesel will be the embedded 30kW solution that is normally found in sites throughout these applications. Issues associated with generator operation include noise, emissions, permitting, fuel supply and maintenance costs will all be examined. The goal will be to show a total cost of ownership for both solutions and educate operators about new approaches for their extended runtime requirements. ©2008 IEEE.
Engineering Village;2006;A genuine design manufacturability check for designers;;The design of integrated circuits (ICs) has been made possible by a simple contract between design and manufacturing: Manufacturing teams encapsulated their process capabilities into a set of design rules such as minimum width and spacing or overlap for each layer, and designers complied with these design rules to get a manufacturable IC. However, since the advent of 90nm technology, designers have to play by the new rules of sub-90nm technologies. The simple design rules have evolved into extremely complex, context-dependent rules. Minimum design rules have been augmented with many levels of yield-driven recommended guidelines. One of the main drivers behind these complex rules is the increase in optical proximity effects that are directly impacting systematic and parametric yields for sub-90nm designs. The design's sensitivity to optical proximity effects increases as features get smaller, however design engineers do not have visibility into the manufacturability of these features. A genuine design for manufacturing (DFM) solution for designers should provide a fast, easy-to-use and cost-effective solution that accurately predicts the designs sensitivity to shape variations throughout the design process. It should identify and reduce design sensitivity by predicting and reducing shape variations. The interface between manufacturing and design must provide designers with the right information to allow them to maximize the manufacturability of their design while shielding them from the effects of resolution enhancement technologies (RET) and manufacturing complexity. This solution should also protect the manufacturing know-how in the case of a fabless foundry flow. Currently, the interface between manufacturing and design solely relies on design rules that do not provide these capabilities. A common proposition for design engineers in predicting shape variation is to move the entire RET/OPC/ORC into the hands of the designer. However, this approach has several major practicality issues that make it unfeasible, even as a 'service' offered to designers: &bull Cost associated with replicating the flow on designer's desktop. &bull The ability of designers to understand RET/OPC and perform lithographic judgments. &bull Confidentiality of the recipes and lithographic settings, especially when working with a foundry. &bull The level of confidence the fab/foundry side has in accepting the resulting RET/OPC. &bull Runtime and data volume explosion. &bull The logistics of reflecting RET/OPC and manufacturing changes. &bull The ability to tie this capability to EDA optimization tools. In this paper we present a new technique and methodology that overcomes these hurdles and meets both the design and manufacturing requirements by providing a genuine DFM solution to designers. We outline a new manufacturing-to-design interface that has evolved from rule-based to model-based, and provides the required visibility to the designer on their design manufacturability. This approach is similar to other EDA approaches which have been used to successfully capture complex behavior by using a formulation that has a higher level of abstraction (for example, SPICE for transistor behavior). We will present how this unique approach uses this abstracted model to provide very accurate prediction of shape variations and at the same time, meet the runtime requirements for a smooth integration into the design flow at 90nm and below. This DFM technology enables designers to improve their design manufacturability, which reduces RET complexity, mask cost and time to volume, and increases the process window and yield.
Engineering Village;2012;Towards dynamic evolution of self-Adaptive systems based on dynamic updating of control loops;;Self-Adaptive systems, which enable runtime adaptation, are promising ways of dealing with environmental changes, including system intrusions or faults. Such software systems must modify themselves to better fit their environment. One of the main approaches to constructing such systems is to introduce multiple control loops. Software evolution is an essential activity for expanding this adaptation capability, and dynamic evolution has been envisaged as a way of systems adapting themselves at runtime. In this paper, we establish a development process to deal with dynamic evolution. We devise a goal model compiler to generate models for designing dynamic evolutions and a programming framework that supports dynamic deployment of control loops. We experimentally applied our approach to a system and discuss how our compiler and framework support dynamic evolution of self-Adaptive systems. © 2012 IEEE.
Engineering Village;2010;Live goals for adaptive service compositions;;Service compositions represent an important family of self-adaptive systems. Though many approaches for monitoring and adapting service compositions have already been proposed, a clear connection with the motivations for using such techniques is still missing. To this aim we address self-adaptation from requirements elicitation down to execution. In this paper, we propose to enrich existing goal models with adaptive goals, responsible for the actual evolution/adaptation of the goal model at runtime. We also translate the goal model with both conventional and adaptive goals, into the actual functionality provided by the system and the adaptation policies needed to make it self-adapt. © 2010 ACM.
Engineering Village;2013;Using goals and customizable services to improve adaptability of process-based service compositions;;When implementing (semi-)automatic business processes with services, engineers are facing two sources of variability. One source of variability are alternative refinements and decompositions of requirements. The other source of variability is that various (combinations of) services can be used to satisfy the same requirements. We suggest a method based on the use of a goal model and customizable services able to exploit these variabilities to design executable business process. This method improves the adaptability of the business process at runtime. We illustrate the contribution of our method with an example. © 2013 IEEE.
Engineering Village;2013;7th International Conference on Research Challenges in Information Science, RCIS 2013 - Conference Proceedings;;The proceedings contain 71 papers. The topics discussed include: towards a proposal to capture usability requirements through guidelines,, collaborative creativity in requirements engineering: analysis and practical advice,, handling requirements dependencies in agile projects: a focus group with agile software development practitioners,, goal formalization and classification for requirements engineering, fifteen years later,, pattern based methodology for UML profiles evolution management,, supporting organizational evolution by means of model-driven reengineering frameworks,, M2Flex: a process metamodel for flexibility at runtime,, model driven development of a generic data control engine: an industrial experience,, modeling temporality and subjectivity in ConML,, a framework for question answering and its application to business intelligence,, and adapting to uncertain and evolving enterprise requirements: the case of business-driven business intelligence.
Engineering Village;2014;6th Workshop on Software Engineering for Resilient Systems, SERENE 2014;;The proceedings contain 13 papers. The special focus in this conference is on Requirements engineering and re-engineering for resilience,, Design of trustworthy and intrusion-safe systems,, Formal and semi-formal techniques for verification and validation,, Quantitative approaches to ensuring resilience and Resilience prediction. The topics include: Community resilience engineering,, enhancing architecture design decisions evolution with group decision making principles,, the role of parts in the system behaviour,, automatic generation of description files for highly available services,, modelling resilience of data processing capabilities of CPS,, formal fault tolerance analysis of algorithms for redundant systems in early design stages,, on applying FMEA to SOAs,, verification and validation of a pressure control unit for hydraulic systems,, simulation testing and model checking,, advanced modelling, simulation and verification for future traffic regulation optimization,, using instrumentation for quality assessment of resilient software in embedded systems,, adaptive domain-specific service monitoring and combined error propagation analysis and runtime event detection in process-driven systems.
Engineering Village;2012;Engineering Secure Software and Systems - 4th International Symposium, ESSoS 2012, Proceedings;;The proceedings contain 14 papers. The topics discussed include: application-replay attack on Java cards: when the garbage collector gets confused,, supporting the development and documentation of ISO 27001 information security management systems through security requirements engineering approaches,, typed assembler for a RISC crypto-processor,, transversal policy conflict detection,, challenges in implementing an end-to-end secure protocol for Java ME-based mobile data collection in low-budget settings,, runtime enforcement of information flow security in tree manipulating processes,, formalisation and implementation of the XACML access control mechanism,, a task ordering approach for automatic trust establishment,, an idea of an independent validation of vulnerability discovery models,, a sound decision procedure for the compositionality of secrecy,, and plagiarizing smartphone applications: attack strategies and defense techniques.
Engineering Village;2007;Dynamic requirements specification for adaptable and open service-oriented systems;;It is not feasible to engineer requirements for adaptable and open service-oriented systems (AOSS) by specifying stakeholders' expectations in detail during system development. Openness and adaptability allow new services to appear at runtime so that ways in, and degrees to which the initial functional and nonfunctional requirements will be satisfied may vary at runtime. To remain relevant after deployment, the initial requirements specification ought to be continually updated to reflect such variation. Depending on the frequency of updates, this paper separates the requirements engineering (RE) of AOSS onto the RE for: individual services (Service RE), service coordination mechanisms (Coordination RE), and quality parameters and constraints guiding service composition (Client RE). To assist existing RE methodologies in dealing with Client RE, the Dynamic Requirements Adaptation Method (DRAM) is proposed. DRAM updates a requirements specification at runtime to reflect change due to adaptability and openness. © Springer-Verlag Berlin Heidelberg 2007.
Engineering Village;2012;How to design and deliver process context sensitive information: Concept and prototype;;Providing employees with relevant, context-specific information is crucial to achieve productivity and efficiency while executing business processes. Today, tools exist to model various aspects of organizations such as processes, organization structures, services, and their descriptions. However, there still exists a gap between information modeling on a conceptual level and information provision on a runtime level that hinders information dissemination and retrieval while employees execute processes. In daily business life, information workers demand for unstructured content to fulfill well-defined process steps. In this paper, we adapt an existing conceptual approach of process-driven information requirements engineering and present its prototypical implementation based on an industry-developed BPM product. Our solution therefore introduces 'information objects' and integrates these with process activities to model the users' information requirements at process runtime. In doing so, users are empowered to leverage context information such as documents, reports, or emails, while executing human steps in a process. © 2012 Springer-Verlag.
Engineering Village;2012;Proceedings - 2012 8th International Conference on the Quality of Information and Communications Technology, QUATIC 2012;;The proceedings contain 61 papers. The topics discussed include: quality-aware mashup composition: issues, techniques and tools,, using web quality models and questionnaires for web applications understanding and evaluation,, automatic event detection for software product quality monitoring,, a runtime quality measurement framework for cloud database service systems,, investigating the impact of personality and temperament traits on pair programming: a controlled experiment replication,, a metamodel-based approach for customizing and assessing agile methods,, model-driven development for requirements engineering: the case of goal-oriented approaches,, complex events specification for properties validation,, integrating test and risk management,, coordinating exceptions of java systems: implementation and formal verification,, using association rules to identify similarities between software datasets,, and developing a process assessment model for technological and business competencies on software development.
Engineering Village;2009;ASE2009 - 24th IEEE/ACM International Conference on Automated Software Engineering;;The proceedings contain 89 papers. The topics discussed include: a Petri net based debugging environment for QVT relations,, validating automotive control software using instrumentation-based verification,, semi-automated test planning for e-ID systems by using requirements clustering,, understanding the value of software engineering technologies,, evaluating the accuracy of fault localization techniques,, spectrum-based multiple fault localization,, clone-aware configuration management,, Looper: lightweight detection of infinite loops at runtime,, improving the efficiency of dependency analysis in logical decision models,, design rule hierarchies and parallelism in software development tasks,, a divergence-oriented approach to adaptive random testing of Java programs,, self-repair through reconfiguration: a requirements engineering approach,, and inferring resource specifications from natural language API documentation.
Engineering Village;2014;A self-tuning scientific framework using model-driven engineering for heterogeneous execution platforms;;This article presents an ongoing work towards the extension of Sm@rtConfig - A dynamic scheduling tool with self-tuning load-balancing functionalities targeting CPUs, GPUs, and other co-processors. This extension is based on the introduction of a high-level modeling phase for scientific applications. These applications are commonly complex, use (heterogeneous) high performance execution platforms, and require stakeholders of several disciplines. This way, it is important to raise abstraction level in earlier stages of development in order to deal with such complexities in an efficient way. By using Model-Driven Engineering, we propose an approach to transform Sm@rtConfig into a scientific framework comprising requirements engineering up to code generation for the target Processing Unit in which a task is scheduled at runtime. We advocate that our envisioned methodology facilitates not just cross-stakeholders development, but also replication of experimentations by the research community.
Engineering Village;2002;SIMOO-RT - An object-oriented framework for the development of real-time industrial automation systems;;This paper presents SIMOO-RT, an object-oriented framework designed to support the whole development cycle of real-time industrial automation systems. It is based on the concept of distributed active objects, which are autonomous execution entities that have their own thread of control, and that interact with each other by means of remote methods invocation. SIMOO-RT covers most of the development phases, from requirements engineering to implementation. It starts with the construction of an object model of the technical plant to be automated, on which user and problem-domain requirements are captured. Here, emphasis on modeling timing constraints is given. The technical details involved in the process of mapping problem-domain objects to design specific entities as well as the automatic code generation for the runtime application are discussed in the paper. Furthermore, details are given on how to monitor the runtime applications and to evaluate its timing restrictions.
Engineering Village;1998;Proceedings of the 1998 20th International Conference on Software Engineering;;The proceedings contains 64 papers from the 20th International Conference on Software Engineering. Topics discussed include: agile software processes,, software process modeling,, software process improvement activities,, virtual reality systems,, object request broker,, goal-driven requirements engineering,, conceptual module querying,, reuse-driven interprocedural slicing,, three dimensional software modeling,, the Internet,, architecture-based runtime software evolution,, regression test selection techniques,, form-based visual programs,, integrating architecture description languages,, automated validation systems,, object-oriented software,, distributed systems,, non-intrusive object introspection,, and object oriented reuse.
Engineering Village;2012;From use cases and their relationships to code;;Use cases are used in many methodologies to drive the software engineering process. Though, their transition to code was usually a mostly manual process. In the context of MDD, use cases gain attention as first-class artifacts with representation notations allowing for automatic transformations to analysis and design models. The paper concentrates on an important problem of constructing transformations that cater for use case relationships. It presents a notation that unifies the ambiguous 'include' and 'extend', and allows for representing them within textual use case scenarios. This notation, equipped with runtime semantics, is used to construct a direct transformation into working code. The code is placed within method bodies of the Controller/Presenter and View layers within the MVC/MVP framework. Based on this transformation, an agile use-case-driven development process is possible. © 2012 IEEE.
Engineering Village;2011;Dynamic generation of semantic documents for web resources;;Web has several resources like request or session objects, HTML, XML and images which do not have semantic information like RDF or ontologies with them. To convert Web into the Semantic Web it is required to add semantic information to Web resources. Previous approaches generate the semantic document statically. This paper provides a model for developers to generate semantic document (RDF) for the request and session data dynamically. It also provides a model which adds semantic information to the Web resources at runtime. Both these proposed models process new threads which use proposed algorithms and invoked by a Web server or a Web entity like servlet. © Springer-Verlag 2011.
Engineering Village;2001;Requirements-based dynamic metrics in object-oriented systems;;Because early design decisions can have a major long-term impact on the performance of a system, early evaluation of the high-level architecture can be an important risk mitigation technique. This paper proposes a technique for predicting the volume of data that will flow across a network in a distributed system. The prediction is based upon anticipated execution of scenarios and can be applied at an extremely early stage of the design. It is driven by requirements specifications and captures dynamic metrics by defining typical usage patterns in terms of scenarios. Scenarios are then mapped to architectural components, and dataflow across inter-partition links is estimated. The feasibility of the approach is demonstrated through an experiment in which predicted metrics are compared to runtime measurements.
Engineering Village;2006;Keeping track of crosscutting requirements in UML models via context-based constraints;;One crosscutting requirement (also called aspect) affects several parts of a software system. Handling aspects is well understood at source-code level or at runtime. However, only a few aspect-oriented approaches handle other software artefact types, like UML models, configuration files, or database schema definitions. Instead of re-writing the same aspect newly for each artefact type, this paper suggests to write down an aspect once independent of artefact types.But, wait a minute: Which places does such an aspect affect? Where do we weave in an aspect if its pointcut doesn't refer to artefact details? This paper suggests expressing aspects via Context-Based Constraints (CoCons). They select their constrained system elements according to the element's context. For instance, CoCons affect all system elements used in a certain department, workflow, or location. CoCons are easy to grasp for users and customers because they express business requirements without referring to technical details. This paper focuses on how to express and monitor crosscutting requirements in UML models via CoCons. Moreover, it reveals that CoCons are a new notion of constrains by comparing CoCons to the Object Constraint Language OCL.
Engineering Village;2013;Improving trace accuracy through data-driven configuration and composition of tracing features;;Software traceability is a sought-after, yet often elusive quality in large software-intensive systems primarily because the cost and effort of tracing can be overwhelming. State-of-the art solutions address this problem through utilizing trace retrieval techniques to automate the process of creating and maintaining trace links. However, there is no simple one-size-fits all solution to trace retrieval. As this paper will show, finding the right combination of tracing techniques can lead to significant improvements in the quality of generated links. We present a novel approach to trace retrieval in which the underlying infrastructure is configured at runtime to optimize trace quality. We utilize a machine-learning approach to search for the best configuration given an initial training set of validated trace links, a set of available tracing techniques specified in a feature model, and an architecture capable of instantiating all valid configurations of features. We evaluate our approach through a series of experiments using project data from the transportation, healthcare, and space exploration domains, and discuss its implementation in an industrial environment. Finally, we show how our approach can create a robust baseline against which new tracing techniques can be evaluated.
Engineering Village;2002;Reflective event service middleware for distributed component based applications;;An Event Service is needed for providing event delivery occurring distributed component-based applications such as multimedia communication, electronic commerce, and traffic control system. It supports asynchronous communication between multiple suppliers and consumers required by the distributed applications. However, the event service specification lacks important features for user requirements reflection as follows,, reflective event filtering, user quality of service (UQoS) and component management. Thus, this paper proposes a Reflective Event Service (RES) Middleware framework for distributed component-based applications. The RES middleware based on CORBA Component Model (CCM) and includes the following components,, reflective event filtering component, event monitor component, and UQoS management Component. Especially, this paper concentrates on providing suitable reflective event filtering component for UQoS service. © Springer-Verlag Berlin Heidelberg 2002.
Engineering Village;2005;The application of compile-time reflection to software fault tolerance using Ada 95;;Transparent system support for software fault tolerance reduces performance in general and precludes application-specific optimizations in particular. In contrast, explicit support - especially at the language level - allows application-specific tailoring. However, current techniques that extend languages to support software fault tolerance lead to interwoven code addressing functional and non-functional requirements. Reflection promises both significant separation of concerns and a malleability allowing the user to customize the language toward the optimum point in a language design space. To explore this potential we compare common software fault tolerance scenarios implemented in both standard and reflective Ada. Specifically, in addition to backward error recovery and recovery blocks, we explore the application of reflection to atomic actions and conversations. We then compare the implementations in terms of expressive power, portability, and performance. © Springer-Verlag Berlin Heidelberg 2005.
Engineering Village;2011;Assessment of ABET student outcomes during industrial internships;;The Paper Science and Engineering (PSEN) program at UW-Stevens Point has had a three-credit industrial internship requirement since 1973. We assessed this requirement through comprehensive student papers covering the technology of the pulp and paper industry and the processes and products of the mills in which students worked. This assessment worked well until roughly ten years ago, when mills began retaining the reports, saying that they contained proprietary information. At the time, faculty decided to share the rubric used to evaluate student papers with mill supervisors so that they would have a standard by which they could rate papers, as well as an evaluation form to provide feedback on student work in the mill. In 2010, we developed a new approach to assessing these internships. Taking advantage of the capabilities of the online course management system Desire2Learn&reg, students now respond to 16 questions about their internship work while they are in the mills. These responses help students to remember activities performed during the entire internship. When they return to campus, students provide two pieces of work to satisfy academic requirements: reflection papers and electronic portfolios that document their internship work, specifically addressing how their internships helped them develop skills in several ABET Student Outcomes for the PSEN program. The portfolios provide evidence that the faculty can use to assess the achievement level for outcomes associated with these internships. In this paper, we describe the assessment method in more detail, and the conference presentation will include a demonstration of the technology. We also discuss the need for students to be able to more clearly identify and articulate achievement of learning outcomes. A critical finding of our initial study is that students often met learning outcomes without realizing they did, without understanding the importance of communicating that they did, or simply by not being able to effectively communicate that they did. © 2011 American Society for Engineering Education.
Engineering Village;2010;Foreword: First workshop requirements@run.time;;The first edition of the Workshop requirements@run.time was held at the Eighteenth International Conference on Requirements Engineering (RE 2010) in the city of Sydney, NSW, Australia on the 28th of September 2010. It was organized by Pete Sawyer, Jon Whittle, Nelly Bencomo, Daniel Berry, and Anthony Finkelstein. This foreword presents a digest of the presentations and discussions that took place during the workshop. © 2010 IEEE.
Engineering Village;2001;EJVM: an economic Java run-time environment for embedded devices;;As network-enabled embedded devices and Java grow in their popularity, embedded system researchers start seeking ways to make devices Java-enabled. However, it is a challenge to apply Java technology to these devices due to their shortage of resources. In this paper, we propose EJVM (Economic Java Virtual Machine), an economic way to run Java programs on network-enabled and resource-limited embedded devices. Espousing the architecture proposed by distributed JVM, we store all Java codes on the server to reduce the storage needs of the client devices. In addition, we use two novel techniques to reduce the client-side memory footprints: server-side class representation conversion and on-demand bytecode loading. Finally, we maintain client-side caches and provide performance evaluation on different caching policies. We implement EJVM by modifying a freely available JVM implementation, Kaffe. From the experiment results, we show that EJVM can reduce Java heap requirements by about 20-50% and achieve 90% of the original performance.
Engineering Village;2012;Relaxing claims: Coping with uncertainty while evaluating assumptions at run time;;Self-adaptation enables software systems to respond to changing environmental contexts that may not be fully understood at design time. Designing a dynamically adaptive system (DAS) to cope with this uncertainty is challenging, as it is impractical during requirements analysis and design time to anticipate every environmental condition that the DAS may encounter. Previously, the RELAX language was proposed to make requirements more tolerant to environmental uncertainty, and Claims were applied as markers of uncertainty that document how design assumptions affect goals. This paper integrates these two techniques in order to assess the validity of Claims at run time while tolerating minor and unanticipated environmental conditions that can trigger adaptations. We apply the proposed approach to the dynamic reconfiguration of a remote data mirroring network that must diffuse data while minimizing costs and exposure to data loss. Results show RELAXing Claims enables a DAS to reduce adaptation costs. © 2012 Springer-Verlag.
Engineering Village;2011;Reasoning about adaptive requirements for self-adaptive systems at runtime;;Increasing proliferation of mobile applications challenge the role of requirements engineering (RE) in developing customizable and adaptive software applications for the end-users. Such adaptive applications need to alter their behavior while monitoring and evaluating the changes in the environment at runtime by being aware of their end-user's needs, context and resources. More specifically, these applications should be able to: (i) reason about their own requirements and refine and validate them at run-time by involving end-users, if necessary,, (ii) provide solutions for the refined or changed requirements at runtime, for instance by exploiting available services. In this position paper we focus on the first issue. We propose to extend our previous work on adaptive requirements with preference-based reasoning and automated planning to enable a continuous adaptive reasoning of requirements at runtime. We describe this vision using a navigation system example and highlight challenges. © 2011 IEEE.
Engineering Village;2010;Continuous adaptive requirements engineering: An architecture for self-adaptive service-based applications;;Engineering self-adaptive service-based applications significantly challenges the role of requirements engineering (RE). Such systems need to cope with the evolving requirements at runtime by monitoring the changes in users' preferences and in the environment, evaluating the changes and enacting a suitable behavior ensuring an acceptable level of quality for their users. This calls for continuous reappraisal of their requirements specification enabling them to reason for them at run-time. Recently, we proposed a novel framework for Continuous Adaptive RE (CARE) supporting self-adaptive service-based applications and on conceptual tools needed to realize RE at run-time. In this position paper, we focus on a conceptual architecture for the CARE framework, and illustrate how it can work using scenarios from travel domain. Potential impact of our work and useful integration with recent studies are discussed, highlighting open points for future research. © 2010 IEEE.
Engineering Village;2011;Run-time resolution of uncertainty;;Requirements awareness should help optimize requirements satisfaction when factors that were uncertain at design time are resolved at runtime. We use the notion of claims to model assumptions that cannot be verified with confidence at design time. By monitoring claims at runtime, their veracity can be tested. If falsified, the effect of claim negation can be propagated to the system's goal model and an alternative means of goal realization selected automatically, allowing the dynamic adaptation of the system to the prevailing environmental context. © 2011 IEEE.
Engineering Village;2009;Runtime monitoring of cross-cutting policy;;In open systems, certain unfavorable situations due to unanticipated user behavior may be seen, which results in a violation of cross-cutting policy. This paper proposes a runtime monitoring method to check such problems. Since there is a large gap, a certain link is needed between the policy and runtime execution method. We employ a two-step checking approach,, an offline symptom checking and a runtime monitoring. The ingredient to tie the two steps is a Linear-time Temporal Logic formula for the cross-cutting policy to look at. © 2009 IEEE.
Engineering Village;2000;Changing class behaviors at run-time in MRP systems;;This paper presents an architecture that can be used to develop and maintain class behaviors in object-oriented material requirements planning (MRP) systems at run-time. The architecture provides this capability through a user-interface, and does not require knowledge of any programming language. The architecture is based on the concept of software reuse, it utilizes a library of fine-grained pre-compiled objects to develop and maintain class behaviors. The architecture is a dynamic-object application builder implemented on top of an object-oriented run-time environment.
Engineering Village;2012;An eclipse modelling framework alternative to meet the Models@Runtime requirements;;Models@Runtime aims at taming the complexity of software dynamic adaptation by pushing further the idea of reflection and considering the reflection layer as a first-class modeling space. A natural approach to Models@Runtime is to use MDE techniques, in particular those based on the Eclipse Modeling Framework. EMF provides facilities for building DSLs and tools based on a structured data model, with tight integration with the Eclipse IDE. EMF has rapidly become the defacto standard in the MDE community and has also been adopted for building Models@Runtime platforms. For example, Frascati (implementing the Service Component Architecture standard) uses EMF for the design and runtime tooling of its architecture description language. However, EMF has primarily been thought to support design-time activities. This paper highlights specific Models@Runtime requirements, discusses the benefits and limitations of EMF in this context, and presents an alternative implementation to meet these requirements. © 2012 Springer-Verlag.
Engineering Village;2005;Engineering runtime requirements-monitoring systems using MDA technologies;;The Model-Driven Architecture (MDA) technology toolset includes a language for describing the structure of meta-data, the MOF, and a language for describing consistency properties that data must exhibit, the OCL. Off-the-shelf tools can generate meta-data repositories and perform consistency checking over the data they contain. In this paper we describe how these tools can be used to implement runtime requirements monitoring of systems by modelling the required behaviour of the system, implementing a meta-data repository to collect system data, and consistency checking the repository to discover violations. We evaluate the approach by implementing a contract checker for the SLAng service-level agreement language, a language defined using a MOF meta-model, and integrating the checker into an Enterprise JavaBeans application. We discuss scalability issues resulting from immaturities in the applied technologies, leading to recommendations for their future development. © Springer-Verlag Berlin Heidelberg 2005.
Engineering Village;2011;Towards adaptive systems through requirements@runtime?;;Software systems must adapt their behavior in response to changes in the environment or in the requirements they are supposed to meet. Despite adaptation capabilities could be modeled with great detail at design time, anticipating all possible adaptations is not always feasible. To address this problem the requirements model of the system, which also includes the adaptation capabilities, is conceived as a run- time entity. This way, it is possible to trace requirements/adaptation changes and propagate them onto the application instances. This paper leverages the FLAGS [1] methodology, which provides a goal model to represent adaptations and a runtime infrastructure to manage require- ments@runtime. First, this paper explains how the FLAGS infrastructure can support requirements@runtime, by managing the interplay between the requirements and the executing applications. Finally, it describes how this infrastructure can be used to adapt the system, and, consequently, support the evolution of requirements.
Engineering Village;2008;Secure workflow development from early requirements analysis;;Requirements engineering is being increasingly adopted as a key step in the software development process and so new challenges and possibilities emerge. Designing of web services and developing of business processes and workflows for web services is one of the most thought challenging issues in requirements engineering. The research on web services design is well under way, but the existing design methodologies for web services do not address the issue of developing secure web services, secure business processes and secure workflows. For the purpose of developing secure workflows based on the early requirements analysis, in this work, we propose a refinement methodology and a language that allows the workflow engine to automatically enforce trust and delegation requirements. Those workflows are then to be distributed,, the security aspects being enforced dynamically at runtime accordingly to the identified requirements. To make the discussion more concrete, we illustrate the proposal with an e-business banking case study. © 2008 IEEE.
Engineering Village;2013;Intention-oriented modelling support for GORE in elastic cloud applications;;Businesses started to exploit the forthcoming value from deployment of cloud computing as a new caterpillar paradigm to reach out more diversified customer slices. Although the general concepts they practically focus on are: viability, survivability, adaptability, etc., however, on the ground, there is still a lack for forming mechanisms to sustain viability with adaptation of new types of requirements that pertain to other un-tackled aspects of the echosystem. Such aspects like social intentionality are of actors and their goals. This paper introduces modern dynamic software programming environment aided with modelling support to achieve operationalization and adaptation of abstract object,, goals and their properties as formation of new type of requirements into service based applications distributed over the cloud. This will in turn provide system runtime components to interactively confer to guarantee self-adaptive behaviour with respect to its functional and non-functional characteristics. © 2013 IEEE.
Engineering Village;2011;Adaptive service composition based on runtime requirements monitoring;;In today's service computing environments, user needs and expectations are constantly changing. New services emerge while old ones become obsolete and need to be replaced. In such settings, composite services need to be adaptive to changes in user requirements and the environment. This paper proposes a conceptual framework for modeling compositional adaptation for services founded on a requirements monitoring facility. This facility helps maintain adherence between user requirements changes and the dynamics of service composition structure and quality attributes. Specifically, user requirements are represented as goals and softgoals, service composition structure is represented with a CSP-like grammar, and the adaptation mechanism is based on AI planning. The proposed approach is evaluated in a service simulation environment of real-world supply-chain adaptation scenarios. © 2011 IEEE.
Engineering Village;2003;Programming at runtime: Requirements & paradigms for nonprogrammer web application development;;We investigate the femibiliy of nonprogramnier web application development and propose the creation of end-user programming tools that address the issue at a high level of abstraction. The results of three related empirical studies and one protoping effort are reported. We surveyed nonprogrammers' needs for web applications and studied how nonprogrammers would naturally approach web development. To express what a tool should provide we summarize high-level components and concepts employed by web applications. To express how a tool may provide its functionality, we propose 'Programming-at-Runtime' - a programming paradigm that is in its core similar to the automatic recalculation in spreadsheets. Finally, we introduce 'FlashLight' - a protoype web development tool for nonprogrammers. © 2003 IEEE.
Engineering Village;2008;Dynamic grid scheduling using job runtime requirements and variable resource availability;;We describe a scheduling technique in which estimated job runtimes and estimated resource availability are used to efficiently distribute workloads across a homogeneous grid of resources with variable availability. The objective is to increase efficiency by minimizing job failure caused by resources becoming unavailable. Optimal scheduling will be accomplished by mapping jobs onto resources with sufficient availability. Both the scheduling technique and the implementation called PGS (Prediction based Grid Scheduling) are described in detail. Results are presented for a set of sleep jobs, and compared with a first come, first serve scheduling approach. © 2008 Springer-Verlag Berlin Heidelberg.
Engineering Village;2012;The research of software requirements analysis of core needs;;Software requirements should be divided into functional requirements and non functional requirements, including run-time quality attributes, develop-time quality attributes and constraints in software engineering, the rate of software rework keeps high because of human factors, fail to grasp the key factors.In the practical point of view, the six key steps can be proposed in the needs analysis study to resist development rework, increase success rate of software development. © 2012 SPIE.
Engineering Village;2011;Dealing with softgoals at runtime: A fuzzy logic approach;;One of the first frameworks to deal with NonFunctional Requirements, or softgoals, is the NFR Framework. This framework allows among other contributions softgoals analysis by applying propagation rules. This analysis is commonly performed during design activities. Instead of working with softgoals at design time, the proposal described in this paper combines propagation rules, fuzzy logic and Multi-Agent Systems in order to provide support for dealing with softgoals at runtime. Observing, for example, how the Requirements Engineering community deals with softgoals analysis by using propagation rules, we developed a propagation simulator centered on a specific algorithm. This simulator tries to replicate the requirements engineers' practices when using propagation rules to make decisions at design time. Based on this propagation simulator, we propose an intentional-MAS-driven reasoning engine capable of analyzing softgoals at runtime by selecting an adequate strategy (i.e. an adequate plan) that will be performed by the intentional agent to achieve the desired goal. © 2011 IEEE.
Engineering Village;2007;Middleware based runtime monitoring and analyzing framework;;Runtime software monitoring and analyzing is not only the approach to improve the quality of software, but also the basis of adaptive software. This paper proposes a pattern-based declarative approach to specify constraints. Based on this approach we implemented a runtime monitoring and analyzing framework on J2EE middleware PKUAS. The most special points of the framework are the flexibility of deploying probes and business logic oriented monitoring. In the end, the paper describes the implementation details and evaluation result of the framework.
Engineering Village;2010;Towards a continuous requirements engineering framework for self-adaptive systems;;Requirements engineering (RE) for self-adaptive systems (SAS) is an emerging research area. The key features of such systems are to be aware of the changes in both their operating and external environments, while simultaneously remaining aware of their users' goals and preferences. This is accomplished by evaluating such changes and adapting to a suitable alternative that can satisfy those changes in the context of the user goals. Most current RE languages do not consider this 'reflective' and online component of requirements models. In this paper, we propose a new framework for building SAS that is goal- and user-oriented. We sketch a framework to enable continuous adaptive requirements engineering (CARE) for SAS that leverage requirements-aware systems and exploits the Techne modeling language and reasoning system. We illustrate our framework by showing how it can handle an adaptive scenario in the travel domain. © 2010 IEEE.
Engineering Village;2013;Intention-oriented programming support for runtime adaptive autonomic cloud-based applications;;The continuing high rate of advances in information and communication systems technology creates many new commercial opportunities but also engenders a range of new technical challenges around maximising systems' dependability, availability, adaptability, and auditability. These challenges are under active research, with notable progress made in the support for dependable software design and management. Runtime support, however, is still in its infancy and requires further research. This paper focuses on a requirements model for the runtime execution and control of an intention-oriented Cloud-Based Application. Thus, a novel requirements modelling process referred to as Provision, Assurance and Auditing, and an associated framework are defined and developed where a given system's non/functional requirements are modelled in terms of intentions and encoded in a standard open mark-up language. An autonomic intention-oriented programming model, using the Neptune language, then handles its deployment and execution. © 2013 Elsevier Ltd. All rights reserved.
Engineering Village;2010;Requirements reflection: Requirements as runtime entities;;Computational reflection is a well-established technique that gives a program the ability to dynamically observe and possibly modify its behaviour. To date, however, reflection is mainly applied either to the software architecture or its implementation. We know of no approach that fully supports requirements reflection- that is, making requirements available as runtime objects. Although there is a body of literature on requirements monitoring, such work typically generates runtime artefacts from requirements and so the requirements themselves are not directly accessible at runtime. In this paper, we define requirements reflection and a set of research challenges. Requirements reflection is important because software systems of the future will be self-managing and will need to adapt continuously to changing environmental conditions. We argue requirements reflection can support such self-adaptive systems by making requirements first-class runtime entities, thus endowing software systems with the ability to reason about, understand, explain and modify requirements at runtime. © 2010 ACM.
Engineering Village;2011;Aspect-oriented requirements engineering for advanced separation of concerns: A review;;Software engineering was introduced to cope with software crisis with two fundamental principles: separation of concerns and modularity. Many programming paradigms have been proposed and used while considering the fundamental principles from the early days. Complex software systems were successfully modularized but complete separation of concerns is still impossible to achieve using today's most popular programming paradigms such as object-oriented programming. There are some concerns which could not be separated properly in a single class or module due to their highly coupling with other classes or modules' behaviors. We call such unmodularized concerns as crosscutting concerns and these are responsible for scattering and tangling. Aspects are the natural evolution of the object-oriented paradigm. They provide a solution to some difficulties encountered with object-oriented programming, sometimes scattering and tangling. Hence, Aspect-Oriented Software Development (AOSD) is another step towards achieving improved modularity during software development. It gives emphasis to the separation of crosscutting concerns i.e. advanced separation of concerns and encapsulation of crosscutting concerns in separate modules, known as aspects. It later uses composition mechanism to weave them with other core modules at loading time, compilation time, or run-time. Aspect-Oriented Requirements Engineering (AORE) is an early phase in AOSD that supports separation of crosscutting concerns at requirements level. It does not replace but rather complements any of the existing requirements methodologies. Over the last few years, several research efforts have been devoted to this area. Several techniques for crosscutting concern identification have already been proposed. In this paper, an attempt is made to review the existing approaches and understand their contribution to requirements engineering.
Engineering Village;2014;Semiautomatic security requirements engineering and evolution using decision documentation, heuristics, and user monitoring;;Security issues can have a significant negative impact on the business or reputation of an organization. In most cases they are not identified in requirements and are not continuously monitored during software evolution. Therefore, the inability of a system to conform to regulations or its endangerment by new vulnerabilities is not recognized. In consequence, decisions related to security might not be taken at all or become obsolete quickly. But to evaluate efficiently whether an issue is already addressed appropriately, software engineers need explicit decision documentation. Often, such documentation is not performed due to high overhead. To cope with this problem, we propose to document decisions made to address security requirements. To lower the manual effort, information from heuristic analysis and end user monitoring is incorporated. The heuristic assessment method is used to identify security issues in given requirements au-tomatically. This helps to uncover security decisions needed to mitigate those issues. We describe how the corresponding security knowledge for each issue can be incorporated into the decision documentation semiautomatically. In addition, violations of security requirements at runtime are monitored. We show how decisions related to those security requirements can be identified through the documentation and updated manually. Overall, our approach improves the quality and completeness of security decision documentation to support the engineering and evolution of security requirements. © 2014 IEEE.
Engineering Village;2011;Intentional models based on measurement theory;;Metrics and measures have always been the subject of quite a lot of research works in the Requirements Engineering (RE) community, including about intentional models of Goal-Oriented RE (GORE) such as those of i,,. However, using recent developments of the Measurement Theory, in this paper we show that the concept of Measurement Framework (MF) for soft-systems is useful for the analysis of business service systems that need long-term service agreements based on consistent measurements at all stages of their life-cycle (from inception to operation). We show that with two kinds of goals and softgoals based on MF, it is possible to improve (a) the elicitation of functional and non- functional requirements, (b) the structure of the i,, models, and (c) the consistency between run-time measurements and the model-based assessments of business services at early stages of RE.
Engineering Village;2010;Requirements engineering for adaptive Service Based Applications;;Service-Based Applications (SBA) are inherently open and distributed, as they rely on third-party services that are available over the Internet, and have to cope with the dynamism of such operating environment. This motivates the need for SBA to be self-adaptive to accommodate changes in service availability and performance, in consumers' needs and preferences, and more generally in the operational environment, which may occur at run-time. Engineering such applications significantly challenges the role of requirements engineering (RE). Usually, RE is carried out at the outset of the whole development process, but in the context of SBA, RE activities are also needed at run-time thus enabling a seamless SBA evolution. In this paper, we investigate RE for SBA at run-time proposing a method that supports the continuous refinement of requirements artifacts at run-time, which involves consumers and the SBA itself as primary stakeholders. © 2010 IEEE.
Engineering Village;2011;From awareness requirements to adaptive systems: A control-theoretic approach;;Several proposals for the design of adaptive systems rely on some kind of feedback loop that monitors the system output and adapts in case of failure. Roadmap papers in the area advocate the need to make such feedback loops first class entities in adaptive systems design. We go further by adopting a Requirements Engineering perspective that is not only based on feedback loops but also applies other concepts from Control Theory to the design of adaptive systems. Our plans include a framework that reasons over requirements at runtime to provide adaptivity to a system proper. In this position paper, we argue for a control-theoretic view for adaptive systems and outline our long-term research agenda, briefly presenting work that we have already accomplished and discussing our plans for the future. © 2011 IEEE.
Engineering Village;2011;Goal-driven adaptation of service-based systems from runtime monitoring data;;Service-based systems need to provide flexibility to adapt both to evolving requirements from multiple, often conflicting, ephemeral and unknown stakeholders, as well as to changes in the runtime behavior of their component services. Goal-oriented models allow representing the requirements of the system whilst keeping information about alternatives. We present the MAESoS approach which uses i* diagrams to identify quality of service requirements over services. The alternatives are extracted and kept in a variability model. A monitoring infrastructure identifies changes in runtime behavior that can propagate up to the level of stakeholder goals and trigger the required adaptations. We illustrate the approach with a scenario of use. © 2011 IEEE.
Engineering Village;2013;Requirements models for design- and runtime: A position paper;;In this position paper we review the history of requirements models and conclude that a goal-oriented perspective offers a suitable abstraction for requirements analysis. We stake positions on the nature of modelling languages in general, and requirements modelling languages in particular. We then sketch some of the desirable features (... 'requirements') of design-time and runtime requirements models and draw conclusions about their similarities and differences. © 2013 IEEE.
Engineering Village;2005;Compile-time stack requirements analysis with GCC motivation, development, and experiments results;;Stack overflows are a major threat to many computer applications and run-time recovery techniques are not always available or appropriate. In such situations, it is of utmost value to prevent overflows from occurring in the first place, which requires evaluating the worst case stack space requirements of an application prior to operational execution time. More generally, as run-time stack areas need memory resources, information on the stack allocation patterns in software components is always of interest. We believe that specialized compiler outputs can be of great value in a stack usage analysis framework and have developed GCC extensions for this purpose. In this paper, we first expand on the motivations for this work, then describe what it consists of so far, future directions envisioned, and experiments results.
Engineering Village;2011;Requirements engineering for self-adaptive systems: Core ontology and problem statement;;The vision for self-adaptive systems (SAS) is that they should continuously adapt their behavior at runtime in response to changing user's requirements, operating contexts, and resource availability. Realizing this vision requires that we understand precisely how the various steps in the engineering of SAS depart from the established body of knowledge in information systems engineering. We focus in this paper on the requirements engineering for SAS. We argue that SAS need to have an internal representation of the requirements problem that they are solving for their users. We formally define a minimal set of concepts and relations needed to formulate the requirements problem, its solutions, the changes in its formulation that arise from changes in the operating context, requirements, and resource availability. We thereby precisely define the runtime requirements adaptation problem that a SAS should be engineered to solve. © 2011 Springer-Verlag.
Engineering Village;2008;When to adapt? Identification of problem domains for adaptive systems;;Dynamically adaptive systems (DASs) change behaviour at run-time to operate in volatile environments. As we learn how best to design and build systems with greater autonomy, we must also consider when to do so. Thus far, DASs have tended to showcase the benefits of adaptation infrastructures with little understanding of what characterizes the problem domains that require run-time adaptation. This position paper posits that context-dependent variation in the acceptable trade-offs between non-functional requirements is a key indicator of problems that require dynamically adaptive solutions. © 2008 Springer-Verlag Berlin Heidelberg.
Engineering Village;2013;End-to-end formal specification, validation, and verification process: A case study of space flight software;;The quality of requirements and the effectiveness of verification and validation (V&V) techniques in guaranteeing that a final system reflects its established requirements have a direct influence on the quality and dependability of the delivered system. The V&V process can be efficient from a managerial point of view, but ineffective from a technical perspective, and vice versa. This paper presents an end-to-end formal computer-aided specification, validation, and verification (SV&V) process, whose feasibility and effectiveness were evaluated against the flight software for the Brazilian Satellite Launcher. Unified modeling language (UML) statechart assertions, scenario-based validation, and runtime verification are used to formally specify and verify the system, and metrics of the ongoing process and its V&V results are collected during the application of the process. The results of the case study indicate that the process and its computer-aided environment were both technically feasible to apply and managerially effective, will likely scale well to cater to SV&V of mission-critical systems that have a larger number of behavioral requirements, and can be used for V&V in a distributed development environment. © 2007-2012 IEEE.
Engineering Village;2000;Field analysis: Getting useful and low-cost interprocedural information;;We present a new limited form of interprocedural analysis called field analysis that can be used by a compiler to reduce the costs of modern language features such as object-oriented programming, automatic memory management, and run-time checks required for type safety. Unlike many previous interprocedural analyses, our analysis is cheap, and does not require access to the entire program. Field analysis exploits the declared access restrictions placed on fields in a modular language (e.g. field access modifiers in Java) in order to determine useful properties of fields of an object. We describe our implementation of field analysis in the Swift optimizing compiler for Java, as well a set of optimizations that exploit the results of field analysis. These optimizations include removal of run-time tests, compile-time resolution of method calls, object inlining, removal of unncessary synchronization, and stack allocation. Our results demonstrate that field analysis is efficient and effective. Speedups average 7% on a wide range of applications, with some times reduced by up to 27%. Compile time overhead of field analysis is about 10%.
Engineering Village;2015;Cyber-physical systems design for runtime trustworthiness maintenance supported by tools;;The trustworthiness of cyber-physical systems is a critical factor for establishing wide-spread adoption of these systems. Hence, especially the behavior of safety-critical software components needs to be monitored and managed during system operation. Runtime trustworthiness maintenance should be planned and prepared in early requirements and design phases. This involves the identification of threats that may occur and affect user's trust at runtime, as well as related controls that can be executed to mitigate the threats. Furthermore, observable and measureable system quality properties have to be identified as indicators of threats, and interfaces for reporting these properties as well as for executing controls have to be designed and implemented. This paper presents a process model for preparing and designing systems for runtime trustworthiness maintenance, which is supported by several tools that facilitate the tasks to be performed by requirements engineers and system designers. Copyright © 2015 by the authors.
Engineering Village;2015;A tool for monitoring and maintaining system trustworthiness at runtime;;Trustworthiness of software systems is a key factor in their acceptance and effectiveness. This is especially the case for cyber-physical systems, where incorrect or even sub-optimal functioning of the system may have detrimental effects. In addition to designing systems with trustworthiness in mind, monitoring and maintaining trustworthiness at runtime is critical to identify issues that could negatively affect a system's trustworthiness. In this paper, we present a fully operational tool for system trustworthiness maintenance, covering a comprehensive set of quality attributes. It automatically detects, and in some cases mitigates, trustworthiness threatening events. The use of such a tool can enable complex software systems to support runtime adaptation and self-healing, thus reducing the overall upkeep cost and complexity. © 2015 by the authors.
Engineering Village;2014;Software evaluation via users' feedback at runtime;;Users' evaluation of software at runtime is a powerful tool which enables us to capture and communicate a richer and updated knowledge on how users view software throughout its life cycle. Users understand the software as a means to meet their requirements and, thus, giving them a voice in the continuous runtime evaluation of software would naturally need to fit this level of abstraction. That is, users' evaluation feedback would mainly communicate their judgment on the role of the system in meeting their requirements. Users' runtime evaluation feedback could be used to take autonomous or semi-autonomous runtime adaptation decisions or to support developers on taking evolution and maintenance decisions. Within this picture, our research focuses on to the development of a modeling and elicitation framework of users' evaluation feedback at runtime. This includes devising mechanisms to structure such an evaluation feedback in a way that makes it easy for users to express and developers to interpret. We motivate our work and articulate the problem and the set of research questions to address in our research and the method to follow and reach them. We also discuss our initial results on the topic. Copyright 2014 ACM.
Engineering Village;2007;Runtime verification and monitoring of embedded systems;;Ensuring the correctness of software applications is a difficult task. The area of runtime verification, which combines the approaches of formal verification and testing, offers a practical but limited solution that can help in finding many errors in software. Runtime verification relies upon tools for monitoring software execution. There are particular difficulties with regard to monitoring embedded systems. The concerns for arranging non-intrusive monitoring of embedded systems in a way that is suitable for use in runtime verification methods are considered here. A number of existing runtime verification tools are referenced, highlighting their requirement for monitoring solutions. Established and emerging approaches for the monitoring of software execution using execution monitors are reviewed, with an emphasis on the approaches that are best suited for use with embedded systems. A suggested solution for non-intrusive monitoring of embedded systems is presented. The conclusions summarise the possibilities for arranging non-intrusive monitoring of embedded systems, and the potential for runtime verification to utilise such monitoring approaches. © The Institution of Engineering and Technology 2007.
Engineering Village;2015;Context uncertainty in requirements engineering: Definition of a search strategy for a systematic review and preliminary results;;[Context and motivation] Cyber-physical systems (CPS) and selfadaptive systems (SAS) strongly rely on the context they are operating in and need to adapt their behavior at run-time based on contextual information. Therefore, it is challenging to completely predict the context of such systems for their entire operating time already at design time. [Question/problem] Since several approaches dealing with uncertainty have been proposed for different research and problem domains in recent years, some might provide valuable insights for the engineering of CPS or SAS in uncertain contexts. However, there is no study so far that provides an overview of them. [Principle ideas/results] Thus, we aim at conducting a systematic literature analysis to create a research landscape of approaches coping with context uncertainty. [Contribution] We manually searched one journal and the proceedings of two conferences in the requirements engineering field to determine and evaluate the adequateness of search strings to be used in an automated search. In doing so, we can furthermore present preliminary findings from the manual search for uncertainty in the requirements engineering field. © 2015 by the authors.
Engineering Village;2011;Towards self-managed systems aware of economic value;;Current self-managed systems do not take into account the economic value they exchange with external systems and services. While value-based software and requirements engineering approaches exist, they do not specifically deal with self-managed systems and their operations. These operations are typically associated with changing requirements, dynamic adaptations, and runtime reconfigurations, as well as changes in the environment. We argue that self-managed systems should be 'aware' of their requirements and the economic value created (or loosed) by their fulfillment (or not fulfillment) as well as the economic value exchanged with other systems. They should be able to reason and take actions based on this 'awareness', especially when their environmental conditions are changing (both technical and economic ones). More generally, self-managed systems should be able to reason about the systems' business goals and relate them with the Quality of Service they offer. This paper discusses these issues and proposes a generic architecture for addressing them. © 2010 IEEE.
Engineering Village;2012;Requirements-driven adaptive security: Protecting variable assets at runtime;;Security is primarily concerned with protecting assets from harm. Identifying and evaluating assets are therefore key activities in any security engineering process from modeling threats and attacks, discovering existing vulnerabilities, to selecting appropriate countermeasures. However, despite their crucial role, assets are often neglected during the development of secure software systems. Indeed, many systems are designed with fixed security boundaries and assumptions, without the possibility to adapt when assets change unexpectedly, new threats arise, or undiscovered vulnerabilities are revealed. To handle such changes, systems must be capable of dynamically enabling different security countermeasures. This paper promotes assets as first-class entities in engineering secure software systems. An asset model is related to requirements, expressed through a goal model, and the objectives of an attacker, expressed through a threat model. These models are then used as input to build a causal network to analyze system security in different situations, and to enable, when necessary, a set of countermeasures to mitigate security threats. The causal network is conceived as a runtime entity that tracks relevant changes that may arise at runtime, and enables a new set of countermeasures. We illustrate and evaluate our proposed approach by applying it to a substantive example concerned with security of mobile phones. © 2012 IEEE.
Engineering Village;2011;A Runtime evaluation methodology and framework for autonomic systems;;An autonomic system provides self-adaptive ability that enables system to dynamically adjust its behavior on environmental changes or system failure. Fundamental process of adaptive behavior in an autonomic system is consist of monitoring system or/and environment information, analyzing monitored information, planning adaptation policy and executing selected policy. Evaluating system utility is one of a significant part among them. We propose a novel approach on evaluating autonomic system at runtime. Our proposed method takes advantage of a goal model that has been widely used at requirement elicitation phase to capture system requirements. We suggest the state-based goal model that is dynamically activated as the system state changes. In addition, we defined type of constraints that can be used to evaluate goal satisfaction level. We implemented a prototype of autonomic computing software engine to verity our proposed method. We simulated the behavior of the autonomic computing engine with the home surveillance robot scenario and observed the validity of our proposed method.
Engineering Village;2007;Visualizing the analysis of dynamically adaptive systems using i* and DSLs;;Self-adaptation is emerging as a crucial enabling capability for many applications, particularly those deployed in dynamically changing environments. One key challenge posed by Dynamically Adaptive Systems (DASs) is the need to handle changes to the requirements und corresponding behavior of a DAS in response to varying environmental conditions. In this paper we propose a visual model-driven approach that uses the i* modeling language to represent goal models for the DAS requirements. Our approach applies a rigorous separation of concerns between the requirements for the DAS to operate in stable conditions and those that enable it to adapt at run-time to enable it to cope with changes in its environment. We further show how requirements derived from the i* modeling can be used by a domain-specific language to achieve requirements model-driven development. We describe our experiences with applying this approach to GridStix, an adaptive flood warning system, deployed on the River Ribble in North Yorkshire, England. © 2007 IEEE.
Engineering Village;2008;Multi-level system integration based on autosar;;The design of distributed embedded real-time system is a challenging task. Besides solving the control-engineering issues, one has to consider real-time scheduling, reliability and production requirements w.r.t. production cost of the electronic control unit (ECU). This has a considerable impact on the employed software design techniques. These design techniques are well known in the automotive software industry, but are applied with different flavors at each vehicle manufacturer and their suppliers. This situation has changed considerably with the results of the AUTOSAR development partnership, which unifies the flavors of automotive software design. Automotive software design is embedded in the so-called V-Cycle of embedded automotive system development [1]. It starts with the requirements analysis which results later on in a model of the control algorithm. The control algorithm is tested against a vehicle model and establishes the topmost level in system integration. The second system integration level is the adaptation of the control algorithm to be run on a rapid-prototyping system. The rapid-prototyping system is integrated into an existing E/E-architecture. The E/E-architecture consists of the ECUs connected by networks like CAN or FlexRay and gateways. Sensors- and actuators being used by several control algorithms are coupled to an ECU, which might propagate signals to other ECUs via a vehicle network. From the software point of view, the control-algorithm has now to respect real-time scheduling and the quantization of the sensor- and actuator signals, no matter whether these signals are generated on the rapid-prototyping system or exchanged via the bus with other ECUs. Further development steps in the V-cycle are the software implementation, the ECU- and the network integration. The integrated ECUs and networks are tested against vehicle models running on Hardware-in-the-loop (HiL) systems. If this works fine, the ECUs are integrated in the real vehicle for calibration. The software implementation and the ECU integration are deeply influenced by AUTOSAR. AUTOSAR is a development partnership of all stakeholders in the automotive software development (e.g. vehicle manufacturers and their suppliers) which unifies several software implementation techniques[2]. It describes a common ECU software architecture[3] consisting of configurable basic software modules (BSW), a runtime environment (RTE) and a software component description[4]. The software component description describes the interfaces for data-exchange as well as the access points for the RTE. The basic idea of AUTOSAR is to establish a so-called virtual functional bus (VFB) consisting of interconnected software components which are later mapped to an E/E-architecture. Currently, the VFB structure of AUTOSAR software architectures is mainly driven by the next generation E/E-architectures. The VFB structure forms the third system integration level. The next integration step into an AUTOSAR environment is the integration of the rapid-prototyping tested control algorithm to AUTOSAR software components. Since most VFB descriptions use fixed-point interfaces, the control algorithm has to be transformed to fixed-point arithmetic. If the control algorithm is modelled in tools like ASCET, this conversion can be achieved by code-generation. The same holds true for control algorithms already being used in E/E-architectures without an AUTOSAR software architecture. The VFB-description with the control-algorithms forms the fourth system integration level, which can be simulated with plant-models on a PC by tools like INTECRIO-VP. The fifth system integration level is given by the mapping process as defined in the AUTOSAR methodology[5] and requires the configuration of the RTE and the BSW modules for a single ECU. At this integration level, one can perform HiL testing and calibration in the same way as for non-AUTOSAR systems. Several evaluation projects, e.g. [6] and [7], have shown that the multi-level integration approach is feasible to guide the configuration capabilities of AUTOSAR software architectures.
Engineering Village;2003;Cyclone: A broadcast-free dynamic instruction scheduler with selective replay;;To achieve high instruction throughput, instruction schedulers must be capable of producing high-quality schedules that maximize functional unit utilization while at the same time enabling fast instruction issue logic. Many solutions exist to the scheduling problem, ranging from compile-time to run-time approaches. Compile-time solutions feature fast and simple hardware, but at the expense of conservative schedules. Dynamic schedulers produce high-quality schedules that incorporate run-time information and dependence speculation, but implementing these schedulers requires complex circuits that can slow processor clock speeds. In this paper, we present the Cyclone scheduler, a novel design that captures the benefits of both compile- and run-time scheduling. Our approach utilizes a list-based single-pass instruction scheduling algorithm, implemented by hardware at run-time in the front end of the processor pipeline. Once scheduled, instructions are injected into a timed queue that orchestrates their entry into execution. To accommodate branch and load/store dependence speculation, the Cyclone scheduler supports a simple selective replay mechanism. We implement this technique by overloading instruction register forwarding to also detect instructions dependent on incorrectly scheduled operations. Detailed simulation analyses suggest that with sufficient queue width, the Cyclone scheduler can rival the instruction throughput of similarly wide monolithic dynamic schedulers. Furthermore, the circuit complexity of the Cyclone scheduler is much more favorable than a broadcast-based scheduler, as our approach requires no global control signals.
Engineering Village;2007;Dynamic requirements specification for adaptable and open service systems;;The Dynamic Requirements Adaptation Method (DRAM) is suggested to assist existing RE methodologies in updating requirements specifications at runtime for adaptable and open service-oriented systems. Updates are needed because an adaptable and open system continually changes how and to what extent initial requirements are achieved. © 2007 IEEE.
Engineering Village;2011;Runtime models for automatic reorganization of multi-robot systems;;This paper presents a reusable framework for developing adaptive multi-robotic systems for heterogeneous robot teams using an organization-based approach. The framework is based on the Organizational Model for Adaptive Computational Systems (OMACS) and the Goal Model for Dynamic Systems (GMoDS). GMoDS is used to capture system-level goals that drive the system. OMACS is an abstract model used to capture the system configuration and allows the team to organize and reorganize without the need for explicit runtime reorganization rules. While OMACS provides an implicit reorganization capability, it also supports policies that can either guide or restrict the resulting organizations thus limiting unexpected or harmful adaptation. We demonstrate our framework by presenting the design and implementation of a multi-robot system for detecting improvised explosive devices. We then highlight the adaptability of the resulting system. © 2011 ACM.
Engineering Village;2015;A lightweight framework for dynamic GUI data verification based on scripts;;Summary: Runtime verification (RV) provides essential mechanisms to enhance software robustness and prevent malfunction. However, RV often entails complex and formal processes that could be avoided in scenarios in which only invariants or simple safety properties are verified, for example, when verifying input data in Graphical User Interfaces (GUIs). This paper describes S-DAVER, a lightweight framework aimed at supporting separate data verification in GUIs. All the verification processes are encapsulated in an independent layer and then transparently integrated into an application. The verification rules are specified in separate files and written in interpreted languages to be changed/reloaded at runtime without recompilation. Superimposed visual feedback is used to assist developers during the testing stage and to improve the experience of users during execution. S-DAVER provides a lightweight, easy-to-integrate and dynamic verification framework for GUI data. It is an integral part of the development, testing and execution stages. An implementation of S-DAVER was successfully integrated into existing open-source applications, with promising results. © 2015John Wiley & Sons, Ltd.
Engineering Village;2010;A metamodel for aspect-oriented analysis approach;;The Aspect-Oriented Requirements Analysis (AORA) approach concentrates on the identification, modularization, representation and composition of crosscutting concerns during Requirements Engineering. Crosscutting concerns are encapsulated in separate modules, known as aspects, and composition mechanisms are later used to compose them back with other core modules, at loading time, compilation time, or run-time. AORA is an integrated framework with a process model, a traceability schema, a conflict resolution technique and tool supporting. But for an approach to be more precisely defined a metamodel specification is desired. This paper presents a metamodel for AORA as a means to define its concepts, relationships and composition rules rigorously. © 2010 by CIbSE 2010.
Engineering Village;2013;Requirements and architectures for adaptive systems;;The growing interest in developing adaptive systems has led to numerous proposals for approaches aimed at supporting the development of such systems. Some approaches define adaptation mechanisms in terms of architectural designs, consisting of concepts such as components, connectors and states. Other approaches are requirements-based, thus concerned with goals, tasks, contexts and preferences as concepts in terms of which adaptation is defined. By considering only a partial view of software systems (either the problem space or the solution space), such proposals are limited in specifying the adaptive behavior of a software system. In this paper we present ongoing work towards deriving architectural models in order to support the design and runtime execution of software adaptation both at a requirements and architectural level.
Engineering Village;2008;A human-machine dimensional inference ontology that weaves human intentions and requirements of context awareness systems;;Changing system requirements, especially for context awareness (CA) systems, often cause modifications in the software systems in order to adapt to dynamic environments. Since the requirements may become temporarily obsolete or contrary to human intentions, the CA systems need to be tuned to resolve the conflict. On the other hand, most CA design methods rely on pre-defined requirements and reasoning engine, thus, fail to address all the possible situations. Consequently, services provided by such a CA system are limited to accommodate some situations and unable to react as expected. Therefore, it is critical for CA systems to capture exceptions at runtime, infer changed human intentions, and adapt to these changes. This study focuses on inference of ever-changing human intentions and monitoring human intentions to handle system evolution. In this paper, we present an inference mechanism of human intentions via the Human-machine Dimensional Inference Ontology (HDIO). This ontology gives inference rules based on the BDI logic to deduce human intentions from contexts. Furthermore, the inference exercises of a healthcare system example shows how user intentions relate to system requirements and how they help improve self-adaptability of CA systems. © 2008 IEEE.
Engineering Village;2002;Using indexed data structures for program specialization;;Given a program and values of static (fixed) inputs, program specialization generates an optimized version of the program that only requires dynamic (run-time) inputs. It has been an useful tool for such areas as operating systems, multimedia applications, and scientific applications. However, the size of specialized code may grow up exponentially which makes program specialization impractical for many applications. In this paper, we present a mechanism to address this problem by using indexed data structures. Unlike traditional program specialization, which encodes the result of specialization only into run-time code, our method encodes the values of multi-valued static expressions into indexed data structures and single-valued static expressions into run-time code. Because the sizes of the indexed data structures are much smaller than that of program code, we can overcome the size problem of program specialization. With a preliminary implementation for Java, we achieved improvement in performance up to a factor of 3 with very low memory and space requirements and overheads.
Engineering Village;2001;Extensibility via a meta-level architecture;;Meta-level architectures are recognized as a means to achieve run-time extensibility, and have been applied as such in existing hypermedia systems. Yet, designing a good meta-level architecture is notoriously hard and remains an art rather than a science. This paper shows how to derive a meta-level architecture for hypermedia navigation, thereby providing a way to control how third-party components interact with the linking engine. This extra level of control allows for a better and safer integration between an extensible system and the third-party components extending it.
Engineering Village;2011;Modelling adaptability and variability in requirements;;The requirements and design level identification and representation of dynamic variability for adaptive systems is a challenging task. This requires time and effort to identify and model the relevant elements as well as the need to consider the large number of potentially possible system configurations. Typically, each individual variability dimension needs to identified and modelled by enumerating each possible alternative. The full set of requirements needs to be reviewed to extract all potential variability dimensions. Moreover, each possible configuration of an adaptive system needs to be validated before use. In this demonstration, we present a tool suite that is able to manage dynamic variability in adaptive systems and tame such system complexity. This tool suite is able to automatically identify dynamic variability attributes such as variability dimensions, context, adaptation rules, and soft/hard goals from requirements documents. It also supports modelling of these artefacts as well as their run-time verification and validation. © 2011 IEEE.
Engineering Village;1999;A virtual hardware handler for RTR systems;;The design of a Virtual Hardware Handler for run-time reconfiguration is presented. A windows-based system that works with the VCC Hotworks board has been implemented and results are presented.
Engineering Village;2009;Proceedings of the 2009 ICSE Workshop on Aspect-Oriented Requirements Engineering and Architecture Design, EA 2009;;The proceedings contain 9 papers. The topics discussed include: automating the discovery of stable domain abstractions for reusable aspects,, runtime monitoring of cross-cutting policy,, support for aspectual modeling to multiagent system architecture,, heterogeneous pointcut expressions,, using tagging to identify and organize concerns during pre-requirements analysis,, concern tracing and change impact analysis: an exploratory study,, on modeling interactions of early aspects with goals,, promoting the software evolution in AOSD with early aspects: architecture-oriented model-based pointcuts,, and model driven development with non-functional aspects.
Engineering Village;2002;Using execution trace data to improve distributed systems;;One of the most challenging problems facing today's software engineer is to understand and modify distributed systems. One reason is that in actual use systems frequently behave differently than the developer intended. In order to cope with this challenge, we have developed a three-step method to study the run-time behavior of a distributed system. First, remote procedure calls are traced using CORBA interceptors. Next, the trace data is parsed to construct RPC call-return sequences, and summary statistics are generated. Finally, a visualization tool is used to study the statistics and look for anomalous behavior. We have been using this method on a large distributed system (more than 500000 lines of code) with data collected during both system testing and operation at a customer's site. Despite the fact that the distributed system had been in operation for over three years, the method has uncovered system configuration and efficiency problems. Using these discoveries, the system support group has been able to improve product performance and their own product maintenance procedures.
Engineering Village;2004;Strategies for handling the activity problem in runtime software evolution by reducing activity;;Different strategies to reduce the activity problem in runtime software evolutions, are presented. The circumstances under which those strategies are viable were also analyzed. A set of three properties that distinguish between the methods that are highly active for different reasons were proposed. A tool was created that automatically analyzes a running system and find the properties for any given methods. The main strategies for reducing the activity problem include outline parts at development time, outline non-blocking code at development time, outline updated part by runtime refractoring, and pause service of a method.
Engineering Village;2008;Compliance of semantic constraints - A requirements analysis for process management systems;;Key to the use of process management systems (PrMS) in practice is their ability to facilitate the implementation, execution, and adaptation of business processes while still being able to ensure error-free process executions. Mechanisms have been developed to prevent errors at the syntactic level such as deadlocks. In many application domains, processes often have to comply with business level rules and policies (i.e., semantic constraints). Hence, in order to ensure error-free executions at the semantic level, PrMS need certain control mechanisms for validating and ensuring the compliance with semantic constraints throughout the process lifecycle. In this paper, we discuss fundamental requirements for a comprehensive support of semantic constraints in PrMS. Moreover, we provide a survey on existing approaches and discuss to what extent they meet the requirements and which challenges still have to be tackled. Finally, we show how the challenge of life time compliance can be dealt with by integrating design time and runtime process validation.
Engineering Village;2015;Model-based generation of a requirements monitor;;Runtime representations of requirements have recently gained interested to deal with uncertainty in the environment and the term requirements at runtime has been established. Runtime representations of requirements support reasoning about the requirements at runtime and adapting the configuration of a system according to changes in the environment. Such systems often called self-adaptive systems. Core part of respective approaches in the field is a requirements monitor. That is, an instance which is able to observe the system's environment and to decide whether a requirement is broken, based on assertions. The problem addressed in this paper is how to generate the applicationspecific parts of a requirements monitor. Such a monitor consists of some goal model for decisions at runtime, assertions connected to the goal model, and parameters on which assertions are defined. We present in this paper a model-driven approach to enhance requirements documents by goal models, assertions, and parameters in a way which is (1) understandable to requirements engineers and (2) a sufficient basis for generating the requirements monitor. The contribution is an integrated view on requirements for self-adaptive systems and a concept for code generation. © 2015 by the authors.
Engineering Village;2007;Runtime service discovery and reconfiguration using OWL-S based semantic web service;;In recent business environment where collaboration between organizations is essential, business organization must fast adapt to changing user and market demands. SOC (Service-Oriented Computing) is believed to be a prominent paradigm for efficient businesses application development. Semantic web enables users to locate, select, employ, compose, and monitor Web-based services automatically. OWL-S, ontology of service, helps this location, selection, and composition of services. In this paper we propose a framework, to support dynamic reconfiguration of service-oriented applications. This framework supports semantic discovery, ranking, and dynamic reconfiguration of services using OWL-S based ontology. A prototype is provided to show the validity of this framework. © 2007 IEEE.
Engineering Village;2012;Approaches for mastering change;;Modern software systems are highly configurable and exist in many different variants in order to operate different application contexts. This is called static variability and predominantly considered in software product line engineering [6,14]. Furthermore, software systems have to evolve over time in order to dealwith changing requirements which is referred to by the term temporal evolvability [10,13]. Additionally, modern software systems are designed to dynamically adapt their internal structure and behavior at runtime dependent on their environment in order to efficiently use the available resources, such as energy or computing power [5]. These three dimensions of change, static variability, temporal evolvability and dynamic adaptation, increase the complexity of system development in all phase, from requirements engineering and system design to implementation and quality assurance. In [15], the challenges of static variability and temporal evolution in all phases of the software development process are discussed. In [15], the engineering challenges of self-adaptive systems are described and future research directions are pointed out. © 2012 Springer-Verlag.
Engineering Village;2008;Refining goal models by evaluating system behaviour;;Nowadays, information systems have to perform in complex, heterogeneous environments, considering a variety of system users with different needs and preferences. Software engineering methodologies need to cope with the complexity of requirements specification in such scenarios, where new requirements may emerge also at run-time and the system's goals are expected to evolve to meet new stakeholder needs. Following an agent-oriented approach, we are studying methods and techniques to design adaptive and evolvable information systems able to fulfill stakeholders' objectives. In a previous work we defined an Agent-Oriented framework to design and code system specifications in terms of goal models and we instantiated it in a tool supported process which exploits the Agent-Oriented Software Engineering methodology Tropos and the Multi-Agent Platform JADE/Jadex [11]. In this paper, we show how to use this framework to develop a system following an iterative process, where the system execution allows enriching the system specification given in terms of goal models. Experimental evaluation has been performed on a simple example and lead to the refinement of the designed goal model upon the analysis of the system's run-time behaviour. © 2008 Springer-Verlag Berlin Heidelberg.
Engineering Village;2014;Feedback-aware requirements documents for smart devices;;[Context/ Motivation] A smart device is a software-intensive system that operates autonomously and interacts to some degree with other systems over wireless connections. Such systems are often faced with uncertainty in the environment. Runtime representations of requirements have recently gained more interested to deal with this challenge and the term requirements at runtime has been established. Runtime representations of requirements support reasoning about the requirements at runtime and adapting the configuration of a system according to changes in the environment. [Questions/Problems] The research question is how the results of runtime monitoring of requirements and the system's decisions about changes in the configuration are communicated back to the requirements engineer to better understand the environment. There is a gap between the written requirements document and the dynamic requirements model inside the system. This problem is exacerbated by the fact that a requirements document are mostly informal while the dynamic requirements model is formal. [Principal ideas/results] This paper introduces an approach to bridge the gap between development time and runtime representations of requirements in order to keep them consistent and to facilitate better understanding. We propose to weave the feedback from the runtime system into requirements documents using a domain-specific language that largely retain the informal nature of requirements. An annotated requirements document helps get a better understanding of the system's actual behavior in a given environment. The approach is implemented using mbeddr, a novel set of domain-specific languages for developing embedded systems, and illustrated using a running example. © 2014 Springer International Publishing Switzerland.
Engineering Village;2009;Reasoning on non-functional requirements for integrated services;;We focus on non-functional requirements for applications offered by service integrators,, i.e., software that delivers service by composing services, independently developed, managed, and evolved by other service providers. In particular, we focus on requirements expressed in a probabilistic manner, such as reliability or performance. We illustrate a unified approach-a method and its support tools - which facilitates reasoning about requirements satisfaction as the system evolves dynamically. The approach relies on run-time monitoring and uses the data collected by the probes to detect if the behavior of the open environment in which the application is situated, such as usage profile or the external services currently bound to the application, deviates from the initially stated assumptions and whether this can lead to a failure of the application. This is achieved by keeping a model of the application alive at run time, automatically updating its parameters to reflect changes in the external world, and using the model's predictive capabilities to anticipate future failures, thus enabling suitable recovery plans. © 2009 IEEE.
Engineering Village;2009;Requirements evolution and what (research) to do about it;;Requirements evolution is a research problem that has received little attention hitherto, but deserves much more. For systems to survive in a volatile world, where business needs, government regulations and computing platforms keep changing, software systems must evolve too in order to survive. We discuss the state-of-the-art for research on the topic, and predict some of the research problems that will need to be addressed in the next decade. We conclude with a concrete proposal for a run-time monitoring framework based on (requirements) goal models. © Springer-Verlag Berlin Heidelberg 2009.
Engineering Village;2011;Automatic derivation of utility functions for monitoring software requirements;;Utility functions can be used to monitor requirements of a dynamically adaptive system (DAS). More specifically, a utility function maps monitoring information to a scalar value proportional to how well a requirement is satisfied. Utility functions may be manually elicited by requirements engineers, or indirectly inferred through statistical regression techniques. This paper presents a goal-based requirements model-driven approach for automatically deriving state-, metric-, and fuzzy logic-based utility functions for RELAXed goal models. State- and fuzzy logic-based utility functions are responsible for detecting requirements violations, and metric-based utility functions are used to detect conditions conducive to a requirements violation. We demonstrate the proposed approach by applying it to the goal model of an intelligent vehicle system (IVS) and use the derived utility functions to monitor the IVS under different environmental conditions at run time. © 2011 Springer-Verlag.
Engineering Village;2001;An architecture for re-engineering of client/server applications;;Netsiel's architectural approach to re-engineering client-server application is presented. The real case of a re-platform project for application of large dimensions composed from 5,000 Java classes and 10,000 Cobol programs.
Engineering Village;2007;A framework for supporting dynamic systems co-evolution;;Businesses and their supporting software evolve to accommodate the constant revision and re-negotiation of commercial goals, and to intercept the potential of new technology. We have adopted the term co-evolution to describe the concept of the business and the software evolving sympathetically, but at potentially different rates. More generally, we extend co-evolution to accommodate wide-informatics systems, that are assembled from parts that co-evolve with each other and their environment, and whose behavior is potentially emergent. Typically these are long-lived systems in which dynamic co-evolution, whereby a system evolves as part of its own execution in reaction to both expected and unexpected events, is the only feasible option for change. Examples of such systems include continuously running business process models, sensor nets, grid applications, self-adapting/tuning systems, peer-to-peer routing systems, control systems, autonomic systems, and pervasive computing applications. The contribution of this paper comprises: a study of the intrinsic nature of dynamic co-evolving systems,, the derivation of a set of intrinsic requirements,, a description of a model and a set of technologies, new and extant, to meet these intrinsic requirements,, and illustrations of how these technologies may be implemented within an architecture description language (ArchWare ADL) and a conventional programming language (Java). The model and technologies address three topics: structuring for dynamic co-evolution, incremental design, and adapting dynamic co-evolving systems. The combination yields a framework that can describe the system's specification, the executing software and the reflective evolutionary mechanisms within a single computational domain in which all three may evolve in tandem. © 2007 Springer Science+Business Media, LLC.
Engineering Village;2003;Architecting Adaptable Software Using COTS: An NFR Approach;;The use of Commercial-Off-The-Shelf (COTS) components presents a great promise, as well as challenges and risks. In this paper, we describe an Adaptable COTS-Aware Software Architecting (ACASA) framework that addresses the concerns of the various stakeholders of the proposed system, in the presence of COTS components, with a special emphasis on adaptability as a non-functional requirement. In particular, we describe a two-phase matching-and-selection scheme: one phase by use of functionality, and the other by use of adaptability. The ACASA framework is illustrated by way of a telepresence system example.
Engineering Village;2004;Twelfth ACM SIGSOFT International Symposium on the Foundations of Software Engineering, SIGSOFT 2004/FSE-12;;The proceedings contain 28 papers from the Twelfth ACM SIGSOFT International Symposium on the Foundations of Software Engineering, SIGSOFT 2004/FSE-12. The topics discussed include: resolving uncertainties during trace analysis,, automating comprehensive safety analysis of concurrent programs using VeriSoft and TXL,, efficient incremental algorithms for dynamic detection of likely invariants,, system architecture: the context for scenario-based model synthesis,, merging partial behavioural models,, reasoning about partial goal satisfaction for requirements and design engineering,, and the usability problem for home appliances: engineers caused it, engineers can fix it.
Engineering Village;2011;Requirements-driven adaptation: Compliance, context, uncertainty, and systems;;The systematic study of software self-adaptation has emerged as one of the key areas of software engineering. The challenges and the ontology relevant to this area are still being formulated. I take this opportunity to present some of my observations on compliance, context, and uncertainty as they pertain to adaptation. I also argue that requirements engineering, and to a large extent software engineering, takes a centralized perspective of systems, and therefore cannot model let alone enable reasoning about adaptation in multiagent systems. © 2011 IEEE.
Engineering Village;2014;Engineering topology aware adaptive security: Preventing requirements violations at runtime;;Adaptive security systems aim to protect critical assets in the face of changes in their operational environment. We have argued that incorporating an explicit representation of the environment's topology enables reasoning on the location of assets being protected and the proximity of potentially harmful agents. This paper proposes to engineer topology aware adaptive security systems by identifying violations of security requirements that may be caused by topological changes, and selecting a set of security controls that prevent such violations. Our approach focuses on physical topologies,, it maintains at runtime a live representation of the topology which is updated when assets or agents move, or when the structure of the physical space is altered. When the topology changes, we look ahead at a subset of the future system states. These states are reachable when the agents move within the physical space. If security requirements can be violated in future system states, a configuration of security controls is proactively applied to prevent the system from reaching those states. Thus, the system continuously adapts to topological stimuli, while maintaining requirements satisfaction. Security requirements are formally expressed using a propositional temporal logic, encoding spatial properties in Computation Tree Logic (CTL). The Ambient Calculus is used to represent the topology of the operational environment - including location of assets and agents - as well as to identify future system states that are reachable from the current one. The approach is demonstrated and evaluated using a substantive example concerned with physical access control. © 2014 IEEE.
Engineering Village;2012;Mitigating the obsolescence of quality specifications models in service-based systems;;Requirements-aware systems address the need to reason about uncertainty at runtime to support adaptation decisions, by representing quality of services (QoS) requirements for service-based systems (SBS) with precise values in run-time queryable model specification. However, current approaches do not support updating of the specification to reflect changes in the service market, like newly available services or improved QoS of existing ones. Thus, even if the specification models reflect design-time acceptable requirements they may become obsolete and miss opportunities for system improvement by self-adaptation. This articles proposes to distinguish 'abstract' and 'concrete' specification models: the former consists of linguistic variables (e.g. 'fast') agreed upon at design time, and the latter consists of precise numeric values (e.g. '2ms') that are dynamically calculated at run-time, thus incorporating up-to-date QoS information. If and when freshly calculated concrete specifications are not satisfied anymore by the current service configuration, an adaptation is triggered. The approach was validated using four simulated SBS that use services from a previously published, real-world dataset,, in all cases, the system was able to detect unsatisfied requirements at run-time and trigger suitable adaptations. Ongoing work focuses on policies to determine recalculation of specifications. This approach will allow engineers to build SBS that can be protected against market-caused obsolescence of their requirements specifications. © 2012 IEEE.
Engineering Village;2015;Modeling and verification of Functional and Non-Functional Requirements of ambient Self-Adaptive Systems;;Self-Adaptive Systems modify their behavior at run-time in response to changing environmental conditions. For these systems, Non-Functional Requirements play an important role, and one has to identify as early as possible the requirements that are adaptable. We propose an integrated approach for modeling and verifying the requirements of Self-Adaptive Systems using Model Driven Engineering techniques. For this, we use Relax, which is a Requirements Engineering language which introduces flexibility in Non-Functional Requirements. We then use the concepts of Goal-Oriented Requirements Engineering for eliciting and modeling the requirements of Self-Adaptive Systems. For properties verification, we use OMEGA2/IFx profile and toolset. We illustrate our proposed approach by applying it on an academic case study. © 2015 Elsevier Inc. All rights reserved.
Engineering Village;2006;Selective code/data migration for reducing communication energy in embedded MpSoC architectures;;Proliferation of embedded on-chip multiprocessor architectures (MpSoC) motivates researchers from both academia and industry to consider optimization techniques for such architectures. While many proposals on code/data partitioning on parallel architectures try to minimize interprocessor communication requirements at runtime, many applications still have significant runtime communication requirements. This paper proposes a novel task/data migration scheme that decides whether to migrate task or data in order to satisfy a given communication requirement. The choice between the two options is made at runtime based on the statistics collected off-line through profiling. An important characteristic of the approach proposed in this paper is that it takes into account future uses of tasks and data and tries to make a decision that is globally optimal (i.e., when considering multiple communications not just the current one). Our results collected so far are very encouraging and indicate that the proposed selective migration strategy is very successful in practice, reducing communication energy and total energy by 38.6% (resp. 18.9%) and 13.8% (resp. 6.8%) on an average, as compared to task (resp. data) migration only, across ten embedded applications tested. Copyright 2006 ACM.
Engineering Village;2015;MiDAS: A model-driven approach for adaptive software;;Some of the main problems in software engineering for adaptive software are: the lack of mechanisms to specify adaptive characteristics in software requirements,, the difficulty to obtain a functional adaptive system based on the elicited requirements,, and the need of maintaining synchronization and traceability between the requirements, design and implementation. To address the above problems, this paper proposes MiDAS, a framework that uses a model-driven approach to develop adaptive software. Specifically, MiDAS provides: (i) a new language for requirements engineering process that takes into account uncertainty in adaptive software,, (ii) a method to derive concrete implementations in specific architectures supporting run-time adaptation,, and, (iii) a mechanism to maintain traceability and synchronization between requirements specifications, design models and implementation architectures.
Engineering Village;2009;Specifying requirements for real-time systems;;Software or System Requirements often change during their operational lifetime[4]. These changes take place due to misconceived requirements perceived during the requirement and design stages. Any changes in requirements are mostly evident during the run time of the system and are known as emergent faults. These emergent faults are unforeseen by the requirement engineers and so there is no built-in fault tolerance solution for the components in this system. In this paper we consider a real-time multi-network system of Alegent Health, where their application servers are used for critical Electronic Medical Record (EMR) application. These application servers frequently blow up due to memory leakages thus resulting in kicking off the users (doctor and nurses) of the system, elevating concerns on patient safety and also causing loss of revenue. We attribute this problem to be a misconceived requirement and as an emergent runtime fault. We propose a health monitoring system which will keep track of the health conditions of the servers with the objective of increasing overall system throughput, decreasing system downtime, and minimizing loss of data. We have proposed the use of the concept of Duration Calculus to specify the requirements of the system in terms of continuous state variables and discrete state variables.
Engineering Village;2012;Towards a goal-driven approach to action selection in self-adaptive software;;Self-adaptive software is a closed-loop system, since it continuously monitors its context (i.e. environment) and/or self (i.e. software entities) in order to adapt itself properly to changes. We believe that representing adaptation goals explicitly and tracing them at run-time are helpful in decision making for adaptation. While goal-driven models are used in requirements engineering, they have not been utilized systematically yet for run-time adaptation. To address this research gap, this article focuses on the deciding process in self-adaptive software, and proposes the Goal-Action-Attribute Model (GAAM). An action selection mechanism, based on cooperative decision making, is also proposed that uses GAAM to select the appropriate adaptation action(s). The emphasis is on building a light-weight and scalable run-time model which needs less design and tuning effort comparing with a typical rule-based approach. The GAAM and action selection mechanism are evaluated using a set of experiments on a simulated multi-tier enterprise application, and two sample ordinal and cardinal action preference lists. The evaluation is accomplished based on a systematic design of experiment and a detailed statistical analysis in order to investigate several research questions. The findings are promising, considering the obtained results, and other impacts of the approach on engineering self-adaptive software. Although, one case study is not enough to generalize the findings, and the proposed mechanism does not always outperform a typical rule-based approach, less effort, scalability, and flexibility of GAAM are remarkable. Copyright © 2011 John Wiley & Sons, Ltd.
Engineering Village;2006;Dynamic weaving of security aspects in service composition;;Web Service composition is to construct complex service through combining available services components as request. Service composition often has to handle the security risk that can not be predicted when the service components are developed. This paper presents an Aspect-Oriented (AO) approach to enhance the security of service composition that can not only realize flexible security policies but also accomplish it with very little run-time overhead. The security control is separated from other functional requirements and encapsuled into service extension aspect. And the composition can be extended by weaving the extension at runtime. It also gives the service composer a chance to unify security policy in composed service by specifying appropriate security extension himself. A Web Service extension Environment (WSXE) is devised to demonstrate the approach. Finally, an application of performing user-defined access control dynamically at runtime is given to exemplify the dynamic extension to service composition. © 2006 IEEE.
Engineering Village;2013;Using requirements models at runtime: Potential and challenges;;
Engineering Village;2012;A requirements-based approach for the design of adaptive systems;;Complexity is now one of the major challenges for the IT industry [1]. Systems might become too complex to be managed by humans and, thus, will have to be self-managed: Self-configure themselves for operation, self-protect from attacks, self-heal from errors and self-tune for optimal performance [2]. (Self-)Adaptive systems evaluate their own behavior and change it when the evaluation indicates that it is not accomplishing the software's purpose or when better functionality and performance are possible [3]. To that end, we need to monitor the behavior of the running system and compare it to an explicit formulation of requirements and domain assumptions [4]. Feedback loops (e.g., the MAPE loop [2]) constitute an architectural solution for this and, as proposed by past research [5], should be a first class citizen in the design of such systems. We advocate that adaptive systems should be designed this way from as early as Requirements Engineering and that reasoning over requirements is fundamental for run-time adaptation. We therefore propose an approach for the design of adaptive systems based on requirements and inspired in control theory [6]. Our proposal is goal-oriented and targets softwareintensive socio-technical systems [7], in an attempt to integrate control-loop approaches with decentralized agents inspired approaches [8]. Our final objective is a set of extensions to state-of-the-art goal-oriented modeling languages that allow practitioners to clearly specify the requirements of adaptive systems and a run-time framework that helps developers implement such requirements. In this 2-page abstract paper, we summarize this approach. © 2012 IEEE.
Engineering Village;2009;Specifying and monitoring interactions and commitments in open business processes;;Business processes are increasingly complex and open because they rely on services that are distributed geographically and across organizations. So, they're prone to several points of failure. Monitoring, therefore, remains an important concern. A new approach specifies and monitors interactions among heterogeneous services by tracking their commitments. This approach extends recent research that views business process design as a composition of interaction protocols. It specifies and monitors policies and commitments as a way to monitor service-level agreements and recover from process failures. © 2009 IEEE.
Engineering Village;2015;From Means-End Analysis to Proactive Means-End Reasoning;;Self-adaptation is a prominent property for developing complex distributed software systems. Notable approaches to deal with self-adaptation are the runtime goal model artifacts. Goals are generally invariant along the system lifecycle but contain points of variability for allowing the system to decide among many alternative behaviors. This work investigates how it is possible to provide goal models at run-time that do not contain tasks, i.e. The description of how to address goals, thus breaking the design-time tie up between Tasks and Goals, generally outcome of a means-end analysis. In this vision the system is up to decide how to combine its available Capabilities: the Proactive Means-End Reasoning. The impact of this research line is to implement a goal-oriented form of self-adaptation where goal models can be injected at runtime. The paper also introduces MUSA, a Middleware for User-driven Service self-Adaptation. © 2015 IEEE.
Engineering Village;2010;A goal-based framework for contextual requirements modeling and analysis;;Requirements engineering (RE) research often ignores or presumes a uniform nature of the context in which the system operates. This assumption is no longer valid in emerging computing paradigms, such as ambient, pervasive and ubiquitous computing, where it is essential to monitor and adapt to an inherently varying context. Besides influencing the software, context may influence stakeholders' goals and their choices to meet them. In this paper, we propose a goal-oriented RE modeling and reasoning framework for systems operating in varying contexts. We introduce contextual goal models to relate goals and contexts,, context analysis to refine contexts and identify ways to verify them,, reasoning techniques to derive requirements reflecting the context and users priorities at runtime,, and finally, design time reasoning techniques to derive requirements for a system to be developed at minimum cost and valid in all considered contexts. We illustrate and evaluate our approach through a case study about a museum-guide mobile information system. © 2010 Springer-Verlag London Limited.
Engineering Village;2010;Requirements-aware systems: A research agenda for RE for self-adaptive systems;;Requirements are sensitive to the context in which the system-to-be must operate. Where such context is well-understood and is static or evolves slowly, existing RE techniques can be made to work well. Increasingly, however, development projects are being challenged to build systems to operate in contexts that are volatile over short periods in ways that are imperfectly understood. Such systems need to be able to adapt to new environmental contexts dynamically, but the contextual uncertainty that demands this self-adaptive ability makes it hard to formulate, validate and manage their requirements. Different contexts may demand different requirements trade-offs. Unanticipated contexts may even lead to entirely new requirements. To help counter this uncertainty, we argue that requirements for self-adaptive systems should be run-time entities that can be reasoned over in order to understand the extent to which they are being satisfied and to support adaptation decisions that can take advantage of the systems' self-adaptive machinery. We take our inspiration from the fact that explicit, abstract representations of software architectures used to be considered design-time-only entities but computational reflection showed that architectural concerns could be represented at run-time too, helping systems to dynamically reconfigure themselves according to changing context. We propose to use analogous mechanisms to achieve requirements reflection. In this paper we discuss the ideas that support requirements reflection as a means to articulate some of the outstanding research challenges. © 2010 IEEE.
Engineering Village;2015;An Empirical Analysis of Providing Assurance for Self-Adaptive Systems at Different Levels of Abstraction in the Face of Uncertainty;;Self-adaptive systems (SAS) must frequently continue to deliver acceptable behavior at run time even in the face of uncertainty. Particularly, SAS applications can self-reconfigure in response to changing or unexpected environmental conditions and must therefore ensure that the system performs as expected. Assurance can be addressed at both design time and run time, where environmental uncertainty poses research challenges for both settings. This paper presents empirical results from a case study in which search-based software engineering techniques have been systematically applied at different levels of abstraction, including requirements analysis, code implementation, and run-time validation, to a remote data mirroring application that must efficiently diffuse data while experiencing adverse operating conditions. Experimental results suggest that our techniques perform better in terms of providing assurance than alternative software engineering techniques at each level of abstraction. © 2015 IEEE.
Engineering Village;2015;Designing an adaptive computer-aided ambulance dispatch system with Zanshin: An experience report;;Summary We have been witnessing growing interest in systems that can adapt their behavior to deal with deviations between their performance and their requirements at run-time. Such adaptive systems usually need to support some form of a feedback loop that monitors the system's output for problems and carries out adaptation actions when necessary. Being an important feature, adaptivity needs to be considered in early stages of development. Therefore, adopting a requirements engineering perspective, we have proposed an approach and a framework (both called Zanshin) for the engineering of adaptive systems based on a feedback loop architecture. As part of our framework's evaluation, we have applied the Zanshin approach to the design of an adaptive computer-aided ambulance dispatch system, whose requirements were based on a well-known case study from the literature. In this paper, we report on the application of Zanshin for the design of an adaptive computer-aided ambulance dispatch system, presenting elements of the design, as well as the results from simulations of run-time scenarios. Copyright © 2013 John Wiley & Sons, Ltd. Copyright © 2013 John Wiley & Sons, Ltd.
Engineering Village;2000;ABCD: Eliminating array bounds checks on demand;;To guarantee typesafe execution, Java and other strongly typed languages require bounds checking of array accesses. Because array-bounds checks may raise exceptions, they block code motion of instructions with side effects, thus preventing many useful code optimizations, such as partial redundancy elimination or instruction scheduling of memory operations. Furthermore, because it is not expressible at bytecode level, the elimination of bounds checks can only be performed at run time, after the bytecode program is loaded. Using existing powerful bounds-check optimizers at run time is not feasible, however, because they are too heavyweight for the dynamic compilation setting. ABCD is a light-weight algorithm for elimination of Array Bounds Checks on Demand. Its design emphasizes simplicity and efficiency. In essence, ABCD works by adding a few edges to the SSA value graph and performing a simple traversal of the graph. Despite its simplicity, ABCD is surprisingly powerful. On our benchmarks, ABCD removes on average 45% of dynamic bound check instructions, sometimes achieving near-ideal optimization. The efficiency of ABCD stems from two factors. First, ABCD works on a sparse representation. As a result, it requires on average fewer than 10 simple analysis steps per bounds check. Second, ABCD is demand-driven. It can be applied to a set of frequently executed (hot) bounds checks, which makes it suitable for the dynamic-compilation setting, in which compile-time cost is constrained but hot statements are known.
Engineering Village;2011;Compliance management with measurement frameworks;;New regulatory regimes advocate the use of 'goal-oriented' regulations that are more flexible during regulatory conversations occurring between the regulators and the regulatees when new regulations are introduced. In that context, long-term 'compliance agreements' between regulators and regulatees are needed. Using recent developments of the Measurement Theory, this paper shows that the concept of Measurement Framework (MF) for soft-systems is of particular importance for providing those compliance agreements. We show that with two kinds of goals and softgoals based on MF, one can improve (a) the elicitation of compliance requirements, (b) the structure of the compliance arguments for compliant requirements, and (c) the consistency between actual compliance at run-time and the intentional compliance at early stages of Requirements Engineering. © 2011 IEEE.
Engineering Village;2007;From a goal-oriented methodology to a BDI agent language: The case of Tropos and alan;;This approach aims at addressing crucial issues in complex distributed software such as capability of evolving and adaptivity. Within the area of goal-oriented software requirements engineering, we propose the use of goal models at different abstraction levels in engineering a Multi-Agent System (MAS), namely, not only at design time, but also as a part of the agent knowledge and choice strategy, at run-time. In this paper we briefly overview a mapping between Tropos concepts and Alan (an agent-object programming language) structures. Specifically, we focus on two advantages of our approach: first, Alan allows us to use in an integrated fashion both agent oriented and object oriented design principles. Second, Alan has a well defined semantics expressed by means of rewriting logic. This allows us to verify the properties of an agent both at design time and at run-time (when its knowledge and behavior can have been modified). © Springer-Verlag Berlin Heidelberg 2007.
Engineering Village;2007;Pedestrian navigation systems: A case study of deep personalization;;Requirements Engineering (RE) focuses on obtaining the user goals and environmental constraints of a proposed system. In traditional RE, users are treated as a consumer-class: what holds for one member is assumed to hold for the rest. Personalization comes through providing options that a user can set at run-time to tailor features of the system to his or her personal preferences. In our work, we take up the personalization issue in more detail. In particular, we believe that some systems require a 'deep personalization' that includes knowledge of an individual user's skills and limitations. In some cases, these skills and limitations might not be self-aware, i.e., a user cannot accurately self-reflect on his or her skills and weaknesses. In this paper, we will demonstrate the notion of deep personalization in the domain of personal navigation systems. We find this an interesting domain for several reasons: (1) There is a domain theory of navigation skills that draws from both Cartography and Psychology. (2) There are individual differences in navigation skills. (3) An individual user may not be self-aware of his or her skills. (4) If a system is delivered that does not match the skills of the user, it may be less than effective, and at worst, abandoned. ©2007 IEEE.
Engineering Village;2005;The MAIS approach to web service design;;This paper presents a first attempt to realize a methodological framework supporting the most relevant phases of the design of a value-added service. A value-added service is defined as a functionality of an adaptive and multi-channel information system obtained by composing services offered by different providers. The framework has been developed as part of the MAIS project. The MAIS framework focuses on the following phases of service life cycle: requirements analysis, design, deployment, run time use and negotiation. In the first phase, the designer elicits, validates and negotiates service requirements according to social and business goals. The design phase is in charge of modelling services with an enhanced version of UML, augmented with new features developed within the MAIS project. The deployment phase considers the network infrastructure and, in particular, provides an approach to implement and coordinate the execution of services from different providers. In the run time use and negotiation phase, the MAIS methodology provides support to the optimal selection and quality renegotiation of services and to the dynamic evaluation of management costs. The paper describes the MAIS methodological tools available for different phases of service life cycle and discusses the main guidelines driving the implementation of a service management architecture, called reflective architecture, that complies with the MAIS methodological approach.
Engineering Village;2011;Satisfying user needs at the right time and in the right place: A research preview;;[Context and motivation] Most requirements engineering (RE) approaches involve analysts in gathering end-user needs. However, we promote the idea that future service-based applications should support end-users in expressing their needs themselves, while the system should be able to respond to these requests by combining existing services in a seamless way. [Question/problem] Research tackling this idea is limited. In this research preview paper we sketch a plan to investigate the following research questions: How can end-users be facilitated by a system to express new needs (e.g. goals, preferences)? How can the continuous analysis of end-user needs result in an appropriate solution? [Principal ideas/results] In our recent research, we have started to explore the idea of involving end-users in RE. Furthermore, we have proposed an architecture that allows performing RE at run-time. The purpose of the planned research is to combine and extend our recent work and to come up with a tool-based solution, which involves end-users in realizing self-adaptive services. Our research objectives include to continuously capture, communicate and analyze end-user needs and feedback in order to provide a tailored solution. [Contribution] In this paper we give a preview on the planned work. After reporting on our recent work we present our research idea and the research objectives in more detail. © 2011 Springer-Verlag Berlin Heidelberg.
Engineering Village;2007;Smart distribution of bio-signal processing tasks in M-health;;The past few years have witnessed a rapid advancement of mobile healthcare systems. However, in the mobile computing environment, the resource fluctuations, stringent application requirements and user mobility have severely hindered the performance and reliability of the healthcare service delivery. The current approaches to solve this resource supply and service demand mismatch problem either limit the adaptation to an isolated node or require significant user's involvement. Given the distributed processing paradigm of mhealth system, we propose that a new adaptation approach could be dynamically redistributing processing tasks across distributed nodes. This PhD research addresses two main issues to validate this approach: (1) computation of a suitable assignment of tasks at compile-time or run-time,, and (2) dynamic distribution of tasks across the nodes according to this new assignment at run-time. © Springer-Verlag Berlin Heidelberg 2007.
Engineering Village;2012;Optimizing monitoring requirements in self-adaptive systems;;Monitoring the system environment is a key functionality of a self-adaptive system. Monitoring requirements denote the information a self-adaptive system has to capture at runtime to decide upon whether an adaptation action has to be taken. The identification of monitoring requirements is a complex task which can easily lead to redundancy and uselessness in the set of information to monitor and this, consequently, means unjustified instalment of monitoring infrastructure and extra processing time. In this paper, we study the optimization of monitoring requirements. We discuss the case of contextual goal model, which is a requirements model that weaves between variability of goals (functional and non-functional requirements) and variability of context (monitoring requirements) and is meant to be used for modelling mobile and self-adaptive systems requirements. We provide automated analysis -based on a SAT-solver- to process a contextual goal model and find a reduced set of contextual information to monitor guaranteeing that this reduction does not sacrifice the system ability of taking correct adaptation decisions when fulfilling its requirements. © 2012 Springer-Verlag Berlin Heidelberg.
Engineering Village;2009;RELAX: Incorporating uncertainty into the specification of self-adaptive systems;;Self-adaptive systems have the capability to autonomously modify their behaviour at run-time in response to changes in their environment. Self-adaptation is particularly necessary for applications that must run continuously, even under adverse conditions and changing requirements,, sample domains include automotive systems, telecommunications, and environmental monitoring systems. While a few techniques have been developed to support the monitoring and analysis of requirements for adaptive systems, limited attention has been paid to the actual creation and specification of requirements of self-adaptive systems. As a result, self-adaptivity is often constructed in an ad-hoc manner. In this paper, we argue that a more rigorous treatment of requirements explicitly relating to self-adaptivity is needed and that, in particular, requirements languages for self-adaptive systems should include explicit constructs for specifying and dealing with the uncertainty inherent in self-adaptive systems. We present RELAX, a new requirements language for selfadaptive systems and illustrate it using examples from the smart home domain. © 2009 IEEE.
Engineering Village;2012;Uncertainty modeling of self-adaptive software requirement;;Service oriented computing utilizes services as fundamental elements for developing applications that have the capability to autonomously modify their behavior at run-time in response to the changes in their environment, which is especially suitable for designing and developing self-adaptive software. While uncertainty induced by randomness environment in service oriented self-adaptive software requirement is a well-studied activity, representing and analyzing uncertainty have not enjoyed equal attention. In this paper, we address this problem by amalgamating context snapshot with goal and business process model to support the representation of uncertainty for self-adaptive software requirements. We define a context snapshot model to represent requirement uncertainty with domain knowledge,, context-specific goal-oriented requirement model is constructed for customer requirements and context-specific process-oriented requirement model is constructed for service requirements,, and finally, propose means-c-end analysis to relate the customer requirements and service requirements with context condition. We illustrate and evaluate our approach through a case study about a city intelligent traffic information system.
Engineering Village;2013;Business process configuration with NFRs and context-awareness;;[Context] Business process models are an important source of information for the development of information systems. Good business processes need to be up-to-date and automated to represent the organizational environment. Representing and configuring business processes variability for a specific organization allows the proper execution of processes. In addition, dynamic environment calls for exible configuration processes that can meet stakeholders' goals. [Question/Problem] Even though current approaches allow the representation of variability of business process models, the selection of business variants in a given context remains a challenging issue. [Main idea] In this proposal, we advocate the use of Non-Functional Requirements (NFR) and contextawareness information to drive the configuration of process models at run-time. In particular, we evaluate the use of NFRs to describe the stakeholders' preferences. [Contribution] We propose a model-driven business process configuration approach that is driven by NFRs and contextual information.
Engineering Village;2006;Extended web services framework to meet non-functional requirements;;Non-functional requirements/characteristics are important for providing effectively every kind of services including web services. A realistic web service must meet both functional and non-functional requirements of its consumers. Therefore, it is important that a web services framework is augmented so that non-functional characteristics of a web service can be determined at run-time and consumers are bound to a service that best meet their functional as well as non-functional requirements. In this paper, we propose an extension of the existing web services framework that enables a collection of functional and non-functional service characteristics at run-time, and usage of the collected data in discovery, binding, and execution of web services. Descriptions of new and enhanced components of the proposed framework are also presented. Typical publishing and usage scenarios in the proposed framework are also described.
Engineering Village;2010;RELAX: A language to address uncertainty in self-adaptive systems requirement;;Self-adaptive systems have the capability to autonomously modify their behavior at run-time in response to changes in their environment. Self-adaptation is particularly necessary for applications that must run continuously, even under adverse conditions and changing requirements,, sample domains include automotive systems, telecommunications, and environmental monitoring systems. While a few techniques have been developed to support the monitoring and analysis of requirements for adaptive systems, limited attention has been paid to the actual creation and specification of requirements of self-adaptive systems. As a result, self-adaptivity is often constructed in an ad-hoc manner. In order to support the rigorous specification of adaptive systems requirements, this paper introduces RELAX, a new requirements language for self-adaptive systems that explicitly addresses uncertainty inherent in adaptive systems. We present the formal semantics for RELAX in terms of fuzzy logic, thus enabling a rigorous treatment of requirements that include uncertainty. RELAX enables developers to identify uncertainty in the requirements, thereby facilitating the design of systems that are, by definition, more flexible and amenable to adaptation in a systematic fashion. We illustrate the use of RELAX on smart home applications, including an adaptive assisted living system. © 2010 Springer-Verlag London Limited.
Engineering Village;2007;Enhancing residential gateways: OSGi service composition;;We propose a schema to support the composition of OSGi services as result of orchestrating atomic services at run-time. With this proposal the OSGi potentiality increases because of the huge amount of new services that can be offered as a result of appropriate combinations. To specify the service composition we propose a totally transparent BPEL-style solution which does not break the OSGi standard. However, the syntactic matchmaking provided by the OSGi standard does not enable a flexible enough automatic service composition. Thus, we propose to define a Semantic OSGi platform, which combined with the BPEL-style solution fits all the OSGi service composition requirements. © 2007 IEEE.
Engineering Village;2012;A framework for service-based business process collaboration;;Nowadays, Web services-based systems have gained the attention of researchers, and it enables business process system to automatically discover and invocate suitable Web services at run time. Further with portable terminals, it has become more and more popular for mobile users to do their business management in modern business process system. This demands a new framework for system requirements elicitation and design in order to support this new application. In this paper, we introduce a framework for the elicitation of function and performance aware adaptation requirements. In this framework, we present rule for verifying and evaluating the behavior of the collaboration system, and the overall reliability of the system from its architecture. We also propose a process mining method to help reconstruct the architecture of business process from execution logs. An agent-based mechanism to support mobile users completing their remote tasks is also discussed. © 2011 by Binary Information Press.
Engineering Village;2001;Towards automatically configurable multimedia applications;;We describe and illustrate an approach to the automatic configuration of component-based multimedia applications. The approach is based on the deployment of a run-time application model that mirrors the active application components, enabling changes to the configuration to be applied and evaluated in the model before they are deployed. The model employs a composite component structure, enabling complexity to be concealed except when detail is required. Constraints and QoS specifications are embedded in the model.
Engineering Village;2005;A CORBA-based dynamic reconfigurable middleware;;The widespread Internet and mobile applications demand increasing requirements for easy and flexible to reconfigure a deployed system during run time. The middleware proposed to help programmer developing distributed application automatically inherits these demands and requirements. In his paper we present a CORBA based middleware system that adopts our technology of Routing Based Workflow (RBW). RBW has modeled the execution environment of cooperative components. Within RBW component instances are temporally bound to routing for their functionality execution. It is the temporal binding makes the dynamic reconfiguration of software components easy to realize and greatly simplify the hard problems of preserving consistency. © Springer-Verlag Berlin Heidelberg 2005.
Engineering Village;2007;Preemption threshold scheduling: Stack optimality, enhancements and analysis;;Using preemption threshold scheduling (PTS) in a multi-threaded real-time embedded system reduces system preemptions and hence reduces run-time overhead while still ensuring real-time constraints are met. However, PTS offers other valuable benefits. In this paper we investigate the use of PTS for hard real-time system with limited RAM. Our primary contribution is to prove the optimality of PTS among all preemptionlimiting methods for minimizing a system's total stack memory requirements. We then discuss characteristics of PTS and show how to reduce average worst-case response times. We also introduce a unified framework for using PTS with existing fixed-priority (e.g. rate-or deadline-monotonic), or dynamic-priority scheduling algorithms (e.g. earliest-deadline first). We evaluate the performance of PTS and our improvements using synthetic workloads and a real-time workload. We show PTS is extremely effective at reducing stack memory requirements. Our enhancements to PTS improve worst-case response-times as well. © 2007 IEEE.
Engineering Village;2007;Physical configuration on-line visualization of Xilinx Virtex-II FPGAs;;Xilinx Virtex-II / Virtex-II Pro FPGAs provide the possibility of partial and dynamic run-time reconfiguration. This feature can be used in adaptive systems providing the possibility to adapt to application requirements by exchanging parts of the hardware while other parts stay operative. This computing in time and space and many other fine grained adjustments within the architectures, opens new dimensions for electronic system design as well as for novel scheduling mechanisms based on well established graph-based algorithms in comparison to pure microprocessor based electronic systems. However, at the moment it is not possible to visualize the physical configuration of the chip and the manifold possibilities of manipulations on the device. This feature allows to demonstrate the system's behavior and helps to debug final integrated reconfigurable systems. This paper presents an approach to the system integration of an autonomously working on-line visualization stand alone IP-Core integrated on Xilinx Virtex-II and Virtex-II Pro FPGAs. © 2007 IEEE.
Engineering Village;2007;Informed evolution;;Ageless Software evolves, to meet new requirements, without reducing its efficiency or understandability. Here we introduce a methodology called Informed Evolution for supporting the construction and evolution of ageless software. This methodology integrates the software architecture (structure and constraints) and the system implementation (behaviour) within system execution. Evolution is effected by evolution patterns which are in turn guided by constraints specified in the software architecture. The availability of the software architecture and implementation at run-time ensures that changes are informed by design and implementation decisions, thus preserving efficiency and understandability. In this paper, we outline Informed Evolution, and describe how evolution patterns may be expressed for systems developed using this methodology. © Springer-Verlag Berlin Heidelberg 2007.
Engineering Village;2007;Secure service orchestration;;We present a framework for designing and composing services in a secure manner. Services can enforce security policies locally, and can invoke other services in a 'call-by-contract' fashion. This mechanism offers a significant set of opportunities, each driving secure ways to compose services. We discuss how to correctly plan service orchestrations in some relevant classes of services and security properties. To this aim, we propose both a core functional calculus for services and a graphical design language. The core calculus is called λreg [10]. It features primitives for selecting and invoking services that respect given behavioural requirements. Critical code can be enclosed in security framings, with a possibly nested, local scope. These framings enforce safety properties on execution histories. A type and effect system over-approximates the actual run-time behaviour of services. Effects include the actions with possible security concerns, as well as information about which services may be selected at run-time. A verification step on these effects allows for detecting the viable plans that drive the selection of those services that match the security requirements on demand. © Springer-Verlag Berlin Heidelberg 2007.
Engineering Village;2006;An agent based synchronization scheme for multimedia applications;;Synchronization of multimedia streams is one of the important issue in multimedia communications. In this paper, we propose an adaptive synchronization agency for synchronization of streams by using an agent based approach. The synchronization agency triggers one of the three synchronization mechanisms, point synchronization or real-time continuous or adaptive synchronization in order to adapt to the run-time and life-time presentation requirements of an application. The scheme or agency employs static and mobile agents for the following purpose: to estimate the network delays in real-time based on sustainable stream loss, to compute the skew, to monitor the loss and estimate the playout times of the presentation units. We have experimentally evaluated the scheme by using IBM Aglets and verified its functioning in terms of synchronization loss and mean buffering delays. The benefits of this agent based scheme are: asynchronous and autonomous delay estimation, flexibility, adaptability, software re-usability and maintainability. © 2005 Elsevier Inc. All rights reserved.
Engineering Village;2001;Towards support for ad-hoc multimedia bindings;;Multimedia applications of tomorrow face new challenges. As we move towards ubiquitous computing systems, users will require that the multimedia applications adopt to behave well in this new setting. This will require that developers of such applications are equipped with new development tools and abstractions to help construct these new applications. In this paper we investigate techniques to better support dynamical construction of multimedia bindings. Two alternatives are considered. The first lets the application choose the bindings it requires at run-time, from a pool of existing bindings. The second approach aims at helping the application dynamically construct the required binding. An evaluation of each of the approaches is given.
Engineering Village;2013;Harnessing evolutionary computation to enable dynamically adaptive systems to manage uncertainty;;This keynote talk and paper intend to motivate research projects that investigate novel ways to model, analyze, and mitigate uncertainty arising in three different aspects of the cyber-physical systems. First, uncertainty about the physical environment can lead to suboptimal, and sometimes catastrophic, results as the system tries to adapt to unanticipated or poorly-understood environmental conditions. Second, uncertainty in the cyber environment can have lead to unexpected and adverse effects, including not only performance impacts (load, traffic, etc.) but also potential threats or overt attacks. Finally, uncertainty can exist with the components themselves and how they interact upon reconfiguration, including unexpected and unwanted feature interactions. Each of these sources of uncertainty can potentially be identified at different stages, respectively run time, design time, and requirements, but their mitigation might be done at the same or a different stage. Based on the related literature and our preliminary investigations, we argue that the following three overarching techniques are essential and warrant further research to provide enabling technologies to address uncertainty at all three stages: model-based development, assurance, and dynamic adaptation. Furthermore, we posit that in order to go beyond incremental improvements to current software engineering techniques, we need to leverage, extend, and integrate techniques from other disciplines. © 2013 IEEE.
Engineering Village;2005;Efficient response time predictions by exploiting application and resource state similarities;;In large-scale Grids with many possible resources (clusters of computing elements) to run applications, it is useful that the resources can provide predictions of job response times so users or resource brokers can make better scheduling decisions. Two metrics need to be estimated for response time predictions: one is how long a job executes on the resource (application run time), the other is how long the job waits in the queue before starting (queue wait time). In this paper we propose an Instance Based Learning technique to predict these two metrics by mining historical workloads. The novelty of our approach is to introduce policy attributes in representing and comparing resource states, which is defined as the pool of running and queued jobs on the resource at the time to make a prediction. The policy attributes reflect the local resource scheduling policies and they can be automatically discovered using a genetic search algorithm. The main advantages of this approach compared with scheduler simulation are twofolds: Firstly, it has a better performance to meet the real time requirement of Grid resource brokering,, secondly, it is more general because the scheduling policies are learned from past observations. Our experimental results on the NIKHEF LCG production cluster show that acceptable prediction accuracy can be obtained, where the relative prediction errors for response times are between 0.35 and 0.70. © 2005 IEEE.
Engineering Village;1999;Dynamic precision management for loop computations on reconfigurable architectures;;Reconfigurable architectures promise significant performance benefits by customizing the configurations to suit the computations. Variable precision for computations is one important method of customization for which reconfigurable architectures are well suited. The precision of the operations can be modified dynamically at run-time to match the precision of the operands. Though the advantages of reconfigurable architectures for dynamic precision have been discussed before, we are not aware of any work which analyzes the qualitative and quantitative benefits which can be achieved. This paper develops a formal methodology for dynamic precision management. We show how the precision requirements can be analyzed for typical computations in loops by computing the precision variation curve. We develop algorithms to generate optimal schedules of configurations using the precision variation curves. Using our approach, we demonstrate 25%-37% improvement in the total execution time of an example loop computation on the XC6200 device.
Engineering Village;2005;Business processes management based on stratified grid;;In the context of current day collaborative business processes, seamlessly integrating the applications residing in different enterprises becomes necessary. Process centered BPM provides the capability for business partners to carry out such processes by providing a platform for cross enterprise applications to take part in the process. However, current business processes management systems based on services can not composite services automatically in order to meet user's requirements and lack the ability of dynamically configuration of services and resources. Roughly, existing methods can be classified into run-time binding based on task decomposition and automatic composition based on bottom-up method, but the former method needs decomposition of complex processes in advance while the later method is more complex. In addition, both methods are not goal-oriented. This paper introduces a stratified grid concept, and based on its characteristics, proposes a goal-oriented business processes management system. The method can search business processes, optimize the composite processes and configure resources in distributed stratified grid environment based on requirements and goals of users. Especially, the method can provide the user best global services and simplify the design of business process.
Engineering Village;2007;Implicit phasing for R6RS libraries;;The forthcoming Revised6 Report on Scheme differs from previous reports in that the language it describes is structured as a set of libraries. It also provides a syntax for defining new portable libraries. The same library may export both procedure and hygienic macro definitions, which allows procedures and syntax to be freely intermixed, hidden, and exported. This paper describes the design and implementation of a portable version of R6RS libraries that expands libraries into a core language compatible with existing R5RS implementations. Our implementation is characterized by its use of inference to determine when the bindings of an imported library are needed, e.g., run time or compile time, relieving programmers of the burden of declaring usage requirements explicitly. Copyright © 2007 ACM.
Engineering Village;2006;Proceedings - SBAC-PAD 2006 18TH International Symposium on Computer Architecture and High Performance Computing;;The proceedings contain 22 papers. The topics discussed include: towards production code effective portability among vector machines and microprocessors-based architectures,, data segmentation management infrastructure in a database grid,, detecting malicious manipulation in grid environments,, policy-based resource allocation in hierarchical virtual organizations for global grids,, a speculative trace reuse architecture with reduced hardware requirements,, controlling the power and area of neural branch predictors for practical implementation in high-performance processors,, a run-time system for efficient execution of scientific workflows on distributed environments,, dual-thread speculation: two threads in the machine are worth eight in the bush,, characterizing the performance of data management systems on hyper-threaded architectures,, and ultra-fast CPU performance prediction: extending the monte carlo approach.
Engineering Village;2006;Policy-driven model for autonomic management of Web services using mas;;Web service is an important technology in open distributed system. But because of the dynamical and heterogeneous nature of web services, it is difficult to be managed directly. On the other hand, multi-agent system provides a model for automatic discovery, negotiation and cooperation. It can be designed as a container of web services for autonomic management. This paper provides a policy-driven model base on multi-agent system that encapsulates the web service as an agent. It can supervise the dynamical agents and web services that may join in or leave out at run time according to high-level policies or business requirements. © 2006 IEEE.
Engineering Village;1999;Integrative approach to requirements modeling;;An approach to requirements modeling based on synergy between multiple views is presented. It integrates natural-language descriptions, visual executable models with run-time behavior monitoring and codegeneration facilities, and interactive graphical user-interface prototypes, through the use of dynamic scenarios. A conceptual design of an integrative requirements modeling system (currently implemented as research prototype) is described. Partial modeling techniques are used to preclude excessive redundancy and permit flexible balancing of emphasis between views. The general objective is to allow industrial teams to flexibly combine the advantages of informal and formal methods, to smoothly shift balance between them with incremental investment in tools and training, and to preserve continuity between old and new projects.
Engineering Village;2005;Component-based system development;;The development of an adequate technology for component-based development faces many challenges. This is in particular true for real-time and embedded systems. Based on the exposition in this section, we structure the issues into several groups Component specification: in the context of embedded systems, it is obvious that interface specifications of components must go beyond syntactic information and include functional and extra-functional characteristics and requirements. For real-time systems the temporal attributes of components and systems are of main interest. For embedded systems the properties specifying the resources and the properties related to dependability are important. However, there is still no consensus about how components for real-time systems should be specified. Prediction of system properties from component properties: Even if we assume that we can specify all the relevant properties of components, it is not necessarily known how they will determine the corresponding properties of systems of which they are composed. Moreover, existing component models do not provide support for predictable composition. In this, one should aim for interfaces providing full functional and extra-functional specifications of components are essential. Managing the interplay between achievable system requirements and component specifications: is complex, as the possible candidate components usually lack one or more required features. Further, the relations between the system requirements and component requirements are complex. Architecture specification: the use of components has an impact on the choice of the system architecture, as it must take into account not only the requirements, but also the available components. Component models: Component models for real-time systems are still in the very early phase of development. In general, existing component models do not support the specification of functional and extra-functional properties, in particular timing and QoS properties. Component evaluation and verification (possibly for certification): the trustworthiness of a component, which is the reliability of component in relation to its interface specification, is an important issue. The issue is difficult since the trend is to deliver components in binary form and the component development process is outside the control of component users. Protocols for component certification are of great interest Component repositories: which address the issues of how to store and retrieve components, how to index components in a component library, and how to find 'similar' components. Managing changes in component requirements: an important issue are changes to components over time and possible conflicts arising from different coexisting versions of a component within the same system. A precise interface specification should allow clarifying this issue. Update and replacement of components at run-time is useful for many real-time systems. In the context of design-time composition, it is a challenge to combine this feature with design-time optimization across component boundaries. For all areas, it is evident that appropriate tools are essential for a successful component-based development. In non real-time domains there exists various tools supporting model based and component-based development and they have proved to be successful, but in the real-time domains there is a lack of such tools. There is thus a unique opportunity for transferring essential results from research into industry through development of tool suites. © Springer-Verlag Berlin Heidelberg 2005.
Engineering Village;2010;Model-based platform-specific co-design methodology for dynamically partially reconfigurable systems with hardware virtualization and preemption;;To facilitate the development of the dynamically partially reconfigurable system (DPRS), we propose a model-based platform-specific co-design (MPC) methodology for DPRS with hardware virtualization and preemption. For DPRS analysis and validation, a model-based verification and estimation framework is proposed to make model-driven architecture (MDA) more realistic and applicable to the DPRS design. Considering inherent characteristics of DPRS and real-time system requirements, a semi-automatic model translator converts the UML models of DPRS into timed automata models with transition urgency semantics for model checking. Furthermore, a UML-based hardware/software co-design platform (UCoP) can support the direct interaction between the UML models and the real hardware architecture. Compared to the existing estimation methods, UCoP can provide accurate and efficient platform-specific verification and estimation. We also propose a hierarchical design that consists of a hardware virtualization mechanism for dynamically linking the device nodes, kernel modules, and on-demand reconfigurable hardware functions and a hardware preemption mechanism for further increasing the utilization of hardware resources per unit time. Further, we realize a dynamically partially reconfigurable network security system (DPRNSS) to show the applicability and practicability of the MPC methodology. The DPRNSS cannot only dynamically adapt some of its hardware functions at run-time to meet different system requirements, but also determine which mechanism will be used. Our experiments also demonstrate that the hardware virtualization mechanism can save the overall system execution time up to 12.8% and the hardware preemption mechanism can reduce up to 41.3% of the time required by reconfiguration-based methods. © 2010 Elsevier B.V. All rights reserved.
Engineering Village;2000;Unified compiler framework for control and data speculation;;Control speculation refers to the execution of instructions before it has been determined that they would be executed in the normal flow of execution. Data speculation refers to the execution of instructions with potentially incorrect operand values, and a typical example is to execute a load before its preceding aliasing stores. Both types of speculation are effective techniques to enrich instruction level parallelism, but the research work for these two types of speculation have remained largely independent so far and the required compiler support has not been well studied. This paper proposes a unified compiler framework to exploit both control and data speculation and provides an in-depth discussion of various compilation issues. The adopted recovery mechanism guarantees the original program semantics including exceptions fully recoverable from a mis-speculation. Cascaded speculation and predication are also addressed. We demonstrate the effectiveness of the compiler optimization techniques for control and data speculation in terms of run-time performance improvements and code size increases through experimental results.
Engineering Village;2007;Path-based error propagation analysis in composition of software services;;In Service-Oriented Architectures (SOA) composed services provide functionalities with certain non-functional properties that depend on the properties of the basic services. Models that represent dependencies among these properties are necessary to analyze non-functional properties of composed services. In this paper we focus on the reliability of a SOA. Most reliability models for software that is assembled from basic elements (e.g. objects, components or services) assume that the elements are independent, namely they do not take into account the dependencies that may exist between basic elements. We relax this assumption here and propose a reliability model for a SOA that embeds the 'error propagation' property. We present a path-based model that generates the possible execution paths within a SOA from a set of scenarios. The reliability of the whole system is then obtained as a combination of the reliability of all generated paths. On the basis of our model, we show on an example that the error propagation analysis may be a key factor for a trustworthy prediction of the reliability of a SOA. Such a reliability model for a SOA may support, during the system development, the allocation of testing effort among services and, at run time, the selection of functionally equivalent services offered by different providers. © Springer-Verlag Berlin Heidelberg 2007.
Engineering Village;2007;A job pause service under LAM/MPI+BLCR for transparent fault tolerance;;Checkpoint/restart (C/R) has become a requirement for long-running jobs in large-scale clusters due to a meantime-to-failure (MTTF) in the order of hours. After a failure, C/R mechanisms generally require a complete restart of an MPI job from the last checkpoint. A complete restart, however, is unnecessary since all but one node are typically still alive. Furthermore, a restart may result in lengthy job requeuing even though the original job had not exceeded its time quantum. In this paper, we overcome these shortcomings. Instead of job restart, we have developed a transparent mechanism for job pause within LAM/MPI+BLCR. This mechanism allows live nodes to remain active and roll back to the last checkpoint while failed nodes are dynamically replaced by spares before resuming from the last checkpoint. Our methodology includes LAM/MPI enhancements in support of scalable group communication with fluctuating number of nodes, reuse of network connections, transparent coordinated checkpoint scheduling and a BLCR enhancement for job pause. Experiments in a cluster with the NAS Parallel Benchmark suite show that our overhead for job pause is comparable to that of a complete job restart. A minimal overhead of 5.6% is only incurred in case migration takes place while the regular checkpoint overhead remains unchanged. Yet, our approach alleviates the need to reboot the LAM run-time environment, which accounts for considerable overhead resulting in net savings of our scheme in the experiments. Our solution further provides full transparency and automation with the additional benefit of reusing existing resources. Executing continues after failures within the scheduled job, i.e., the application staging overhead is not incurred again in contrast to a restart. Our scheme offers additional potential for savings through incremental checkpointing and proactive diskless live migration, which we are currently working on. © 2007 IEEE.
Engineering Village;2004;A debugging strategy based on the requirements of testing;;Testing and debugging activities consume a significant amount of the software development and maintenance budget. To reduce this cost, the use of testing information for debugging purposes has been advocated. In general, heuristics are used to select structural testing requirements (nodes, branches and definition-use associations) more closely related to the manifestation of a failure, which are then mapped into a piece of code. The intuition is that the selected piece of code is likely to contain the fault. However, this approach has its drawbacks. Heuristics that select a manageable piece of code are less likely to hit the fault and the piece of code itself does not provide enough guidance for program understanding - a major factor in program debugging. These problems occur because this approach relies only on static Information - a fragment of code. We introduce a strategy for fault localization that addresses these problems. The strategy - called the debugging strategy based on the requirements of testing (DRT) - is based on the investigation of indications (or hints) provided at run-time by data-flow testing requirements (definition-use associations). Our claim is that the selected definition-use associations may fail to hit the fault site, but still provide information useful for fault localization. The strategy's novelty and attractiveness are threefold: (i) the focus on dynamic information related to testing data,, (ii) implementation in state-of-the-practice symbolic debuggers with a low overhead,, and (iii) the use of algorithms which consume constant memory and are linear on the number of branches in the program. A case study shows that our claim is valid (for the subject program) and a prototype tool implements the strategy. Copyright © 2004 John Wiley & Sons, Ltd.
Engineering Village;2013;Towards preventing error propagation in a real-time Ethernet switch;;Flexible Time-Triggered communication (FTT) allows a distributed embedded system (DES) to adapt to changing real-time requirements at runtime. This facilitates the continuous operation of the DES under dynamic environments that change over time. However, for continuous operation, high reliability in the nodes of the DES is also crucial. This can be achieved using node replication, as long as failure independence between replicas is ensured, which calls for preventing the propagation of errors. Our goal is to prevent the propagation of Byzantine node behaviours and to ensure that local errors in the channel cannot disturb the global communication. For this, we construct the HaRTES/PG switch, a new switch based on the HaRTES implementation of FTT for Ethernet. This paper presents as a first step a study of the possible errors that may lead to Byzantine node behaviours and a global communication disturbance in HaRTES, as well as some ideas on how to prevent the propagation of these errors in HaRTES/PG. © 2013 IEEE.
Engineering Village;2012;Middleware for differentiated quality in spontaneous networks;;Spontaneous-network management requires application-driven middleware to address differentiated application-specific requirements at runtime. Real Ad hoc Multihop Peer-to-peer (RAMP) middleware easily deploys over existing and heterogeneous wireless networks, supporting adaptive and per-application strategies even in challenging scenarios, such as multimedia streaming. © 2012 IEEE.
Engineering Village;2013;'Computing' requirements in open source software projects;;Due to high dissimilarity with traditional software development, Requirements Engineering (RE) in Open Source Software (OSS) remains poorly understood, despite the visible success of many OSS projects. In this study, we approach OSS RE as a sociotechnical and distributed cognitive activity where multiple actors deploy heterogeneous artifacts to 'compute' requirements as to reach a collectively-held understanding of what the software is going to do. We conduct a case study of a popular OSS project, Rubinius (a Ruby programming language runtime environment). Specifically, we investigate the ways in which this project exhibits distribution of cognitive efforts along social, structural, and temporal dimensions and how its requirements computation takes place accordingly. In particular, we seek to generalize to a theoretical framework that explains how three temporally-ordered processes of distributed cognition in OSS projects, denoted excavation, instantiation, and testing-inthe- wild, tie together to form a powerful distributed computational structure to manage requirements. © (2013) by the AIS/ICIS Administrative Office All rights reserved.
Engineering Village;2009;Monitoring probabilistic properties;;Monitoring allows for checking if a system fulfils its requirements at runtime. This is required for quality assurance purposes. Currently several approaches exist to monitor standard and timing properties. However, a current challenge is to provide a comprehensive approach for monitoring probabilistic properties, as they are used to formulate performance, reliability, safety, and availability requirements. The main problem of these probabilistic properties is that there is no binary acceptance condition. To overcome this problem, this paper describes a monitoring approach called ProMo that is based on acceptance sampling and sequential hypothesis testing. This approach is validated based on several experiments that have been performed on an example system which provides medical assistance in remote areas. Copyright 2009 ACM.
Engineering Village;2006;Using product line techniques to build adaptive systems;;Adaptive systems are able to adapt their properties and resource requirements at runtime in response to dynamically varying user needs and resource constraints. With the emergence of mobile and service oriented computing, such variation is becoming increasingly common, and the need for adaptivity is increasing accordingly. Software product line engineering has proved itself as an efficient way to deal with varying user needs and resource constraints. In this paper we present an approach to building adaptive systems based on product line oriented techniques such as variability modeling and component based architectures. By representing the product line architecture at runtime, we are able to delegate much of the complexity of adaptation to a reusable adaptation platform. To validate our approach we have built a prototype adaptation platform and developed a few pilot applications exploiting the platform to achieve adaptivity. © 2006 IEEE.
Engineering Village;2010;Using requirements traceability links at runtime - A position paper;;During software development a large amount of varied information is created. It comprises the requirements specification and depending artifacts such as design, code or test cases, as well as supporting information such as traceability links. This information is intended to be used during development time. The research in requirements at runtime has so far focused on using the requirements specification at runtime. This paper explores how to use the existing traceability links between requirements and other artifacts at runtime. © 2010 IEEE.
Engineering Village;2008;Supporting the UML state machine diagrams at runtime;;Input models that are not completely checked generate ill-formed output models in MDA transformation processes. Model executability is a means for, at development time, simulating/testing models and thus making them compliant with requirements. At runtime, persistent models bring added values like the monitoring and control of applications through the observation of the active states, the guards which hold true, the occurring events... This paper on purpose presents a Java-based execution engine for the UML State Machine Diagrams. In order to incorporate this UML interpreter into MDA tools, the execution semantics of the UML State Machine Diagrams is first analyzed and next disambiguated. Execution semantics choices are thus proposed and justified accordingly. © 2008 Springer-Verlag Berlin Heidelberg.
Engineering Village;2009;From requirements to embedded software - Formalising the key steps;;Failure of a design to satisfy a system's requirements can result in schedule and cost overruns. When using current approaches, ensuring requirements are satisfied is often delayed until late in the development process during a cycle of testing and debugging. This paper introduces a more rigorous approach to design using Behavior Engineering, which has previously been applied primarily to requirements analysis and specification development. To support design with Behavior Engineering we introduce the embedded Behavior Runtime Environment, a virtual machine created to execute a Behavior Engineering design on an embedded system. The result is a model-driven development approach that can create embedded system software that satisfies its requirements, as a result of applying the development process.
Engineering Village;2012;Efficient resource management based on non- Functional requirements for sensor/actuator networks;;In this paper, a novel resource management approach is presented for publish-subscribe middleware for sensor/actuator networks. The resource management was designed with the possibility to add non-functional requirements at runtime to subscription messages. This approach allows utilizing service level agreements that can then be employed in order to guarantee a certain quality of service or to reduce the energy consumption of a sensor node in a sensor/actuator network. As an example, a sensor/actuator network for facility logistics system (a conveyor belt system) is evaluated with respect to energy consumption. This sensor/actuator network is mostly controlled by image processing based sensor nodes. It is shown that an adaptive processing interval for these sensor nodes can reduce the energy consumption of the entire network. The utilization of non-functional requirements allows the system to adapt - After software development - To context changes such as the extension of the conveyor belt systems topology.
Engineering Village;2005;Self-optimization of large scale wildfire simulations;;The development of efficient parallel algorithms for large scale wildfire simulations is a challenging research problem because the factors that determine wildfire behavior are complex. These factors make static parallel algorithms inefficient, especially when large number of processors is used because we cannot predict accurately the propagation of the fire and its computational requirements at runtime. In this paper, we propose an Autonomic Runtime Manager (ARM) to dynamically exploit the physics properties of the fire simulation and use them as the basis of our self-optimization algorithm. At each step of the wildfire simulation, the ARM decomposes the computational domain into several natural regions (e.g., burning, unburned, burned) where each region has the same temporal and special characteristics. The number of burning, unburned and burned cells determines the current state of the fire simulation and can then be used to accurately predict the computational power required for each region. By regularly monitoring and analyzing the state of the simulation, and using that to drive the runtime optimization, we can achieve significant performance gains because we can efficiently balance the computational load on each processor. Our experimental results show that the performance of the fire simulation has been improved by 45% when compared with a static portioning algorithm. © Springer-Verlag Berlin Heidelberg 2005.
Engineering Village;2010;From ODRL-S to low-level DSL: A case study based on license compliance in service oriented systems;;In this paper, we present a case study in the framework of COMPAS, a research project focused on supporting compliance monitoring and verification in service based systems. In this paper, we also illustrate how we translate high-level service licenses (specified in Open Digital Rights Language for Services (ODRL-S)) to low-level rules for verifying the compliance requirements at runtime. We validate our approach by architecting a compliance driven service oriented system, where at runtime business processes are monitored for compliance.
Engineering Village;2008;Engineering pluripotent information systems;;A pluripotent information system is an open and distributed information system that (i) automatically adapts at runtime to changing operating conditions, and (ii) satisfies both the requirements anticipated at development time, and those unanticipated before but relevant at runtime. Engineering pluripotency into an information system therefore responds to two recurring critical issues: (i) the need for adaptability given the uncertainty in a system's operating environment, and (ii) the difficulty to fully anticipate and account for all possible stakeholders' requirements at development time and respond to the change of requirements at runtime. We draw on our group's research efforts over the last two years to show and discuss how pluripotency can be engineered into information systems.
Engineering Village;2007;A fault detection mechanism for fault-tolerant SOA-based applications;;Fault tolerance is an important capability for SOA-based applications, since it ensures the dynamic composition of services and improves the dependability of SOA-based applications. Fault detection is the first step of fault detection, so this paper focuses on fault detection, and puts forward a fault detection mechanism, which is based on the theories of artificial neural network and probability change point analysis rather than static service description, to detect the services that fail to satisfy performance requirements at runtime. This paper also gives reference model of fault-tolerance control center of Enterprise Services Bus. © 2007 IEEE.
Engineering Village;2010;A Map-reduce system with an Alternate API for multi-core environments;;Map-reduce framework has received a significant attention and is being used for programming both large-scale clusters and multi-core systems. While the high productivity aspect of map-reduce has been well accepted, it is not clear if the API results in efficient implementations for different subclasses of data-intensive applications. In this paper, we present a system MATE (Map-reduce with an AlternaTE API), that provides a high-level, but distinct API. Particularly, our API includes a programmer-managed reduction object, which results in lower memory requirements at runtime for many data-intensive applications. MATE implements this API on top of the Phoenix system, a multi-core map-reduce implementation from Stanford. We evaluate our system using three data mining applications and compare its performance to that of both Phoenix and Hadoop. Our results show that for all the three applications, MATE outperforms Phoenix and Hadoop. Despite achieving good scalability, MATE also maintains the easy-to-use API of map-reduce. Overall, we argue that, our approach, which is based on the generalized reduction structure, provides an alternate highlevel API, leading to more efficient and scalable implementations. © 2010 IEEE.
Engineering Village;2015;The e-mobility case study;;Electro-mobility (e-mobility) is one of the promising technologies being considered by automotive OEMs as an alternative to internal combustion engines as a means of propulsion. The e-mobility case study provides a novel example of a relevant industry application with in the ASCENS framework. An overview of the system design is given which describes how e-mobility is conceptualized and then transformed using the ensemble development life cycle (EDLC) approach into a distributed autonomic (i.e self-aware, self-adaptive) component-based software system.The system requirements engineering is based on the state-of-the affairs (SOTA) approach and the invariant refinement method (IRM)which are both revisited and applied. Regarding the implementation and deployment of the system, a dependable emergent ensembles of components(DEECo) approach is utilized. The DEECo components and ensembles are coded and deployed using the Java-based jDEECo runtime environment. The runtime environment integrates the multi-agent transport simulation tool (MATSim), which is used to predict the effects of the physical interactions of users, vehicles and infrastructure resources.jDEECo handles multiple MATSim instances to allow for different belief states between components and ensembles. © Springer International Publishing Switzerland2015.
Engineering Village;2011;Empirical evaluation of tropos4AS modelling;;Our work addresses the challenges arising in the development of self-adaptive software, which has to work autonomously in an unpredictable environment, fulfilling the objectives of its stakeholders, while avoiding failure. In this context we developed the Tropos4AS framework, which extends the AOSE methodology Tropos to capture and detail at design time the specific decision criteria needed for a system to guide selfadaptation at run-time, and to preserve the concepts of agent and goal model explicitly along the whole development process until run-time. In this paper, we present the design of an empirical study for the evaluation of Tropos4AS, with the aim of assessing the modeling effort, expressiveness and comprehensibility of Tropos4AS models. This experiment design can be reused for the evaluation of other modeling languages extensions.
Engineering Village;2005;Autonomic runtime system for large scale parallel and distributed applications;;The development of efficient parallel algorithms for large scale wildfire simulations is a challenging research problem because the factors that determine wildfire behavior are complex,, they include fuel characteristics and configurations, chemical reactions, balances between different modes of heat transfer, topography, and fire/atmosphere interactions. These factors make static parallel algorithms inefficient, especially when large number of processors are used because we cannot predict accurately the propagation of the fire and its computational requirements at runtime. In this paper, we present an Autonomic Runtime Manager (ARM) to dynamically exploit the physics properties of the fire simulation and use them as the basis of our self-optimization algorithm. At each step of the wildfire simulation, the ARM decomposes the computational domain into several natural regions (e.g., burning, unburned, burned) where each region has the same temporal and special characteristics. The number of burning, unburned and burned cells determines the current state of the fire simulation and can then be used to accurately predict the computational power required for each region. By regularly monitoring the state of the simulation and analyzing it, and use that to drive the runtime optimization, we can achieve significant performance gains because we can efficiently balance the computational load on each processor. Our experimental results show that the performance of the fire simulation has been improved by 45% when compared with a static portioning algorithm that does not take into considerations the state of the computations. © Springer-Verlag Berlin Heidelberg 2005.
Engineering Village;2012;Accelerating throughput-aware runtime mapping for heterogeneous MPSoCs;;Modern embedded systems need to support multiple time-constrained multimedia applications that often employ multiprocessor-systems-on-chip (MPSoCs). Such systems need to be optimized for resource usage and energy consumption. It is well understood that a design-time approach cannot provide timing guarantees for all the applications due to its inability to cater for dynamism in applications. However, a runtime approach consumes large computation requirements at runtime and hence may not lend well to constrained-aware mapping. In this article, we present a hybrid approach for efficient mapping of applications in such systems. For each application to be supported in the system, the approach performs extensive design-space exploration (DSE) at design time to derive multiple design points representing throughput and energy consumption at different resource combinations. One of these points is selected at runtime efficiently, depending upon the desired throughput while optimizing for energy consumption and resource usage. While most of the existing DSE strategies consider a fixed multiprocessor platform architecture, our DSE considers a generic architecture, making DSE results applicable to any target platform. All the compute-intensive analysis is performed during DSE, which leaves for minimum computation at runtime. The approach is capable of handling dynamism in applications by considering their runtime aspects and providing timing guarantees. The presented approach is used to carry out a DSE case study for models of real-life multimedia applications: H.263 decoder, H.263 encoder, MPEG-4 decoder, JPEG decoder, sample rate converter, and MP3 decoder. At runtime, the design points are used to map the applications on a heterogeneous MPSoC. Experimental results reveal that the proposed approach provides faster DSE, better design points, and efficient runtime mapping when compared to other approaches. In particular, we show that DSE is faster by 83% and runtime mapping is accelerated by 93% for some cases. Further, we study the scalability of the approach by considering applications with large numbers of tasks. © 2012 ACM.
Engineering Village;2014;Worst-case scheduling of software tasks a constraint optimization model to support performance testing;;Real-Time Embedded Systems (RTES) in safety-critical domains, such as maritime and energy, must satisfy strict performance requirements to be deemed safe. Therefore, such systems have to be thoroughly tested to ensure their correct behavior even under the worst operating conditions. In this paper, we address the need of deriving worst case scenarios with respect to three common performance requirements, namely task deadlines, response time, and CPU usage. Specifically, we investigate whether this worst-case analysis can be effectively re-expressed as a Constrained Optimization Problem (COP) over the space of possible inputs to the system. Solving this problem means finding the sets of inputs that maximize the chance to violate performance requirements at runtime. Such inputs can in turn be used to test if the target RTES meets the expected performance even in the worst case. We develop an OPL model for IBM ILOG CP Optimizer that implements a task priority-based preemptive scheduling, and apply it to a case study from the maritime and energy domain. Our validation shows that (1) the input to our model can be provided with reasonable effort in an industrial setting, and (2) the COP effectively identifies test cases that maximize deadline misses, response time, and CPU usage. © 2014 Springer International Publishing Switzerland.
Engineering Village;2012;Identifying test requirements by analyzing SLA guarantee terms;;Service Level Agreements (SLAs) are used to specify the negotiated conditions between the provider and the consumer of services. In this paper we present a stepwise method to identify and categorize a set of test requirements that represent the potential situations that can be exercised regarding the specification of each isolated guarantee term of an SLA. This identification is addressed by means of devising a set of coverage levels that allow grading the thoroughness of the tests. The utilization of these test requirements would focus on twofold objectives: (1) the generation of a test suite that allows exercising the situations described in the test requirements and (2) the support for the derivation of a monitoring plan that checks the compliance of these requirements at runtime. The approach is illustrated over an eHealth case study. © 2012 IEEE.
Engineering Village;2005;Monitoring with behavior view diagrams for debugging;;UML sequence diagrams are widely used during requirements analysis and design for specifying the expected message exchanges among a set of objects in various scenarios for the program to perform a certain task. In this paper, we present the behavior view diagrams, a type of extended sequence diagrams, to facilitate execution monitoring during debugging. Using a behavior view diagram, software developers can precisely specify the runtime objects whose behaviors will be monitored during debugging. Software developers can also specify the important message exchanges to be observed among these objects during the progress of various scenarios, and may further define the monitoring actions to be performed for inspecting the program state when a message exchange is observed. We also present a debugger that can automatically monitor the program execution using the information specified in a behavior view diagram. Through this monitoring, the debugger can not only check whether the scenarios are progressed as intended, but also check whether the actions performed by the program have the the desired effects on the program states. Therefore, it will be useful for detecting and localizing bugs. © 2005 IEEE.
Engineering Village;2013;Support for intention driven cloud government entitlement services;;This paper examines the role of judicial cloud based services in assurance of law enforcement and access to justice for citizen social networks. We are using open service oriented model standard XACML as dynamic runtime based business process modeling, while citizens can dynamically express their instant needs using intentions. We foresee these intentions as temporal goal oriented models by automatically reasoning the requirements driven analysis for these goals. Our approach provides complete business model architecture that stimulates citizen needs. The new contribution of our approach shows how temporal changes of citizen's intentions shall provide different role based entitlement as forced by government laws. We examine judicial entitlement services like notarization services and how different purpose based citizen-to-citizen, citizen-to-business scenarios for each service shall be helpful to anticipate context interactions, thus to avoid business conflict of interest between each of citizens, their societies and government laws and policies. Failing to fulfill minimum requirement for such law enforcement, will reason for automatic re-routing of such intention model to use litigation services, which also contain automatic escalation and management to court based litigation services. © 2013 IEEE.
Engineering Village;2012;DRF4SOA: A dynamic reconfigurable framework for designing autonomic application based on SOA;;Service Oriented Architecture (SOA) allows modeling dynamic interaction between heterogeneous providers. Coupling with Service Component Architecture (SCA) enables governing the development of complex applications. The dynamic reconfiguration of such applications is a key feature, since it gives measures to ensure runtime adaptation in order to guarantee Quality of Service (QoS) and manage the performance. For instance, recovering a QoS degradation requires the identification of its sources and the capacity of reconfiguration planning and execution. This paper presents DRF4SOA, a Dynamic Reconfigurable Framework to design autonomic application based on SOA, that implements the autonomic control loop phases (Monitoring, Analysis, Planning, Execution) with SCA in order to provide flexibility to support evolving of itself and to include new non-functional requirements at runtime. © 2012 IEEE.
Engineering Village;2012;Intention-oriented modelling support for socio-technical driven elastic cloud applications;;Businesses have already started to exploit potential uses of cloud computing as a new paradigm for promoting their services. Although the general concepts they practically focus on are: viability, survivability, adaptability, etc., however, there is still a lack of forming mechanisms to sustain viability with adaptation of new requirements in cloud-based applications. This has inspired a pressing need to adopt new methodologies and abstract models which support system acquisition for self-adaptation, thus guaranteeing autonomic cloud application behaviour. In this paper we use state-of-the-art Neptune framework as runtime adaptive software development environment developed mainly for distributed computing supported with intention-oriented modelling language. The representation and adaptation of goal based model artifacts and their intrinsic properties requirements will in turn support distributed service based applications running in the cloud. This harnesses obedient system behaviour with respect to its functional and non-functional characteristics. © 2012 IEEE.
Engineering Village;2010;Template based SOA framework for dynamic and adaptive composition of Web Services;;Automated Web service composition is currently one of the major research problems in the area of service oriented computing. Web services facilitate seamless business-to-business integration. Whenever it becomes difficult to find a single service for a particular task, a composition of services that can together perform the given task, is required. In general, this is accomplished using Service Oriented Architecture (SOA). However, the requirements of the users are frozen before the system locates, composes and executes the required services. The response is not personalized to the user environment. Further, conventional web services cannot handle the context and the context aware web services need to contain the context processing logic. Hence, we propose a framework for dynamic composition of Web services using templates in SOA. This framework allows maximum flexibility for the users to change their requirements at runtime and provides adaptive composition irrespective of whether a web service is context enabled or not. © 2010 IEEE.
Engineering Village;2011;A fast real-time rendering method of 3D terrain using out-of-core visualization;;In this paper, we propose an improved algorithm for real-time rendering of large 3D scene, in which we combine quad-tree hierarchy, level of detail (LOD) and out-of-core algorithm. In order to get an efficient rendering method, we construct a scene hierarchy to maintain the rendering scene tree, use quad-tree to simplify terrain meshes, and import the out-of-core algorithm to reduce memory requirements. There are two key steps in his algorithm: Firstly, we will simplify the scene data using quad-tree algorithm. Then we could get a set of simplified meshes for rendering. Secondly, View-dependent out-of-core method is imported to determine which part of meshes should be rendering or delete from the memory. Then we could reduce the memory requirements at runtime. In the last part of the paper, we use terrain data to test out algorithm. Comparing to traditional quad-tree algorithm our method run faster and need less memory requirements.
Engineering Village;2004;A method to develop feasible requirements for java mobile code application;;We propose a method for analyzing trade-off between an environment where a Java mobile code application is running and requirements for the application. In particular, we focus on the security-related problems that originate in low-level security policy of the code-centric style of the access control in Java runtime. As the result of this method, we get feasible requirements with respect to security issues of mobile codes. This method will help requirements analysts to compromise the differences between customers' goals and realizable solutions. Customers will agree to the results of the analysis by this method because they can clearly trace the reasons why some goals are achieved but others are not. We can clarify which functions can be performed under the environment systematically. We also clarify which functions in mobile codes are needed so as to meet the goals of users by goal oriented requirements analysis(GORA). By comparing functions derived from the environment and functions from the goals, we can find conflicts between the environments and the goals, and also find vagueness of the requirements. By resolving the conflicts and by clarifying the vagueness, we can develop bases for the requirements specification.
Engineering Village;2014;Managing expectations: Runtime negotiation of information quality requirements in event-based systems;;Interconnected smart devices in the Internet of Things (IoT) provide fine-granular data about real-world events, leveraged by service-based systems using the paradigm of event-based systems (EBS) for invocation. Depending on the capabilities and state of the system, the information propagated in EBS differs in content but also in properties like precision, rate and freshness. At runtime, consumers have different dynamic requirements about those properties that constitute quality of information (QoI) for them. Current approaches to support quality-related requirements in EBS are either domain-specific or limited in terms of expressiveness, flexibility and scope as they do not allow participants to adapt their behavior. We introduce the generic concept of expectations to express, negotiate and enforce arbitrary requirements about information quality in EBS at runtime. In this paper, we present the model of expectations, capabilities and feedback based on generic properties. Participants express requirements and define individual tradeoffs between them as expectations while system features are expressed as capabilities. We discuss the algorithms to (i) negotiate requirements at runtime in the middleware by matching expectations to capabilities and (ii) adapt participants as well as the middleware. We illustrate the architecture for runtime-support in industry-strength systems by describing prototypes implemented within a centralized and a decentralized EBS.
Engineering Village;2015;Instruction-cache locking for improving embedded systems performance;;Cache memories in embedded systems play an important role in reducing the execution time of applications. Various kinds of extensions have been added to cache hardware to enable software involvement in replacement decisions, improving the runtime over a purely hardware-managed cache. Novel embedded systems, such as Intel's XScale and ARM Cortex processors, facilitate locking one or more lines in cache,, this feature is called cache locking. We present a method in for instruction-cache locking that is able to reduce the average-case runtime of a program. We demonstrate that the optimal solution for instruction cache locking can be obtained in polynomial time. However, a fundamental lack of correlation between cache hardware and software program points renders such optimal solutions impractical. Instead, we propose two practical heuristics-based approaches to achieve cache locking. First, we present a static mechanism for locking the cache, in which the locked contents of the cache are kept fixed over the execution of the program. Next, we present a dynamic mechanism that accounts for changing program requirements at runtime. We devise a cost-benefit model to discover the memory addresses that should be locked in the cache. We implement our scheme inside a binary rewriter, widening the applicability of our scheme to binaries compiled using any compiler. Results obtained on a suite of MiBench benchmarks show that our static mechanism results in 20% improvement in the instruction-cache miss rate on average and up to 18% improvement in the execution time on average for applications having instruction accesses as a bottleneck, compared to no cache locking. The dynamic mechanism improves the cache miss rate by 35% on average and execution time by 32% on instruction-cache-constrained applications. © 2015 ACM.
Engineering Village;2010;Utilizing parallelism of TMR to enhance power efficiency of reliable ASIC designs;;Due to aggressive scaling, reliability issues influence the design process of integrated circuits more and more. A well known technique to tackle these issues represents Triple Modular Redundancy (TMR). It strongly improves reliability of a design at the expense of at least tripled area and power consumption. In this contribution, we propose an enhanced TMR approach that significantly decreases the power overhead of conventional TMR designs. Therefore, the control logic was modified so as to switch between a TMR mode and a parallel mode. This parallel mode allows the circuit to operate with decreased frequency without losing performance by taking advantage of the parallelism offered by the tripled design. Achieved results of investigations on the ISCAS benchmark circuits show power savings of up to 50 % with a small reliability penalty compared to a conventional TMR approach for permanent failures. We also propose strategies how to utilize both operating modes in order to balance the design concerning reliability and power consumption requirements at runtime. ©2010 IEEE.
Engineering Village;2015;Challenges in rendering and maintaining trustworthiness for long-living software systems;;Trustworthiness plays a key role in acceptance and adoption of software by the end-users. When maintaining long-living software systems, trustworthiness has to be addressed since trust of the end-user is volatile and can change over time. In this paper, we discuss the challenges regarding trustworthiness of long-living software systems. Trustworthiness should be considered in the whole life-cycle of a long-living system, i.e., in all development phases aiming at building trustworthiness into the core of the system at design-time and later maintaining it during run-time. But, our focus in this paper is on challenges in requirements engineering and also planning for the run-time activities, e.g., what are the needed monitor interfaces, what are the planned actions and how are the execution interfaces for performing those actions.
Engineering Village;2003;Compiler-decided dynamic memory allocation for scratch-pad based embedded systems;;This paper presents a highly predictable, low overhead and yet dynamic, memory allocation strategy for embedded systems with scratch-pad memory. A scratch-pad is a fast compiler-managed SRAM memory that replaces the hardware-managed cache. It is motivated by its better real-time guarantees vs cache and by its significantly lower overheads in energy consumption, area and overall runtime, even with a simple allocation scheme [4]. Existing scratch-pad allocation methods are of two types. First, software-caching schemes emulate the workings of a hardware cache in software. Instructions are inserted before each load/store to check the software-maintained cache tags. Such methods in-cur large overheads in runtime, code size, energy consumption and SRAM space for tags and deliver poor real-time guarantees just like hardware caches. A second category of algorithms partitions variables at compile-time into the two banks. For example, our previous work in [3] derives a provably optimal static allocation for global and stack variables and achieves a speedup over all earlier methods. However, a drawback of such static allocation schemes is that they do not account for dynamic program behavior. It is easy to see why a data allocation that never changes at runtime cannot achieve the full locality benefits of a cache. In this paper we present a dynamic allocation method for global and stack data that for the first time, (i) accounts for changing program requirements at runtime (ii) has no software-caching tags (iii) requires no run-time checks (iv) has extremely low overheads, and (v) yields 100% predictable memory access times. In this method data that is about to be accessed frequently is copied into the SRAM using compiler-inserted code at fixed and infrequent points in the program. Earlier data is evicted if necessary. When compared to a provably optimal static allocation our results show runtime reductions ranging from 11% to 38%, averaging 31.2%, using no additional hardware support. With hardware support for pseudo-DMA and full DMA, which is already provided in some commercial systems, the runtime reductions increase to 33.4% and 34.2% respectively. Copyright 2002 ACM.
Engineering Village;2005;Dynamic preemption threshold scheduling for specific real-time control systems;;Application specific operating systems (ASOS) are developing quickly as a new trend in real-time control systems development. It often belongs to system on chip. The scheduling for ASOS should satisfy two basic demands (a) Context switching overheads are not significant,, (b) The scheduling should use small amount of RAM memory. According to characteristics of ASOS, we present a novel scheduling algorithm, named dynamic preemption threshold (DPT) scheduling, which integrates preemption threshold scheduling into the EDF (earliest deadline first). The scheduling can achieve greater processor utilization, theoretically even up to all of a processor capacity. Meanwhile, the preemption times between tasks can be effectively decreased using DPT scheduling by two ways: 1) threads allocating,, 2) dynamic thresholds regularly adjusting at runtime. With the reduction of task preemptions, memory requirements are also decreased. In addition, the DPT gives an approach to transform a static model to dynamic model seamlessly. The DPT algorithm can perfectly schedule a mixed task set with preemptive and non-preemptive tasks, and subsumes both as special cases. Thus it remains the scheduling flexibility and also decreases unnecessary context switching and memory requirements at runtime. © 2005 IEEE.
Engineering Village;2014;A Requirements Monitoring Infrastructure for Very-Large-Scale Software Systems;;[Context and motivation] Approaches for requirements monitoring check the compliance of systems with their requirements during operation. [Question/problem] Despite many advances, requirements monitoring remains challenging particularly for very-large-scale software systems (VLSS) with system-of-systems architectures. [Principal ideas/results] In this research preview we describe key characteristics of industrial VLSS and discuss implications for requirements monitoring. Furthermore, we report on our ongoing work of developing a requirements monitoring infrastructure addressing these characteristics. [Contribution] Our infrastructure supports runtime monitoring of requirements across systems,, variability management of requirements-based monitors,, and the integration of monitoring data from different sources in a VLSS. © 2014 Springer International Publishing Switzerland.
Engineering Village;2008;Requirements capture with RCAT;;NASA spends millions designing and building spacecraft for its missions. The dependence on software is growing as spacecraft become more complex. With the increasing dependence on software comes the risk that bugs can lead to the loss of a mission. At NASA's Jet Propulsion Laboratory new tools are being developed to address this problem. Logic model checking [9] and runtime verification [5] can increase the confidence in a design or an implementation. A barrier to the application of such property-based checks is the difficulty in mastering the requirements notations that are currently available. For these techniques to be easily usable, a simple but expressive requirement specification method is essential. This paper describes a requirements capture notation and supporting tool that graphically captures formal requirements and converts them into automata that can be used in model checking and for runtime verification. © 2008 IEEE.
Engineering Village;2015;27th International Conference on Advanced Information Systems Engineering, CAiSE 2015;;The proceedings contain 31 papers. The special focus in this conference is on Collaborative Computing, Business Process Modeling, Languages, Complex Information Management, Requirements Elicitation and Enterprise Data Management. The topics include: An approach to separation of concerns in crowdsourced data management,, run-time and task-based performance of event detection techniques for twitter,, a graphical notation for resource assignments in business processes,, revising the vocabulary of business process element labels,, the requirements and needs of global data usage in product lifecycle management,, a clustering approach for protecting GIS vector data,, need-to-share and non-diffusion requirements verification in exchange policies,, leveraging familiar environments to prime stakeholder memory during process elicitation,, handling regulatory goal model families as software product lines,, managing data warehouse traceability,, specification and incremental maintenance of linked data mashup views,, a model-driven approach to enterprise data migration,, detecting complex changes during metamodel evolution,, completing workflow traces using action languages,, extracting decision logic from process models,, equivalence transformations for the design of interorganizational data-flow,, automatic generation of optimized process models from declarative specifications,, towards the automated annotation of process models,, discovery and validation of queueing networks in scheduled processes,, verification and validation of UML artifact-centric business process models,, empirical challenges in the implementation of IT portfolio management,, deriving artefact-centric interfaces for overloaded web services,, fundamental systems thinking concepts for is engineering and a practical tutorial on the framework for evaluation in design science research.
Engineering Village;2012;Stateful requirements monitoring for self-repairing socio-technical systems;;Socio-technical systems consist of human, hardware and software components that work in tandem to fulfill stakeholder requirements. By their very nature, such systems operate under uncertainty as components fail, humans act in unpredictable ways, and the environment of the system changes. Self-repair refers to the ability of such systems to restore fulfillment of their requirements by relying on monitoring, reasoning, and diagnosing on the current state of individual requirements. Self-repair is complicated by the multi-agent nature of socio-technical systems, which demands that requirements monitoring and self-repair be done in a decentralized fashion. In this paper, we propose a stateful requirements monitoring approach by maintaining an instance of a state machine for each requirement, represented as a goal, with runtime monitoring and compensation capabilities. By managing the interactions between the state machines, our approach supports hierarchical goal reasoning in both upward and downward directions. We have implemented a customizable Java framework that supports experimentation by simulating a socio-technical system. Results from our experiments suggest effective and precise support for a wide range of self-repairing decisions in a socio-technical setting. © 2012 IEEE.
Engineering Village;2010;A method to acquire compliance monitors from regulations;;Developing software systems in heavily regulated industries requires methods to ensure systems comply with regulations and law. A method to acquire finite state machines (FSM) from stakeholder rights and obligations for compliance monitoring is proposed. Rights and obligations define what people are permitted or required to do,, these rights and obligations affect software requirements and design. The FSM allows stakeholders, software developers and compliance officers to trace events through the invocation of rights and obligations as pre- and post-conditions. Compliance is monitored by instrumenting runtime systems to report these events and detect violations. Requirements and software engineers specify the rights and obligations, and apply the method using three supporting tasks: 1) identify under-specifications, 2) balance rights with obligations, and 3) generate finite state machines. Preliminary validation of the method includes FSMs generated from U.S. healthcare regulations and tool support to parse these specifications and generate the FSMs. ©2010 IEEE.
Engineering Village;2007;Towards security monitoring patterns;;Runtime monitoring is performed during system execution to detect whether the system's behaviour deviates from that described by requirements. To support this activity we have developed a monitoring framework that expresses the requirements to be monitored in event calculus - a formal temporal first order language. Following an investigation of how this framework could be used to monitor security requirements, in this paper we propose patterns for expressing three basic types of such requirements, namely confidentiality, integrity and availability. These patterns aim to ease the task of specifying confidentiality, integrity and availability requirements in monitorable forms by non-expert users. The paper illustrates the use of these patterns using examples of an industrial case study. Copyright 2007 ACM.
Engineering Village;2011;A practical formal approach for requirements validation and verification of dependable systems;;Classical requirements validation methods usually work with static behavioral models, and under the assumption that there are no dependencies and interactions between the requirements. Requirements verification is mostly done by statically analyzing the design artifacts and by running tests. This work presents a practical formal approach for requirements validation and verification (V&V) of dependable systems, under two different perspectives: development and acquisition. The approach considers the system's dynamic behavior that is formally represented as statechart assertions and validated using JUnit test scenarios. Runtime execution monitoring (REM) data is used to create JUnit tests to verify the system's behavior against the assertions. The V&V activities are supported by the StateRover tool. Two space systems case studies are briefly presented. As dependability often manifests as decidable system sequencing behaviors, the main contribution of this work is centered on the validation and verification of such behaviors. © 2011 IEEE.
Engineering Village;2012;Scenario-driven development of context-aware adaptive web services;;Context-awareness and adaptability are highly desirable features for web services that operate in dynamic environments. In recent years, a number of approaches have been proposed to support the development of such services. However, the requirements elicitation of this kind of services and the synthesis of their design models from the requirements are still major challenges. In this paper, we propose a novel scenario-driven approach to developing context-aware adaptive web services. Our approach enables the elicitation of a web service's requirements as two sets of scenarios: functional and adaptation. The functional scenarios capture the service's functionality while the adaptation scenarios represent the service's adaptation logic to cope with runtime context changes. We also support the synthesis of the service's design model from its scenarios, and the automatic transformation from the service's design model to the executable service code. To demonstrate the applicability of our approach, we have used it to develop a context-aware travel guide service. © 2012 Springer-Verlag.
Engineering Village;2005;The Eclipse 3.0 platform: Adopting OSGi technology;;From its inception Eclipse was mainly designed to be a tooling platform, but with Version 3.0, Eclipse is now evolving toward a Rich Client Platform (RCP). This change, driven by the open-source community, brought a whole set of new requirements and challenges for the Eclipse platform, such as dynamic plug-in management, services, security, and improved performance. This paper describes the path from the proprietary Eclipse 2.1 runtime to the new Eclipse 3.0 runtime based on OSGi™ specifications. It details the motivation for such a change and discusses the challenges this change presented. © Copyright 2005 by International Business Machines Corporation.
Engineering Village;2011;Study of goal-oriented migrating workflow system based on mobile computing paradigm;;Workflow systems provide the automation of business processes where a collection of tasks is organized between participants according to a set of defined rules to accomplish some business goals. However, due to the lack of flexible mechanisms, such as fast changing customer requirements and enterprise goal shifts, a static workflow definition designed at build time is inflexible to meet the complex, dynamic situations that happen at run time. This paper presents a goaland agent-based migrating workflow model. Structure of goal model in goal-oriented migrating instance and a way how GoMI pursues its goals is presented. A case study on goal-oriented migrating workflow system using the proposed methodology is also illustrated in this paper, The major contribution of this paper includes: (1) a definition of GoMWf model adopted the mobile computing paradigm and a framework of GoMWf,, (2) a definition of goal-oriented migrating instance (GoMI) and its architecture based on BDI model,, (3) evaluation of goal-based workflow system and process-based workflow system.
Engineering Village;2008;A reflective framework for fine-grained adaptation of aspect-oriented compositions;;Dynamic Aspect Oriented Programming (AOP) technologies typically provide coarse-grained mechanisms for adapting aspects that cross-cut a system deployment,, i.e. whole aspect modules can be added and removed at runtime. However, in this paper we demonstrate that adaptation of the finer-grained elements of individual aspect modules is required in highly dynamic systems and applications. We present AspectOpenCOM, a principled, reflection-based component framework that provides a meta object protocol capable of fine-grained adaptation of deployed aspects. We then evaluate this solution by eliciting a set of requirements for dynamic fine-grained adaptation from a series of case studies, and illustrate how the framework successfully meets these criteria. We also investigate the performance gains of fine-grained adaptation versus a coarse-grained approach. © 2008 Springer-Verlag Berlin Heidelberg.
Engineering Village;2001;QoS-aware middleware for ubiquitous and heterogeneous environments;;Middleware systems have emerged in recent years to support applications in heterogeneous and ubiquitous computing environments. Specifically, future middleware platforms are expected to provide quality of service support, which is required by a new generation of QoS-sensitive applications such as media streaming and e-commerce. This article presents four key aspects of a QoS-aware middleware system: QoS specification to allow description of application behavior and QoS parameters,, QoS translation and compilation to translate specified application behavior into candidate application configurations for different resource conditions,, QoS setup to appropriately select and instantiate a particular configuration,, and finally, QoS adaptation to adapt to runtime resource fluctuations. We also provide a comparison of existing QoS-aware middleware systems in these four aspects.
Engineering Village;2009;Operational semantics of goal models in adaptive agents;;Several agcnt-oricnted software engineering methodologies address the emerging challenges posed by the increasing need of adaptive software. A common denominator of such methodologies is the paramount importance of the concept of goal model in order to understand the requirements of a software system. Goal models consist of goal graphs representing AND/OR-dccomposition of abstract goals down to operationalisable leaf-level goals. Goal models are used primarily in the earlier phases of software engineering, for social modelling, requirements elicitation and analysis, to concretise abstract objectives, to detail them and to capture alternatives for their satisfaction. Although various agent programming languages incorporate the notion of (leaf-level) goal as a language construct, none of them natively support the definition of goal mod-els. However, the semantic gap between goal models used at design-time and the concept of goal used at implementation and execution time represent a limitation especially in the development of self-adaptive and fault-tolerant systems. In such systems, design-time knowledge on goals and variability becomes relevant at run-time, to take autonomous decisions for achieving high level objectives correctly. Recently, unifying operational semantics for (leaf) goals have been proposed [15]. We extend this work to define an operational semantics for the behaviour of goals in goal models, maintaining the flexibility of using different goal types and conditions. We use a simple example to illustrate how the proposed approach effectively deals with the semantic gap between design-time goal models and run-time agent implementations. Categories and Subject Descriptors 1.2.11 [Artificial intelligence]: Distributed Artificial Intelligence-Intelligent agents, Languages and structures,, D.2.1 [Software Engineering]: Requirements/Specifications,, D.3.1 [Programming Languages]: Formal Definitions and Theory General Terms. Copyright © 2009, International Foundation for Autonomous Agents and Multiagent Systems.
Engineering Village;2004;Study on integrated simulation environment for air defense combat;;Building Integrated Simulation Environment (ISE) is the effective method for air defense combat simulation. The architecture and functions of ISE of air defense combat are presented on the basis of requirements analysis in this paper, and the run-time framework of air defense combat simulation based on such ISE is discussed in detail. ISE provides strong tools for the development of air defense combat simulation application with reusability and creditability.
Engineering Village;2012;Requirements-driven qualitative adaptation;;Coping with run-time uncertainty pose an ever-present threat to the fulfillment of requirements for most software systems (embedded, robotic, socio-technical, etc.). This is particularly true for large-scale, cooperative information systems. Adaptation mechanisms constitute a general solution to this problem, consisting of a feedback loop that monitors the environment and compensates for deviating system behavior. In our research, we apply a requirements engineering perspective to the problem of designing adaptive systems, focusing on developing a qualitative software-centric, feedback loop mechanism as the architecture that operationalizes adaptivity. In this paper, we propose a framework that provides qualitative adaptation to target systems based on information from their requirements models. The key characteristc of this framework is extensibility, allowing for it to cope with qualitative information about the impact of control (input) variables on indicators (output variables) in different levels of precision. Our proposal is evaluated with a variant of the London Ambulance System case study. © 2012 Springer-Verlag.
Engineering Village;2013;From GMoDS models to object-oriented specifications in Event-B;;This paper represents a first attempt to express in Event-B the models of the GMoDS (The Goal Model for Dynamic Systems) methodology. GMoDS is a major result of the research related to Organization-Based Multiagent System Engineering methodology (O-MaSE), allowing to specify goals during requirements engineering process and then to use them throughout the system development and at runtime. We choose Event-B as a modelling language for specifying the GMoDS models for two reasons: (a) Event-B has the concept of proving correctness, which supports the accuracy of software development, and (b) its supporting tool, RODIN, is open-source, and it is used in many software industrial applications. Because Event-B is not object-oriented, and because several concepts used in GMoDS are related to object-orientation, we have included in the corresponding Event-B specifications the object-oriented concepts used in the GMoDS models, without changing the syntax and semantics of Event-B, and without using the UML-B tool. © 2013 IEEE.
Engineering Village;1998;Proceedings of the 1998 International Conference on Software Engineering;;The proceedings contains 64 papers from the 1998 IEEE 20th International Conference on Software Engineering. Topics discussed include: experiences with software process improvement,, formal modeling,, reverse engineering,, object orientation,, Internet,, mobile codes,, security of data,, object-oriented technology,, large scale and complex system development,, computer supported cooperative work,, project estimation and simulation,, project and workflow management,, data analysis,, cost estimation,, and requirements engineering.
Engineering Village;2010;SIGAda 2010 - Proceedings of the 2010 ACM International Conference on Ada and Related Technologies;;The proceedings contain 9 papers. The topics discussed include: effective requirements engineering,, use of object oriented technologies in high reliability system,, designing real-time, concurrent, and embedded software systems using UML and Ada,, Ada for parallel, embedded, and real-time applications,, unmanned systems with Ada and RTEMS,, systems software integrity assurance,, a deterministic run-time environment for Ada-05 on the ATmega 16 microcontroller,, a methodology for avoiding known compiler problems using static analysis,, wouldn't it be nice to have software labels?,, experience report: Ada & Java integration in the FAA's ERAM SWIM program,, real-time system development in Ada using Lego&reg Mindstorms&reg NXT,, parallelism generics for Ada 2005 and beyond,, extending Ada to support multi-core based monitoring and fault tolerance,, towards Ada 2012: an interim report,, and the rise, fall and persistence of Ada.
Engineering Village;2009;Engineering adaptive requirements;;Challenges in the engineering of self-adaptive software have been recently discussed and summarised in a seminal research road map. Following it, we focus on requirements engineering issues, with a two-fold, long term objective. The first objective is to support the system analyst to engineer adaptive requirements at requirements-time, the second is to make software able to reason on requirements at run-time in order to enable a goal-oriented adaptation. Along the first objective, in this position paper we propose a characterisation of adaptive requirements. Moreover, we investigate how available techniques aimed at eliciting and specifying domain properties, stakeholders' goals and preferences, can provide a practical support to the analyst while capturing adaptive requirements. © 2009 IEEE.
Engineering Village;2010;First step towards a domain specific language for self-adaptive systems;;Self-adaptive systems are capable of autonomously modifying their behavior at run-time in response to changing environmental conditions. In order to modify the behavior, requirements play an important role, as they tend to change for these systems. For this we need to identify those requirements that are concerned with the adaptability features of the self-adaptive systems. In order to cope with the uncertainty inherent in self-adaptive systems, requirements engineering languages for these systems should include explicit constructs. RELAX is a requirement engineering language for self-adaptive systems that incorporates uncertainty into the specification of these systems. To go one step further, we aim at developing a domain specific language that would bridge the gap between requirements and the overall system model. The first step that is illustrated in this paper is to build a textual editor for RELAX. ©2010 IEEE.
Engineering Village;2004;A propositional logic-based method for verification of feature models;;The feature model is a domain/product-line oriented requirements model based on hierarchical structure and explicit variability modeling, and has been adopted by several important software reuse methods. However, with respect to the problem of verification of constraints on features and verification of partially customized feature models, these methods tend to be semi-formal and offer little formal assistance. In this paper, we propose a propositional logic-based method for the verification of feature models at different binding times. The most important characteristic of this method is that it integrates the logical verification with binding times, which makes it can be used to verify any partially customized feature models at any binding time (except run-time). In this method, constraints on features are formalized by logical sentences. Thus, the verification of feature models is converted into satisfaction problems in the logic. With this formal method, verification problems such as the detection of inconsistent constraints or the detection of conflicting or unnecessary binding resolutions can be automatically revealed. © Springer-Verlag Berlin Heidelberg 2004.
Engineering Village;2015;Towards an ontology-based approach to safety management in cooperative intelligent transportation systems;;The expected increase in transports of people and goods across Europe will aggravate the problems related to traffic congestion, accidents and pollution. As new road infrastructure alone would not solve such problems, Intelligent Transportation Systems (ITS) has been considered as new initiatives. Due to the complexity of behaviors, novel methods and tools for the requirements engineering, correct-by-construction design, dependability, product variability and lifecycle management become also necessary. This chapter presents an ontology-based approach to safety management in Cooperative ITS (C-ITS), primarily in an automotive context. This approach is supposed to lay the way for all aspects of ITS safety management, from simulation and design, over run-time risk assessment and diagnostics. It provides the support for ontology driven ITS development and its formal information model. Results of approach validation in CarMaker are also given in this Chapter. The approach is a result of research activities made in the framework of Swedish research initiative, referred to as SARMITS (Systematic Approach to Risk Management in ITS Context). © Springer International Publishing Switzerland 2015.
Engineering Village;2015;Designing adaptive systems;;In this work, we investigate the interplay between requirements and architecture in the context of adaptive systems. Furthermore, we propose the Multi-Level Adaptation for Software Systems (MULAS) framework. It is centred on the iterative and incremental refinement of a goal model, towards the creation of a design goal model, which can be used at runtime to drive adaptation on a system that is properly instrumented. Moreover, the framework includes a toolsupported process for generating statechart behavioural models from a design goal model. Copyright © 2015 for this paper by its authors.
Engineering Village;2012;Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop;;Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods. © 2012 Elsevier Inc. All rights reserved.
Engineering Village;2014;The observer-based technique for requirements validation in embedded real-time systems;;Model-based requirements validation is an increasingly attractive approach to discovering hidden flaws in requirements in the early phases of systems development life cycle. The application of using traditional methods such as model checking for the validation purpose is limited by the growing complexity of embedded real-time systems (ERTS). The observer-based technique is a lightweight validation technique, which has shown its potential as a means of validating the correctness of model behaviors. In this paper, the novelty of our contributions is three-fold: 1) we formally define the observer constructs for our formal specification language namely the Timed Abstract State Machine (TASM) language and, 2) we propose the Events Monitoring Logic (EvML) to facilitate the observer specification and, 3) we show how to execute observers to validate the requirements describing the functional behaviors and non-functional properties (such as timing) of ERTS. We also illustrate the applicability of the extended TASM language through an industrial application of a Vehicle Locking-Unlocking system. © 2014 IEEE.
Engineering Village;2003;Low complexity crosstalk cancellation through line selection in upstream VDSL;;Crosstalk is the major source of performance degradation in VDSL. A number of crosstalk cancellation techniques have been proposed to address this. Whilst these schemes lead to large performance increases they also have high run-time complexities, a problem which grows rapidly with the number of lines within a binder. Since the majority of crosstalk typically comes from only a few dominant crosstalkers it is possible to do partial crosstalk cancellation. In this paper we present a low-complexity, partial crosstalk cancellation technique for VDSL based on line selection. We derive the optimal line selection technique, and several low-complexity selection algorithms which give near-optimal performance in most scenarios. These techniques lead to significant reductions in run-time complexity whilst giving similar performance to full crosstalk cancellation.
Engineering Village;2011;Are your sites down? Requirements-driven self-tuning for the survivability of web systems;;Running in a highly uncertain and greatly complex environment, Web systems cannot always provide full set of services with optimal quality, especially when work loads are high or subsystem failures are frequent. Hence, it is significant to continuously maintain a high satisfaction level of survivability, hereafter survivability assurance, while relaxing or sacrificing certain quality or functional requirements that are not crucial to the survival of the entire system. After giving a value-based interpretation to survivability assurance to facilitate a quantitative analysis, we propose a requirements-driven self-tuning method for the survivability assurance of Web systems. Maintaining an enriched and live goal model, our method adapts to runtime tradeoff decisions made by our PID (proportional-integral-derivative) controller and goal-oriented reasoner for both quality and functional requirements. The goal-based configuration plans produced by the reasoner is carried out on the live goal model, and then mapped into system architectural configurations. Experiments on an online shopping system are conducted to validate the effectiveness of the proposed method. © 2011 IEEE.
Engineering Village;2013;Self-adaptive systems requirements modelling: Four related approaches comparison;;When developing Self Adaptive Systems (SAS), their highly adaptiveness has to be taken into account as early as the requirements elicitation. Because such systems modify their behaviour at run-time in response to changing environmental conditions, Non Functional Requirements (NFR's) play an important role. One has to identify as early as possible the requirements that are adaptable. Because of the inherent uncertainty in these systems, goal based approaches can help in the development of their requirements. In order to cope with this purpose, we have defined a combined approach based on several requirements modelling techniques. In this paper we use a common case study and well defined comparison criteria to illustrate the way those techniques can benefit from each other. This submission is a synthesis and hence make some reference of more specific requirements models submissions. © 2013 IEEE.
Engineering Village;2014;Development and optimization of a multivariate RP-UPLC method for determination of telmisartan and its related substances by applying a two-level factorial design approach: Application to quality control study;;Herein we report a simple, specific and rapid reverse-phase ultraperformance liquid chromatographic (RP-UPLC) method that was developed and optimized by using a two-level factorial design approach for telmisartan (TLM) and its eight critical process-related impurities. This method has excellent resolution of critical impurities, low cost of analysis and a reduced run time,, moreover, it shows excellent resolution by using a BEH C18 column (100 × 2.1 mm, 1.7 μm) with phosphate buffer (10 mM) as mobile phase A (pH = 2.5),, acetonitrile:methanol:water as mobile phase B (75:15:10 v/v/v),, column oven temperature, 30 °C,, flow rate, 0.33 mL min-1,, and 2.0 μL injection volume, which is monitored at 235 nm. Furthermore, the peak purity was checked for specificity, and the robustness of the method was enhanced by applying a two-level factorial design. In this design, the influence of different factors such as % methanol in mobile phase B, flow, column oven temperature, pH of the mobile phase on the responses such as resolution between impurity E and TLM, and % recovery of impurity E was investigated. Based on the response of the design, the developed method is precise, accurate, linear, successfully implemented for monitoring the reactions in R&D laboratories, and thus suitable for quality control purposes as well as the stability analysis of TLM-active pharmaceutical ingredients. © 2014 The Royal Society of Chemistry.
Engineering Village;2001;Residual requirements and architectural residues;;Monitoring running systems is a useful technique available to requirements engineers, to ensure that systems meet their requirements and in some cases to ensure that they obey the assumptions under which they were created. This report studies relationships between the original requirements and the monitoring infrastructure. Here we postulate that the monitored requirements are in fact just compilations of original requirements, called 'residual' requirements. Dynamic architectural models have become important tools for expressing requirements on modern distributed systems. Monitoring residual requirements will be seen to involve 'architectural residues,' skeletal run-time images of the original logical architecture. An example sales support system is used to illustrate the issues involved, employing modest extensions to the Acme architecture description language to reason about architectural dynamism.
Engineering Village;2007;Combining model processing and middleware configuration for building distributed high-integrity systems;;Requirements of High Integrity systems now encompass distribution mechanisms along with strong junctional and non-functional features (run-time support for hardware, dependability, safety, analyzability). In this paper, we show how model processing help addressing such needs. We present a generic distribution model suitable for High Integrity systems, and demonstrate how a high-level modeling deployment view allows one to greatly reduce the model complexity. Finally, we conclude by assessing a case study. © 2007 IEEE.
Engineering Village;2004;Quantification of 5-azacytidine in plasma by electrospray tandem mass spectrometry coupled with high-performance liquid chromatography;;5-Azacytidine (5AC), a nucleoside analogue and hypomethylating agent, has anticancer properties and has been utilized in the treatment of various malignancies. 5AC is unstable and rapidly hydrolyzed to several by-products, including 5-azacytosine and 5-azauracil. A sensitive, reliable method was developed to quantitate 5AC using LC/MS/MS to perform pharmacokinetic and pharmacodynamic studies on 5AC combination therapy trials. Blood samples were collected in a heparinized tube and immediately processed for storage. To increase the stability of 5AC in plasma, 25 ng/mL tetrahydrouridine was added to the plasma and snap frozen. Plasma samples were extracted using acetonitrile then cleaned up by Oasis MCX ion exchange solid-phase extraction cartridges. 5AC was separated on an YMC Jsphr M80™ C18 column with gradient elution of ammonium acetate (2 mM) with 0.1% formic acid and methanol mobile phase. 5AC elutes at 5.0 ± 0.2 min with a total run time of 30 min. Identification was through positive-ion mode and multiple reaction monitoring mode at m/z+ 244.9 &rarr 113.0 for 5AC and m/z+ 242.0 &rarr 126.0 for 5-methyl-2′-deoxycytidine, the internal standard. The lower limit of quantitation of 5AC was 5 ng/mL in human plasma, and linearity was observed from 5 to 500 ng/mL fitted by linear regression with 1/x weight. This method is 50 times more sensitive than previously published assays and successfully allows studies to characterize the pharmacokinetics and pharmacodynamics of 5AC. © 2004 Elsevier B.V. All rights reserved.
Engineering Village;2015;Requirements-driven self-optimization of composite services using feedback control;;In an uncertain and changing environment, a composite service needs to continuously optimize its business process and service selection through runtime adaptation. To achieve the overall satisfaction of stakeholder requirements, quality tradeoffs are needed to adapt the composite service in response to the changing environments. Existing approaches on service selection and composition, however, are mostly based on quality preferences and business processes decisions made statically at the design time. In this paper, we propose a requirements-driven self-optimization approach for composite services. It measures the quality of services (QoS), estimates the earned business value, and tunes the preference ranks through a feedback loop. The detection of unexpected earned business value triggers the proposed self-optimization process systematically. At the process level, a preference-based reasoner configures a requirements goal model according to the tuned preference ranks of QoS requirements, reconfiguring the business process according to its mappings from the goal configurations. At the service level, selection decisions are optimized by utilizing the tuned weights of QoS criteria. We used an experimental study to evaluate the proposed approach. Results indicate that the new approach outperforms both fixed-weighted and floating-weighted service selection approaches with respect to earned business value and adaptation flexibility. © 2008-2012 IEEE.
Engineering Village;2011;Requirements evolution: From assumptions to reality;;Requirements evolution is a main driver for systems evolution. Traditionally, requirements evolution is associated to changes in the users' needs and environments. In this paper, we explore another cause for requirements evolution: assumptions. Requirements engineers often make assumptions stating, for example, that satisfying certain sub-requirements and/or correctly executing certain system functionalities would lead to reach a certain requirement. However, assumptions might be, or eventually become, invalid. We outline an approach to monitor, at runtime, the assumptions in a requirements model and to evolve the model to reflect the validity level of such assumptions. We introduce two types of requirements evolution: autonomic (which evolves the priorities of system alternatives based on their success/failure in meeting requirements) and designer-supported (which detects loci in the requirements model containing invalid assumptions and recommends designers to take evolutionary actions). © 2011 Springer-Verlag.
Engineering Village;1997;Integrated specification and analysis of functional, temporal, and resource requirements;;The Graphical Communicating Shared Resources, GCSR, is a specification language with a precise, operational semantics for the specification and analysis of real-time systems. GCSR allows a designer to integrate the functional and temporal requirements of a real-time system along with its run-time resource requirements. The integration is orthogonal in the sense that it produces system models that are easy to modify, e.g., to reflect different resource requirements, allocations and scheduling disciplines. In addition, it renders the verification of resource related requirements natural and straightforward. The formal semantics of GCSR allows the simulation of a system model and the thorough verification of system requirements through equivalence checking and state space exploration. This paper reviews GCSR and reports our experience with the production cell case study.
Engineering Village;2012;Privacy arguments: Analysing selective disclosure requirements for mobile applications;;Privacy requirements for mobile applications offer a distinct set of challenges for requirements engineering. First, they are highly dynamic, changing over time and locations, and across the different roles of agents involved and the kinds of information that may be disclosed. Second, although some general privacy requirements can be elicited a priori, users often refine them at runtime as they interact with the system and its environment. Selectively disclosing information to appropriate agents is therefore a key privacy management challenge, requiring carefully formulated privacy requirements amenable to systematic reasoning. In this paper, we introduce privacy arguments as a means of analysing privacy requirements in general and selective disclosure requirements (that are both content- and context-sensitive) in particular. Privacy arguments allow individual users to express personal preferences, which are then used to reason about privacy for each user under different contexts. At runtime, these arguments provide a way to reason about requirements satisfaction and diagnosis. Our proposed approach is demonstrated and evaluated using the privacy requirements of BuddyTracker, a mobile application we developed as part of our overall research programme. © 2012 IEEE.
Engineering Village;2004;Goal and scenario driven product line development;;Product line development has proven a successful approach to achieve strategic and large-grained reuse and hence time-to-market and productivity. A key to successful software product lines is to identify and anlyze the right functionality for reusable implementation, and thus perform delated requirements analysis for product lines to exploit commonality and variability (C&V) within a family of related systems. In this paper, we describe the goal and scenario driven approach for developing software product lines, which elicits product line requirements and analyzes C&V in products of a product line, as well as supports developing a paricluar product in the product line. We also discuss our ultimate goal that is to develop a dynamic software product line, which can produce new products at runtime by dynamic reconfiguration of the product line based on goals and scenarios. © 2004 IEEE.
Engineering Village;2014;The requirements problem for adaptive systems;;Requirements Engineering (RE) focuses on eliciting, modeling, and analyzing the requirements and environment of a system-to-be in order to design its specification. The design of the specification, known as the Requirements Problem (RP), is a complex problem-solving task because it involves, for each new system, the discovery and exploration of, and decision making in a new problem space. A system is adaptive if it can detect deviations between its runtime behavior and its requirements, specifically situations where its behavior violates one or more of its requirements. Given such a deviation, an Adaptive System uses feedback mechanisms to analyze these changes and decide, with or without human intervention, how to adjust its behavior as a result. We are interested in defining the Requirements Problem for Adaptive Systems (RPAS). In our case, we are looking for a configurable specification such that whenever requirements fail to be fulfilled, the system can go through a series of adaptations that change its configuration and eventually restore fulfilment of the requirements. From a theoretical perspective, this article formally shows the fundamental differences between standard RE (notably Zave and Jackson [1997]) and RE for Adaptive Systems (see the seminal work by Fickas and Feather [1995], to Letier and van Lamsweerde [2004], and up to Whittle et al. [2010]). The main contribution of this article is to introduce the RPAS as a new RP class that is specific to Adaptive Systems. We relate the RPAS to RE research on the relaxation of requirements, the evaluation of their partial satisfaction, and the monitoring and control of requirements, all topics of particular interest in research on adaptive systems [de Lemos et al. 2013]. From an engineering perspective, we define a protoframework for solving RPAS, which illustrates features needed in future frameworks for adaptive software systems. © 2014 ACM.
Engineering Village;2014;Improving business value assurance in large-scale IT projects - A quantitative method based on founded requirements assessment;;The probability of IT project failures can be mitigated more successfully when discovered early. To support a more insightful management of IT projects, which may also facilitate an early detection of IT project failures, transparency regarding a project's cash flows shall be increased. Therefore, an appropriate analysis of a project's benefits, costs, requirements, their respective risks and interdependencies is inevitable. However, to date, in requirements engineering only few methods exist that appropriately consider these factors when estimating the ex ante project business case. Furthermore, empirical studies reveal that a lot of risk factors emerge during the runtime of projects why the ex ante valuation of IT projects even with respect to requirements seems insufficient. Therefore, using the Action Design Research approach, we design, apply, and evaluate a practicable method for value-based continuous IT project steering especially for large-scale IT projects. © 2014 ACM.
Engineering Village;2014;Flexibility requirements in real-world process scenarios and prototypical realization in the care domain;;Flexibility is a key concern in business process management and mature solutions and systems have been developed during the last years. What can be observed is that the approaches mostly consider process instances that are executed based on a process schema reflecting a process type, e.g., an order process. The process instances might be adapted during runtime in an individual manner (ad-hoc changes) or the process schema evolves due to, for example, new regulations. We studied cases from four different domains for their requirements on flexibility. These use cases are characterized by long running, highly adaptive, and individual instances, i.e., instances that are not based on a common process schema, but develop during runtime based on context and processrelevant data. The requirements analysis shows that can only part of them can be met by existing approaches. To illustrate an initial solution meeting the identified requirements, a prototypical implementation of the care domain use case is demonstrated, followed by a discussion of lessons learned and a research agenda.
Engineering Village;2012;Service-driven migrating of enterprise information systems: A case study;;There are many business information systems under operation supporting the essential business processes of large scale multi-national enterprises, which forms the essential IT assets of these organizations. However, the isolation of these systems, improper process configurations and unbalanced resource allocations often leads to the deterioration of these enterprises, or even major management crisis. Service-oriented computing is considered a possible remedy for such issues with relatively low migration cost that has attracted much industrial attention. In this paper, through a survey to an industrial consulting case from a Southeastern Asia Garment Manufacture, we discuss how service requirements are elicited, how to evolve conventional management information systems into composite service systems that is reconfigurable at runtime, so as to realize resources optimization and efficiency improvement. The purpose of this paper is to analyze the know how of migrating conventional management information systems into service-oriented architecture, from a requirements engineering perspective. © 2012 IEEE.
Engineering Village;2014;Rationalism with a dose of empiricism: Case-based reasoning for requirements-driven self-adaptation;;Requirements-driven approaches provide an effective mechanism for self-adaptive systems by reasoning over their runtime requirements models to make adaptation decisions. However, such approaches usually assume that the relations among alternative behaviours, environmental parameters and requirements are clearly understood, which is often simply not true. Moreover, they do not consider the influence of the current behaviour of an executing system on adaptation decisions. In this paper, we propose an improved requirements-driven self-adaptation approach that combines goal reasoning and case-based reasoning. In the approach, past experiences of successful adaptations are retained as adaptation cases, which are described by not only requirements violations and contexts, but also currently deployed behaviours. The approach does not depend on a set of original adaptation cases, but employs goal reasoning to provide adaptation solutions when no similar cases are available. And case-based reasoning is used to provide more precise adaptation decisions that better reflect the complex relations among requirements violations, contexts, and current behaviours by utilizing past experiences. Our experimental study with an online shopping benchmark shows that our approach outperforms both requirements-driven approach and case-based reasoning approach in terms of adaptation effectiveness and overall quality of the system. © 2014 IEEE.
Engineering Village;2010;2010 1st International Workshop on Requirements@Run.Time, RE@RunTime 2010;;The proceedings contain 7 papers. The topics discussed include: goal-oriented requirements modeling for running systems,, towards a continuous requirements engineering framework for self-adaptive systems,, continuous adaptive requirements engineering: an architecture for self-adaptive service-based applications,, using requirements traceability links at runtime - a position paper,, run-time monitoring of system performance: a goal-oriented and system architecture simulation approach,, and adaptive monitoring of software requirements.
Engineering Village;2011;Enforcing safety requirements for industrial automation systems at runtime position paper;;Current industrial automation systems are becoming more and more complex, and typically involve different phases of engineering, such as design time and runtime. System requirements, which are usually elicited during design time by engineers, currently are not sufficiently represented at runtime, like the runtime enforcement of safety requirements for industrial automation systems. Such kind of enforcement usually is very hard to model and predict at design time. Hence, the need exists to capture and manage safety requirements at design time and runtime, since safety requirements of industrial automation systems may lead to high risks if not addressed properly. In this position paper, we introduce a safety requirements enforcement framework and the using of Boilerplates for requirements elicitation and by explicitly modeling the runtime requirements knowledge for further application. We illustrate and evaluate the approach with data from a real-world case study in the area of industrial process systems. Major result was that the Boilerplates and explicit engineering knowledge are well suited to capture and enforce runtime safety requirements of industrial automation systems. © 2011 IEEE.
Engineering Village;2012;Run-time model evaluation for requirements model-driven self-adaptation;;A self-adaptive system adjusts its configuration to tolerate changes in its operating environment. To date, requirements modeling methodologies for self-adaptive systems have necessitated analysis of all potential system configurations, and the circumstances under which each is to be adopted. We argue that, by explicitly capturing and modelling uncertainty in the operating environment, and by verifying and analysing this model at runtime, it is possible for a system to adapt to tolerate some conditions that were not fully considered at design time. We showcase in this paper our tools and research results. © 2012 IEEE.
Engineering Village;2002;Supporting objects in run-time bytecode specialization;;This paper describes a run-time specialization system for the Java language. One of the main difficulties of supporting the full Java language resides in a sound yet effective management of references to objects. This is because the specialization process may share references with the running application that executes the residual code, and because side-effects through those references by the specialization process could easily break the semantics of the running application. To cope with these difficulties, we elaborate requirements that ensure sound run-time specialization. Based on them, we design and implement a run-time specialization system for the Java language, which exhibits, for instance, approximately 20-25% speed-up factor for a ray-tracing application.
Engineering Village;2006;FPGA implementation of dynamic run-time behavior reconfiguration in robots;;Using a single robot for multiple operations has been a significant problem for researchers in Robotics since available space, cost, and power consumption are constraints to the increasing number of behaviors on a robot. An efficient technique to implement multiple behaviors in robots using FPGA based partial reconfigurable hardware is presented in this paper. Robots built using reconfigurable FPGAs can have their functionalities modified at run-time without completely taking the robot off-line. Resource efficiency increases because only the modules corresponding to the current behavior are implemented in the FPGA while inactive modules are stored in an external memory. The approach is validated through a case study where teams of robots are configured to meet application specific requirements and the run-time behaviors of these robots are modified dynamically using Xilinx Virtex-II Pro FPGAs. © 2006 IEEE.
Engineering Village;2005;Run-time monitoring of requirements for systems composed of Web-services: Initial implementation and evaluation experience;;This paper describes a framework supporting the run-time monitoring of requirements for systems implemented as compositions of web-services specified in BPEL. The requirements that can be monitored are specified in event calculus. The paper presents an overview of the framework and describes the architecture and implementation of a tool that we have developed to ope rationalise it. It also presents the results of a preliminary experimental evaluation of the framework.
Engineering Village;2015;Run-time verification of MSMAS norms using event calculus;;Modelling Self-managing Multi Agent Systems (MSMAS) is a software development methodology that facilitates designing and developing complex distributed systems based on the multivalent systems paradigm. MSMAS uses a declarative modelling style to capture system requirements by specifying four types of what we call system norms over: the system goals, the system roles, the business activities, and communications. MSMAS utilises system norms to capture system requirements in a formal language which can subsequently be monitored and verified at runtime. In this paper we present the main elements of MSMAS and introduce MSMAS defined norm types. We model the life cycle of MSMAS norms as non-atomic activities and formally express them as Event Calculus (EC) theories. Our acclimatisation of MSMAS system norms as first-order EC allows for reasoning with a metric time representation which we illustrate through a monitoring example of two execution traces to verify the system compliance with its intended design requirements and show how to detect any violation of norms. © 2014 IEEE.
Engineering Village;2011;Foreword: 2nd workshop requirements@run.time;;The 2nd edition of the Workshop requirements@run.time was held at the 19th International Conference on Requirements Engineering (RE 2011) in the city of Trento, Italy on the 30th of August 2011. It was organized by Nelly Bencomo, Emmanuel Letier, Jon Whittle, Anthony Finkelstein, and Kris Welsh. This foreword presents a digest of the discussions and presentations that took place during the workshop. © 2011 IEEE.
IEEE;2011;Reasoning about adaptive requirements for self-adaptive systems at runtime;Feedback,,Planning,,Requirements Engineering,,Self-Adaptive Systems;Increasing proliferation of mobile applications challenge the role of requirements engineering (RE) in developing customizable and adaptive software applications for the end-users. Such adaptive applications need to alter their behavior while monitoring and evaluating the changes in the environment at runtime by being aware of their end-user's needs, context and resources. More specifically, these applications should be able to: (i) reason about their own requirements and refine and validate them at run-time by involving end-users, if necessary,, (ii) provide solutions for the refined or changed requirements at runtime, for instance by exploiting available services. In this position paper we focus on the first issue. We propose to extend our previous work on adaptive requirements with preference-based reasoning and automated planning to enable a continuous adaptive reasoning of requirements at runtime. We describe this vision using a navigation system example and highlight challenges.
IEEE;2010;Continuous adaptive requirements engineering: An architecture for self-adaptive service-based applications;Requirements Engineering,,Self-adaptive systems,,Service based Applications;Engineering self-adaptive service-based applications significantly challenges the role of requirements engineering (RE). Such systems need to cope with the evolving requirements at runtime by monitoring the changes in users' preferences and in the environment, evaluating the changes and enacting a suitable behavior ensuring an acceptable level of quality for their users. This calls for continuous reappraisal of their requirements specification enabling them to reason for them at run-time. Recently, we proposed a novel framework for Continuous Adaptive RE (CARE) supporting self-adaptive service-based applications and on conceptual tools needed to realize RE at run-time. In this position paper, we focus on a conceptual architecture for the CARE framework, and illustrate how it can work using scenarios from travel domain. Potential impact of our work and useful integration with recent studies are discussed, highlighting open points for future research.
IEEE;1998;Reconciling system requirements and runtime behavior;;This paper considers the problem of system deviations from requirements specifications. Such deviations may arise from lack of anticipation of possible behaviors of environment agents at specification time, or from evoking conditions in this environment. We discuss an architecture and a development process for monitoring system requirements at runtime to reconcile the requirements and the system's behavior. This process is deployed on three scenarios of requirements-execution reconciliation for the Meeting Scheduler system. The work builds on our previous work on goal-driven requirements engineering and on runtime requirements monitoring
IEEE;2015;From Means-End Analysis to Proactive Means-End Reasoning;;Self-adaptation is a prominent property for developing complex distributed software systems. Notable approaches to deal with self-adaptation are the runtime goal model artifacts. Goals are generally invariant along the system lifecycle but contain points of variability for allowing the system to decide among many alternative behaviors. This work investigates how it is possible to provide goal models at run-time that do not contain tasks, i.e. The description of how to address goals, thus breaking the design-time tie up between Tasks and Goals, generally outcome of a means-end analysis. In this vision the system is up to decide how to combine its available Capabilities: the Proactive Means-End Reasoning. The impact of this research line is to implement a goal-oriented form of self-adaptation where goal models can be injected at runtime. The paper also introduces MUSA, a Middleware for User-driven Service self-Adaptation.
IEEE;2011;Requirements models at run-time to support consistent system evolutions;Adaptive systems,,consistent evolution,,requirements engineering;Self-adaptive systems call for run-time management because of the environment uncertainty. In addition users may put forward new needs while the system is in execution possibly in response to environment variations. This means that a self-adaptive system needs to evolve at runtime accordingly to the user and context variations. We propose a context-aware framework that is inspired by the feature engineering perspective, and brings requirements specifications at run time by emphasizing the requirements that are context-dependent. We support system evolution by proposing a notion of correctness which is based on our context requirements. Our framework is generic and it is amenable to augment the system with new requirements arising at run-time. Since new requirements may interact with deployed requirements we provide the support to keep those entities at run-time and check their correctness jointly. Furthermore we identify the characteristics that a requirement language should have, to manage and check requirements at run-time in our framework.
IEEE;2011;Enforcing safety requirements for industrial automation systems at runtime position paper;industrial automation systems,,requirements at runtime,,requirements elicitation,,safety requirements;Current industrial automation systems are becoming more and more complex, and typically involve different phases of engineering, such as design time and runtime. System requirements, which are usually elicited during design time by engineers, currently are not sufficiently represented at runtime, like the runtime enforcement of safety requirements for industrial automation systems. Such kind of enforcement usually is very hard to model and predict at design time. Hence, the need exists to capture and manage safety requirements at design time and runtime, since safety requirements of industrial automation systems may lead to high risks if not addressed properly. In this position paper, we introduce a safety requirements enforcement framework and the using of Boilerplates for requirements elicitation and by explicitly modeling the runtime requirements knowledge for further application. We illustrate and evaluate the approach with data from a real-world case study in the area of industrial process systems. Major result was that the Boilerplates and explicit engineering knowledge are well suited to capture and enforce runtime safety requirements of industrial automation systems.
IEEE;2010;Requirements reflection: requirements as runtime entities;reflection,,requirements,,runtime,,self-adaptive systems;Computational reflection is a well-established technique that gives a program the ability to dynamically observe and possibly modify its behaviour. To date, however, reflection is mainly applied either to the software architecture or its implementation. We know of no approach that fully supports requirements reflection- that is, making requirements available as runtime objects. Although there is a body of literature on requirements monitoring, such work typically generates runtime artefacts from requirements and so the requirements themselves are not directly accessible at runtime. In this paper, we define requirements reflection and a set of research challenges. Requirements reflection is important because software systems of the future will be self-managing and will need to adapt continuously to changing environmental conditions. We argue requirements reflection can support such self-adaptive systems by making requirements first-class runtime entities, thus endowing software systems with the ability to reason about, understand, explain and modify requirements at runtime.
IEEE;2010;Requirements Engineering for Adaptive Service Based Applications;Requirements Engineering,,Self-Adaptive Systems,,Service Based Applications;Service-Based Applications (SBA) are inherently open and distributed, as they rely on third-party services that are available over the Internet, and have to cope with the dynamism of such operating environment. This motivates the need for SBA to be self-adaptive to accommodate changes in service availability and performance, in consumers' needs and preferences, and more generally in the operational environment, which may occur at run-time. Engineering such applications significantly challenges the role of requirements engineering (RE). Usually, RE is carried out at the outset of the whole development process, but in the context of SBA, RE activities are also needed at run-time thus enabling a seamless SBA evolution. In this paper, we investigate RE for SBA at run-time proposing a method that supports the continuous refinement of requirements artifacts at run-time, which involves consumers and the SBA itself as primary stakeholders.
IEEE;2010;Using requirements traceability links at runtime - a position paper;development time,,requirements,,runtime,,traceability;During software development a large amount of varied information is created. It comprises the requirements specification and depending artifacts such as design, code or test cases, as well as supporting information such as traceability links. This information is intended to be used during development time. The research in requirements at runtime has so far focused on using the requirements specification at runtime. This paper explores how to use the existing traceability links between requirements and other artifacts at runtime.
IEEE;2011;Dealing with softgoals at runtime: A fuzzy logic approach;decisions at runtime,,fuzzy logic,,non-functional requirements analysis,,propagation rules,,reasoning engine;One of the first frameworks to deal with NonFunctional Requirements, or softgoals, is the NFR Framework. This framework allows - among other contributions - softgoals analysis by applying propagation rules. This analysis is commonly performed during design activities. Instead of working with softgoals at design time, the proposal described in this paper combines propagation rules, fuzzy logic and Multi-Agent Systems in order to provide support for dealing with softgoals at runtime. Observing, for example, how the Requirements Engineering community deals with softgoals analysis by using propagation rules, we developed a propagation simulator centered on a specific algorithm. This simulator tries to replicate the requirements engineers' practices when using propagation rules to make decisions at design time. Based on this propagation simulator, we propose an intentional-MAS-driven reasoning engine capable of analyzing softgoals at runtime by selecting an adequate strategy (i.e. an adequate plan) that will be performed by the intentional agent to achieve the desired goal.
IEEE;2011;Foreword: 2nd Workshop requirements@run.time;Requirements,,reflection,,run-time,,self-adaptation;The 2nd edition of the Workshop requirements@run.time was held at the 19th International Conference on Requirements Engineering (RE 2011) in the city of Trento, Italy on the 30th of August 2011. It was organized by Nelly Bencomo, Emmanuel Letier, Jon Whittle, Anthony Finkelstein, and Kris Welsh. This foreword presents a digest of the discussions and presentations that took place during the workshop.
IEEE;2013;Requirements models for design- and runtime: A position paper;;In this position paper we review the history of requirements models and conclude that a goal-oriented perspective offers a suitable abstraction for requirements analysis. We stake positions on the nature of modelling languages in general, and requirements modelling languages in particular. We then sketch some of the desirable features (... “requirements”) of design-time and runtime requirements models and draw conclusions about their similarities and differences.
IEEE;2011;Requirements Analysis for Run-Time Service Compositions;;This chapter contains sections titled: Introduction, Related Work, Policies for Run-Time Service Composition, Requirements Analysis of Run-Time Service Compositions, Example, Conclusion and Future Work
IEEE;2008;Secure Workflow Development from Early Requirements Analysis;;Requirements engineering is being increasingly adopted as a key step in the software development process and so new challenges and possibilities emerge. Designing of Web services and developing of business processes and workflows for Web services is one of the most thought challenging issues in requirements engineering. The research on Web services design is well under way, but the existing design methodologies for Web services do not address the issue of developing secure Web services, secure business processes and secure workflows. For the purpose of developing secure workflows based on the early requirements analysis, in this work, we propose a refinement methodology and a language that allows the workflow engine to automatically enforce trust and delegation requirements. Those workflows are then to be distributed,, the security aspects being enforced dynamically at runtime accordingly to the identified requirements. To make the discussion more concrete, we illustrate the proposal with an e-business banking case study.
IEEE;2010;Towards a continuous requirements engineering framework for self-adaptive systems;;Requirements engineering (RE) for self-adaptive systems (SAS) is an emerging research area. The key features of such systems are to be aware of the changes in both their operating and external environments, while simultaneously remaining aware of their users' goals and preferences. This is accomplished by evaluating such changes and adapting to a suitable alternative that can satisfy those changes in the context of the user goals. Most current RE languages do not consider this `reflective' and online component of requirements models. In this paper, we propose a new framework for building SAS that is goal- and user-oriented. We sketch a framework to enable continuous adaptive requirements engineering (CARE) for SAS that leverage requirements-aware systems and exploits the Techne modeling language and reasoning system. We illustrate our framework by showing how it can handle an adaptive scenario in the travel domain.
IEEE;2012;Run-time model evaluation for requirements model-driven self-adaptation;model-driven,,run-time requirements,,self-adaptation,,self-adaptive;A self-adaptive system adjusts its configuration to tolerate changes in its operating environment. To date, requirements modeling methodologies for self-adaptive systems have necessitated analysis of all potential system configurations, and the circumstances under which each is to be adopted. We argue that, by explicitly capturing and modelling uncertainty in the operating environment, and by verifying and analysing this model at runtime, it is possible for a system to adapt to tolerate some conditions that were not fully considered at design time. We showcase in this paper our tools and research results.
IEEE;2012;Mitigating the obsolescence of quality specifications models in service-based systems;Quality of Service,,Requirements-awareness,,dynamically adaptive systems,,model@runtime,,requirements model,,service-based systems;Requirements-aware systems address the need to reason about uncertainty at runtime to support adaptation decisions, by representing quality of services (QoS) requirements for service-based systems (SBS) with precise values in run-time queryable model specification. However, current approaches do not support updating of the specification to reflect changes in the service market, like newly available services or improved QoS of existing ones. Thus, even if the specification models reflect design-time acceptable requirements they may become obsolete and miss opportunities for system improvement by self-adaptation. This articles proposes to distinguish “abstract” and “concrete” specification models: the former consists of linguistic variables (e.g. “fast”) agreed upon at design time, and the latter consists of precise numeric values (e.g. “2ms”) that are dynamically calculated at run-time, thus incorporating up-to-date QoS information. If and when freshly calculated concrete specifications are not satisfied anymore by the current service configuration, an adaptation is triggered. The approach was validated using four simulated SBS that use services from a previously published, real-world dataset,, in all cases, the system was able to detect unsatisfied requirements at run-time and trigger suitable adaptations. Ongoing work focuses on policies to determine recalculation of specifications. This approach will allow engineers to build SBS that can be protected against market-caused obsolescence of their requirements specifications.
IEEE;2010;Requirements-Aware Systems: A Research Agenda for RE for Self-adaptive Systems;Requirements,,reflection,,run-time,,self-adaptive systems;Requirements are sensitive to the context in which the system-to-be must operate. Where such context is well understood and is static or evolves slowly, existing RE techniques can be made to work well. Increasingly, however, development projects are being challenged to build systems to operate in contexts that are volatile over short periods in ways that are imperfectly understood. Such systems need to be able to adapt to new environmental contexts dynamically, but the contextual uncertainty that demands this self-adaptive ability makes it hard to formulate, validate and manage their requirements. Different contexts may demand different requirements trade-offs. Unanticipated contexts may even lead to entirely new requirements. To help counter this uncertainty, we argue that requirements for self-adaptive systems should be run-time entities that can be reasoned over in order to understand the extent to which they are being satisfied and to support adaptation decisions that can take advantage of the systems' self-adaptive machinery. We take our inspiration from the fact that explicit, abstract representations of software architectures used to be considered design-time-only entities but computational reflection showed that architectural concerns could be represented at run-time too, helping systems to dynamically reconfigure themselves according to changing context. We propose to use analogous mechanisms to achieve requirements reflection. In this paper we discuss the ideas that support requirements reflection as a means to articulate some of the outstanding research challenges.
IEEE;2015;Towards systems for increased access to justice using goal modeling;Analysis,,Goal-oriented Requirement Language,,cyberjustice,,online dispute resolution,,requirements engineering;Emerging cyberjustice systems are in need of relevant requirements engineering approaches, for example, to provide citizens with better access to the judicial system. In this context, this paper proposes the use of goal modeling for developing Online Dispute Resolution (ODR) systems in Canada. With ODR, the use of technology has the potential of increasing access to justice at low cost. We argue that a goal-oriented view is needed to capture early requirements about who are the stakeholders, what goals and quality criteria they have and how the various enabling technologies can be combined to meet these goals. A particular case is made for the use of the Goal-oriented Requirement Language (GRL), which covers the above and enables trade-off analysis as well as the introduction of indicators for measurement activities. GRL also has the potential of being used to guide some run-time decisions in ODR systems.
IEEE;2015;Adaptive Knowledge Bases in Self-Adaptive System Design;adaptation rules,,feedback loop,,knowledge base,,modelling,,run-time models,,self-adaptive systems;Self-adaptive systems allow for flexible solutions in changing environments. Usually, a fixed set of predefined rules is used to define the adaptation possibilities of a system. The main problem of such systems is to cope with environment behaviours that were not anticipated at design-time. In this case, no adaptation rule might be applicable or adaptations might not have the expected effect. In this paper, we propose an extended architecture of IBM's MAPE-K loop to cope with this problem. We impose a structure on the knowledge base consisting of an abstract system and environment model, a global goal model, and a set of (current) adaptation rules. Furthermore, we introduce an evaluation component that deletes failed adaptation rules, as well as a learning component that uses run-time models to autonomously generate new rules if the current ones are not applicable. With our approach, not only functional components can dynamically be adapted but also the adaptation logic itself.
IEEE;2003;Low complexity crosstalk cancellation through line selection in upstream VDSL;;Crosstalk is the major source of performance degradation in VDSL. A number of crosstalk cancellation techniques have been proposed to address this. Whilst these schemes lead to large performance increases they also have high run-time complexities, a problem which grows rapidly with the number of lines within a binder. Since the majority of crosstalk typically comes from only a few dominant crosstalkers, it is possible to do partial crosstalk cancellation. We present a low-complexity, partial crosstalk cancellation technique for VDSL based on line selection. We derive the optimal line selection technique, and several low-complexity selection algorithms which give near-optimal performance in most scenarios. These techniques lead to significant reductions in runtime complexity whilst giving similar performance to full crosstalk cancellation.
IEEE;2012;A requirements-based approach for the design of adaptive systems;;Complexity is now one of the major challenges for the IT industry [1]. Systems might become too complex to be managed by humans and, thus, will have to be self-managed: Self-configure themselves for operation, self-protect from attacks, self-heal from errors and self-tune for optimal performance [2]. (Self-)Adaptive systems evaluate their own behavior and change it when the evaluation indicates that it is not accomplishing the software's purpose or when better functionality and performance are possible [3]. To that end, we need to monitor the behavior of the running system and compare it to an explicit formulation of requirements and domain assumptions [4]. Feedback loops (e.g., the MAPE loop [2]) constitute an architectural solution for this and, as proposed by past research [5], should be a first class citizen in the design of such systems. We advocate that adaptive systems should be designed this way from as early as Requirements Engineering and that reasoning over requirements is fundamental for run-time adaptation. We therefore propose an approach for the design of adaptive systems based on requirements and inspired in control theory [6]. Our proposal is goal-oriented and targets softwareintensive socio-technical systems [7], in an attempt to integrate control-loop approaches with decentralized agents inspired approaches [8]. Our final objective is a set of extensions to state-of-the-art goal-oriented modeling languages that allow practitioners to clearly specify the requirements of adaptive systems and a run-time framework that helps developers implement such requirements. In this 2-page abstract paper, we summarize this approach.
IEEE;2011;From awareness requirements to adaptive systems: A control-theoretic approach;adaptive systems,,awareness,,control theory,,feedback loop,,requirements;Several proposals for the design of adaptive systems rely on some kind of feedback loop that monitors the system output and adapts in case of failure. Roadmap papers in the area advocate the need to make such feedback loops first class entities in adaptive systems design. We go further by adopting a Requirements Engineering perspective that is not only based on feedback loops but also applies other concepts from Control Theory to the design of adaptive systems. Our plans include a framework that reasons over requirements at runtime to provide adaptivity to a system proper. In this position paper, we argue for a control-theoretic view for adaptive systems and outline our long-term research agenda, briefly presenting work that we have already accomplished and discussing our plans for the future.
IEEE;2013;From GMoDS models to object-oriented specifications in Event-B;;This paper represents a first attempt to express in Event-B the models of the GMoDS (The Goal Model for Dynamic Systems) methodology. GMoDS is a major result of the research related to Organization-Based Multiagent System Engineering methodology (O-MaSE), allowing to specify goals during requirements engineering process and then to use them throughout the system development and at runtime. We choose Event-B as a modelling language for specifying the GMoDS models for two reasons: (a) Event-B has the concept of proving correctness, which supports the accuracy of software development, and (b) its supporting tool, RODIN, is open-source, and it is used in many software industrial applications. Because Event-B is not object-oriented, and because several concepts used in GMoDS are related to object-orientation, we have included in the corresponding Event-B specifications the object-oriented concepts used in the GMoDS models, without changing the syntax and semantics of Event-B, and without using the UML-B tool.
IEEE;2011;Requirements-driven adaptation: Compliance, context, uncertainty, and systems;Commitments,,Crisp requirements,,Critical requirements,,Fuzzy requirements,,Interaction,,Multiagent Systems,,Requirements engines,,Verifiability;The systematic study of software self-adaptation has emerged as one of the key areas of software engineering. The challenges and the ontology relevant to this area are still being formulated. I take this opportunity to present some of my observations on compliance, context, and uncertainty as they pertain to adaptation. I also argue that requirements engineering, and to a large extent software engineering, takes a centralized perspective of systems, and therefore cannot model let alone enable reasoning about adaptation in multiagent systems.
IEEE;2009;Engineering adaptive requirements;;Challenges in the engineering of self-adaptive software have been recently discussed and summarised in a seminal research road map. Following it, we focus on requirements engineering issues, with a two-fold, long term objective. The first objective is to support the system analyst to engineer adaptive requirements at requirements-time, the second is to make software able to reason on requirements at run-time in order to enable a goal-oriented adaptation. Along the first objective, in this position paper we propose a characterisation of adaptive requirements. Moreover, we investigate how available techniques aimed at eliciting and specifying domain properties, stakeholders' goals and preferences, can provide a practical support to the analyst while capturing adaptive requirements.
IEEE;2007;Visualizing the Analysis of Dynamically Adaptive Systems Using i* and DSLs;;Self-adaptation is emerging as a crucial enabling capability for many applications, particularly those deployed in dynamically changing environments. One key challenge posed by dynamically adaptive systems (DASs) is the need to handle changes to the requirements and corresponding behavior of a DAS in response to varying environmental conditions. In this paper we propose a visual model-driven approach that uses the i* modeling language to represent goal models for the DAS requirements. Our approach applies a rigorous separation of concerns between the requirements for the DAS to operate in stable conditions and those that enable it to adapt at run-time to enable it to cope with changes in its environment. We further show how requirements derived from the i* modeling can be used by a domain-specific language to achieve requirements model-driven development. We describe our experiences with applying this approach to GridStix, an adaptive flood warning system, deployed on the River Ribble in North Yorkshire, England.
IEEE;2015;An Empirical Analysis of Providing Assurance for Self-Adaptive Systems at Different Levels of Abstraction in the Face of Uncertainty;assurance,,search-based software engineering,,self-adaptive systems;Self-adaptive systems (SAS) must frequently continue to deliver acceptable behavior at run time even in the face of uncertainty. Particularly, SAS applications can self-reconfigure in response to changing or unexpected environmental conditions and must therefore ensure that the system performs as expected. Assurance can be addressed at both design time and run time, where environmental uncertainty poses research challenges for both settings. This paper presents empirical results from a case study in which search-based software engineering techniques have been systematically applied at different levels of abstraction, including requirements analysis, code implementation, and run-time validation, to a remote data mirroring application that must efficiently diffuse data while experiencing adverse operating conditions. Experimental results suggest that our techniques perform better in terms of providing assurance than alternative software engineering techniques at each level of abstraction.
IEEE;2011;[Title page];;The following topics are dealt with: The 2nd edition of the Workshop requirements@run.time was held at the 19th International Conference on Requirements Engineering (RE 2011) in the city of Trento, Italy on the 30th of August 2011. It was organized by Nelly Bencomo, Emmanuel Letier, Jon Whittle, Anthony Finkelstein, and Kris Welsh. This foreword presents a digest of the discussions and presentations that took place during the workshop.
IEEE;2011;Compliance management with measurement frameworks;;New regulatory regimes advocate the use of “goaloriented” regulations that are more flexible during regulatory conversations occurring between the regulators and the regulatees when new regulations are introduced. In that context, long-term “compliance agreements” between regulators and regulatees are needed. Using recent developments of the Measurement Theory, this paper shows that the concept of Measurement Framework (MF) for soft-systems is of particular importance for providing those compliance agreements. We show that with two kinds of goals and softgoals based on MF, one can improve (a) the elicitation of compliance requirements, (b) the structure of the compliance arguments for compliant requirements, and (c) the consistency between actual compliance at run-time and the intentional compliance at early stages of Requirements Engineering.
IEEE;2015;Adventures in Adaptation: A Software Engineering Playground!;;Summary form only given. A long research career has inevitably meant that the focus of our work may appear almost random, meandering through distributed computing, software architectures, requirements engineering and model checking! However, in retrospect, a rational reconstruction suggests that there might have been a thread which binds these research adventures together: that of dealing with change. The need to handle change, particularly at run-time, provides a wonderful set of challenges, making research into adaptive and self-managing systems a playground for software engineering researchers. We need to provide a set of comprehensive, consistent and pragmatic approaches to deal with challenges in aspects such as requirements goals and goal revision, domain modelling and model revision, planning and plan revision, and software configuration and reconfiguration. Based on our experiences, this talk will provide some insight into our approaches and suggest some recommendations for those that enjoy adventure playgrounds.
IEEE;2010;First step towards a domain specific language for self-adaptive systems;Domain Specific Language (DSL),,Dynamically Adaptive Systems (DASs),,Eclipse ModeIing Framework (EMF);Self-adaptive systems are capable of autonomously modifying their behavior at run-time in response to changing environmental conditions. In order to modify the behavior, requirements play an important role, as they tend to change for these systems. For this we need to identify those requirements that are concerned with the adaptability features of the self-adaptive systems. In order to cope with the uncertainty inherent in self-adaptive systems, requirements engineering languages for these systems should include explicit constructs. RELAX is a requirement engineering language tor self-adaptive systems that incorporates uncertainty into the specification of these systems. To go one step further, we aim at developing a domain specific language that would bridge the gap between requirements and the overall system model. The first step that is illustrated in this paper is to build a textual editor for RELAX.
IEEE;2013;Runtime goal models: Keynote;Goal reasoning,,Requirements at runtime,,Runtime goal models,,Self-adaptive systems;Goal models capture stakeholder requirements for a system-to-be, but also circumscribe a space of alternative specifications for fulfilling these requirements. Recent proposals for self-adaptive software systems rely on variants of goal models to support monitoring and adaptation functions. In such cases, goal models serve as mechanisms in terms of which systems reflect upon their requirements during their operation. We argue that existing proposals for using goal models at runtime are using design artifacts for purposes they were not intended, i.e., for reasoning about runtime system behavior. In this paper, we propose a conceptual distinction between Design-time Goal Models (DGMs)-used to design a system-and Runtime Goal Models (RGMs)-used to analyze a system's runtime behavior with respect to its requirements. RGMs extend DGMs with additional state, behavioral and historical information about the fulfillment of goals. We propose a syntactic structure for RGMs, a method for deriving them from DGMs, and runtime algorithms that support their monitoring.
IEEE;2011;Run-time resolution of uncertainty;goals,,requirements models,,self adaptive systems;Requirements awareness should help optimize requirements satisfaction when factors that were uncertain at design time are resolved at runtime. We use the notion of claims to model assumptions that cannot be verified with confidence at design time. By monitoring claims at runtime, their veracity can be tested. If falsified, the effect of claim negation can be propagated to the system's goal model and an alternative means of goal realization selected automatically, allowing the dynamic adaptation of the system to the prevailing environmental context.
IEEE;2015;SACRE: A tool for dealing with uncertainty in contextual requirements at runtime;;Self-adaptive systems are capable of dealing with uncertainty at runtime handling complex issues as resource variability, changing user needs, and system intrusions or faults. If the requirements depend on context, runtime uncertainty will affect the execution of these contextual requirements. This work presents SACRE, a proof-of-concept implementation of an existing approach, ACon, developed by researchers of the Univ. of Victoria (Canada) in collaboration with the UPC (Spain). ACon uses a feedback loop to detect contextual requirements affected by uncertainty and data mining techniques to determine the best operationalization of contexts on top of sensed data. The implementation is placed in the domain of smart vehicles and the contextual requirements provide functionality for drowsy drivers.
IEEE;2008;Dynamic Maintenance of Software Systems at Runtime;Availability,,Dynamic Software maintenance,,Software requirements,,Type-Safety;Software systems suffer one basic problem: They are frequently adapted and updated due to changes in requirements and new bugs surfacing. The result of these adaptations is lower availability and missing state of the program. In addition, applying these adaptations requires professional developers which inflict high costs on the organization. In this article, we introduce several methods for adapting programs at run-time without disrupting program execution. This increases system availability. Also, in order to reduce the need for professionals for program modification, some methods that simplify the implementation of new or changed requirements will be offered. As an example, an application written in Java 1.6 has been produced in which both simplification of requirements implementation and hot-swapping (adapting programs at run-time) have been integrated. In this application, we do hot-swapping by using "Java Platform Debugger Architecture API" and attain simplification of presenting new requirements by using "XML-based languages ".
IEEE;2013;Test Strategies for Reliable Runtime Reconfigurable Architectures;FPGA,,online test,,reconfigurable architectures;Field-programmable gate array (FPGA)-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. The reliability of FPGAs, being manufactured in latest technologies, is threatened by soft errors, as well as aging effects and latent defects. To ensure reliable reconfiguration, it is mandatory to guarantee the correct operation of the reconfigurable fabric. This can be achieved by periodic or on-demand online testing. This paper presents a reliable system architecture for runtime-reconfigurable systems, which integrates two nonconcurrent online test strategies: preconfiguration online tests (PRET) and postconfiguration online tests (PORT). The PRET checks that the reconfigurable hardware is free of faults by periodic or on-demand tests. The PORT has two objectives: It tests reconfigured hardware units after reconfiguration to check that the configuration process completed correctly and it validates the expected functionality. During operation, PORT is used to periodically check the reconfigured hardware units for malfunctions in the programmable logic. Altogether, this paper presents PRET, PORT, and the system integration of such test schemes into a runtime-reconfigurable system, including the resource management and test scheduling. Experimental results show that the integration of online testing in reconfigurable systems incurs only minimum impact on performance while delivering high fault coverage and low test latency.
IEEE;2014;Semiautomatic security requirements engineering and evolution using decision documentation, heuristics, and user monitoring;Security requirements engineering,,decision documentation,,decision knowledge,,heuristic analysis,,knowledge carrying software,,software evolution,,user mon-itoring;Security issues can have a significant negative impact on the business or reputation of an organization. In most cases they are not identified in requirements and are not continuously monitored during software evolution. Therefore, the inability of a system to conform to regulations or its endangerment by new vulnerabilities is not recognized. In consequence, decisions related to security might not be taken at all or become obsolete quickly. But to evaluate efficiently whether an issue is already addressed appropriately, software engineers need explicit decision documentation. Often, such documentation is not performed due to high overhead. To cope with this problem, we propose to document decisions made to address security requirements. To lower the manual effort, information from heuristic analysis and end user monitoring is incorporated. The heuristic assessment method is used to identify security issues in given requirements automatically. This helps to uncover security decisions needed to mitigate those issues. We describe how the corresponding security knowledge for each issue can be incorporated into the decision documentation semiautomatically. In addition, violations of security requirements at runtime are monitored. We show how decisions related to those security requirements can be identified through the documentation and updated manually. Overall, our approach improves the quality and completeness of security decision documentation to support the engineering and evolution of security requirements.
IEEE;2012;Requirements-driven adaptive security: Protecting variable assets at runtime;Adaptation,,Causal reasoning,,Security requirements;Security is primarily concerned with protecting assets from harm. Identifying and evaluating assets are therefore key activities in any security engineering process - from modeling threats and attacks, discovering existing vulnerabilities, to selecting appropriate countermeasures. However, despite their crucial role, assets are often neglected during the development of secure software systems. Indeed, many systems are designed with fixed security boundaries and assumptions, without the possibility to adapt when assets change unexpectedly, new threats arise, or undiscovered vulnerabilities are revealed. To handle such changes, systems must be capable of dynamically enabling different security countermeasures. This paper promotes assets as first-class entities in engineering secure software systems. An asset model is related to requirements, expressed through a goal model, and the objectives of an attacker, expressed through a threat model. These models are then used as input to build a causal network to analyze system security in different situations, and to enable, when necessary, a set of countermeasures to mitigate security threats. The causal network is conceived as a runtime entity that tracks relevant changes that may arise at runtime, and enables a new set of countermeasures. We illustrate and evaluate our proposed approach by applying it to a substantive example concerned with security of mobile phones.
IEEE;2011;Adaptive Service Composition Based on Runtime Requirements Monitoring;Requirement Monitoring,,goal refinement,,service composition;In today's service computing environments, user needs and expectations are constantly changing. New services emerge while old ones become obsolete and need to be replaced. In such settings, composite services need to be adaptive to changes in user requirements and the environment. This paper proposes a conceptual framework for modeling compositional adaptation for services founded on a requirements monitoring facility. This facility helps maintain adherence between user requirements changes and the dynamics of service composition structure and quality attributes. Specifically, user requirements are represented as goals and soft goals, service composition structure is represented with a CSP-like grammar, and the adaptation mechanism is based on AI planning. The proposed approach is evaluated in a service simulation environment of real-world supply-chain adaptation scenarios.
IEEE;2015;Rail Internet of Things: An Architectural Platform and Assured Requirements Model;Assistance,,Assured model,,Inclusive,,IoT,,Rail Internet of Things (RIoT),,Situation Calculus;Given the plethora of individual preferences and requirements of public transport passengers for travel, seating, catering, etc., it becomes very challenging to tailor generic services to individuals' requirements using the existing service platforms. As tens of thousands of sensors have been already deployed along roadsides and rail tracks and on buses and trains in many countries, it is expected that the introduction of IP networking will revolutionise the functionality of public transport in general and rail services in particular. In this paper, we propose a new communication paradigm to improve rail services and address the requirement of rail service users: the Rail Internet of Things (RIoT). To the best of our knowledge, it is the first work to define the RIoT and design an architectural platform that includes its components and the data communication channels. Moreover, we develop an assured requirements model using the situation calculus modelling to represent the fundamental requirements for adjustable, decentralised feedback control mechanisms necessary for the RIoT-ready software systems. The developed formal model is applied to demonstrate the design of passenger assistance software that interacts with the RIoT ecosystem and provides passengers with real-time information that is tailored to their requirements with runtime adaptability.
IEEE;2003;Programming at runtime: Requirements & paradigms for nonprogrammer web application development;;We investigate the femibiliy of nonprogramnier web application development and propose the creation of end-user programming tools that address the issue at a high level of abstraction. The results of three related empirical studies and one protoping effort are reported. We surveyed nonprogrammersÂ¿ needs for web applications and studied how nonprogrammers would naturally approach web development. To express what a tool should provide we summarize high-level components and concepts employed by web applications. To express how a tool may provide its functionality, we propose "Programming-at-Runtime" - a programming paradigm that is in its core similar to the automatic recalculation in spreadsheets. Finally, we introduce "FlashLight" - a protoype web development tool for nonprogrammers.
IEEE;2015;QuantUn: Quantification of uncertainty for the reassessment of requirements;Bayesian Surprise,,requirements assessment,,requirements reflection,,self-adaptation,,uncertainty;Self-adaptive systems (SASs) should be able to adapt to new environmental contexts dynamically. The uncertainty that demands this runtime self-adaptive capability makes it hard to formulate, validate and manage their requirements. QuantUn is part of our longer-term vision of requirements reflection, that is, the ability of a system to dynamically observe and reason about its own requirements. QuantUn's contribution to the achievement of this vision is the development of novel techniques to explicitly quantify uncertainty to support dynamic re-assessment of requirements and therefore improve decision-making for self-adaption. This short paper discusses the research gap we want to fill, present partial results and also the plan we propose to fill the gap.
IEEE;2010;Towards Self-Managed Systems Aware of Economic Value;autonomic computing,,requirements engineering,,self-managed systems,,value-based software engineering;Current self-managed systems do not take into account the economic value they exchange with external systems and services. While value-based software and requirements engineering approaches exist, they do not specifically deal with self-managed systems and their operations. These operations are typically associated with changing requirements, dynamic adaptations, and runtime reconfigurations, as well as changes in the environment. We argue that self-managed systems should be “aware” of their requirements and the economic value created (or loosed) by their fulfillment (or not fulfillment) as well as the economic value exchanged with other systems. They should be able to reason and take actions based on this “awareness”, especially when their environmental conditions are changing (both technical and economic ones). More generally, self-managed systems should be able to reason about the systems' business goals and relate them with the Quality of Service they offer. This paper discusses these issues and proposes a generic architecture for addressing them.
IEEE;2013;Harnessing evolutionary computation to enable dynamically adaptive systems to manage uncertainty;Dynamically adaptive systems,,design,,model-based development,,requirements engineering,,run-time,,uncertainty;This keynote talk and paper intend to motivate research projects that investigate novel ways to model, analyze, and mitigate uncertainty arising in three different aspects of the cyber-physical systems. First, uncertainty about the physical environment can lead to suboptimal, and sometimes catastrophic, results as the system tries to adapt to unanticipated or poorly-understood environmental conditions. Second, uncertainty in the cyber environment can have lead to unexpected and adverse effects, including not only performance impacts (load, traffic, etc.) but also potential threats or overt attacks. Finally, uncertainty can exist with the components themselves and how they interact upon reconfiguration, including unexpected and unwanted feature interactions. Each of these sources of uncertainty can potentially be identified at different stages, respectively run time, design time, and requirements, but their mitigation might be done at the same or a different stage. Based on the related literature and our preliminary investigations, we argue that the following three overarching techniques are essential and warrant further research to provide enabling technologies to address uncertainty at all three stages: model-based development, assurance, and dynamic adaptation. Furthermore, we posit that in order to go beyond incremental improvements to current software engineering techniques, we need to leverage, extend, and integrate techniques from other disciplines.
IEEE;2009;Situ: A Situation-Theoretic Approach to Context-Aware Service Evolution;Context aware,,hidden Markov chain,,human intention,,requirements,,runtime,,service,,situation theoretic,,smart home,,software evolution.;Evolvability is essential for computer systems to adapt to the dynamic and changing requirements in response to instant or delayed feedback from a service environment that nowadays is becoming more and more context aware,, however, current context-aware service-centric models largely lack the capability to continuously explore human intentions that often drive system evolution. To support service requirements analysis of real-world applications for services computing, this paper presents a situation-theoretic approach to human-intention-driven service evolution in context-aware service environments. In this study, we give situation a definition that is rich in semantics and useful for modeling and reasoning human intentions, whereas the definition of intention is based on the observations of situations. A novel computational framework is described that allows us to model and infer human intentions by detecting the desires of an individual as well as capturing the corresponding context values through observations. An inference process based on hidden Markov model makes instant definition of individualized services at runtime possible, and significantly, shortens service evolution cycle. We illustrate the possible applications of this framework through a smart home example aimed at supporting independent living of elderly people.
IEEE;2012;Service-Driven Migrating of Enterprise Information Systems: A Case Study;information system,,requirements engineering,,service-oriented;There are many business information systems under operation supporting the essential business processes of large scale multi-national enterprises, which forms the essential IT assets of these organizations. However, the isolation of these systems, improper process configurations and unbalanced resource allocations often leads to the deterioration of these enterprises, or even major management crisis. Service-oriented computing is considered a possible remedy for such issues with relatively low migration cost that has attracted much industrial attention. In this paper, through a survey to an industrial consulting case from a Southeastern Asia Garment Manufacture, we discuss how service requirements are elicited, how to evolve conventional management information systems into composite service systems that is reconfigurable at runtime, so as to realize resources optimization and efficiency improvement. The purpose of this paper is to analyze the know how of migrating conventional management information systems into service-oriented architecture, from a requirements engineering perspective.
IEEE;2015;Applying Game Theoretic Approach to Goal-Driven Requirements Trade-Off Analysis for Self-Adaptation;game theory,,requirements engineering,,self-adaptive software;A self-adaptive software aims at adjusting itself in response to changes at runtime while considering several important factors. To do this task, such a system should realize the adaptation process, which consists of four phases. Among those phases, we just focus on the deciding (planning) process, especially addressing adequate reflection of requirements to adaptation. In this paper, we represent the arising requirements trade-off problem in terms of goal oriented requirements engineering while adapting to the changes. In addition we apply a game theoretical approach which gives us an insight into analyzing conflicts among requirements. It could provide self-adaptive software with the rationale behind selecting adaptation behaviors.
IEEE;2014;The observer-based technique for requirements validation in embedded real-time systems;TASM,,model-based requirements validation,,observer technique,,run-time monitoring,,systems functional behaviors and non-functional properties;Model-based requirements validation is an increasingly attractive approach to discovering hidden flaws in requirements in the early phases of systems development life cycle. The application of using traditional methods such as model checking for the validation purpose is limited by the growing complexity of embedded real-time systems (ERTS). The observer-based technique is a lightweight validation technique, which has shown its potential as a means of validating the correctness of model behaviors. In this paper, the novelty of our contributions is three-fold: 1) we formally define the observer constructs for our formal specification language namely the Timed Abstract State Machine (TASM) language and, 2) we propose the Events Monitoring Logic (EvML) to facilitate the observer specification and, 3) we show how to execute observers to validate the requirements describing the functional behaviors and non-functional properties (such as timing) of ERTS. We also illustrate the applicability of the extended TASM language through an industrial application of a Vehicle Locking-Unlocking system.
IEEE;2015;An Application Conflict Detection and Resolution System for Smart Homes;Smart Home,,conflict resolution,,model checking,,requirements at runtime;One of the applications of Cyber-Physical Systems (CPSs) is the Smart Homes. In Smart Homes, multiple apps operate the sensors and actuators to provide rich user experience in a living environment. Because actuators are entities that affect the surrounding environment, conflicts may occur if two or more apps are running simultaneously, especially when they try to use a single actuator or when they use different actuators causing different effects. There have been attempts to resolve these conflicts at app installation time. However the state-of-the-art solutions can detect conflicts only if the apps actuate on devices with conditions based on time, and resolute conflicts by creating a total order between all apps, regardless of the situations of the conflicts. In this paper, we create a Kripke structure to detect conflicts by model-checking the assertion "no two apps use actuators to create different effects at the same location". Our proposed system, which provides install-time conflict detection, enables detection of application conflicts triggered by conditions based on events. In addition, it supports users in prioritising apps by reducing the number of conflicts by dividing them into groups of the same situation which are meaningful to the users. By prioritising apps for each situation, rather than creating a total order, our system allows the apps to run in a more flexible way.
IEEE;2013;Requirements-Driven Self-Repairing against Environmental Failures;;Self-repairing approaches have been proposed to alleviate the runtime requirements satisfaction problem by switching to appropriate alternative solutions according to the feedback monitored. However, little has been done formally on analyzing the relations between specific environmental failures and corresponding repairing decisions, making it a challenge to derive a set of alternative solutions to withstand possible environmental failures at runtime. To address these challenges, we propose a requirements-driven self-repairing approach against environmental failures, which combines both development-time and runtime techniques. At the development phase, in a stepwise manner, we formally analyze the issue of self-repairing against environmental failures with the support of the model checking technique, and then design a sufficient and necessary set of alternative solutions to withstand possible environmental failures. The runtime part is a runtime self-repairing mechanism that monitors the operating environment for unsatisfiable situations, and makes self-repairing decisions among alternative solutions in response to the detected environmental failures.
IEEE;2009;Reasoning on Non-Functional Requirements for Integrated Services;Discrete Time Markov Chains,,Integrated Services,,Non-Functional Requirements;We focus on non-functional requirements for applications offered by service integrators,, i.e., software that delivers service by composing services, independently developed, managed, and evolved by other service providers. In particular, we focus on requirements expressed in a probabilistic manner, such as reliability or performance. We illustrate a unified approach-a method and its support tools-which facilitates reasoning about requirements satisfaction as the system evolves dynamically. The approach relies on run-time monitoring and uses the data collected by the probes to detect if the behavior of the open environment in which the application is situated, such as usage profile or the external services currently bound to the application, deviates from the initially stated assumptions and whether this can lead to a failure of the application. This is achieved by keeping a model of the application alive at run time, automatically updating its parameters to reflect changes in the external world, and using the model's predictive capabilities to anticipate future failures, thus enabling suitable recovery plans.
IEEE;2002;eModel: addressing the need for a flexible modeling framework in autonomic computing;;The paper describes a novel, flexible framework, eModel, designed to address the runtime requirements of autonomic computing: on-line workload measurement, analysis, and prediction. The eModel architecture has been developed using platform independent technology (XML and Java) to allow for maximum portability while also allowing for ease-of-integration with existing measurement and system management tools. The eModel toolkit consists of a GUI based model builder tool, a data base deployment tool, a runtime tool, and an analysis tool. In addition to the toolkit, the eModel design provides a runtime architecture which can be deployed directly without using any interaction with the GUI. The architecture is flexible enough to allow for incorporation with models of various complexity, including modeling techniques that require a hierarchical approach to attain reasonable accuracy based upon on-line, measured data. We present examples that illustrate eModel as a capacity planning tool as well as an augmentation to autonomic system management in an effort to highlight the technological gaps that the eModel framework is capable of bridging.
IEEE;2001;Residual requirements and architectural residues;;Monitoring running systems is a useful technique available to requirements engineers, to ensure that systems meet their requirements and in some cases to ensure that they obey the assumptions under which they were created. This report studies relationships between the original requirements and the monitoring infrastructure. It postulates that the monitored requirements are in fact just compilations of original requirements, called residual requirements. Dynamic architectural models have become important tools for expressing requirements on modem distributed systems. Monitoring residual requirements will be seen to involve architectural residues, skeletal run-time images of the original logical architecture. An example sales support system is used to illustrate the issues involved, employing modest extensions to the Acme architecture description language to reason about architectural dynamism
IEEE;2011;Are your sites down? Requirements-driven self-tuning for the survivability of Web systems;earned business value,,goal-oriented reasoning,,self-tuning,,survivability assurance;Running in a highly uncertain and greatly complex environment, Web systems cannot always provide full set of services with optimal quality, especially when work loads are high or subsystem failures are frequent. Hence, it is significant to continuously maintain a high satisfaction level of survivability, hereafter survivability assurance, while relaxing or sacrificing certain quality or functional requirements that are not crucial to the survival of the entire system. After giving a value-based interpretation to survivability assurance to facilitate a quantitative analysis, we propose a requirements-driven self-tuning method for the survivability assurance of Web systems. Maintaining an enriched and live goal model, our method adapts to runtime tradeoff decisions made by our PID (proportional-integral-derivative) controller and goal-oriented reasoner for both quality and functional requirements. The goal-based configuration plans produced by the reasoner is carried out on the live goal model, and then mapped into system architectural configurations. Experiments on an online shopping system are conducted to validate the effectiveness of the proposed method.
IEEE;2007;Aspectizing a Web Server for Adaptation;;Web servers are exposed to extremely changing runtime requirements. Going offline to adjust policies and configuration parameters in order to cope with such requirements is not an available choice for long running Web servers. Many of the policies that need to be adapted are crosscutting in nature. Aspect-oriented programming (AOP) provides mechanisms to encapsulate the crosscutting policies as aspects. This paper describes the integration of a statically configurable Web server with our dynamic aspect weaving infrastructure. This integration transformed the server to a dynamically adaptable one that could adjust its policies and configuration parameters at runtime according to the changing requirements. This paper further provides a comprehensive analysis of the memory and runtime costs associated with this transformation, and explains how our dynamic aspect weaving infrastructure via its tailored support facilitates to minimise these costs.
IEEE;2009;RELAX: Incorporating Uncertainty into the Specification of Self-Adaptive Systems;requirements,,self-adaptation,,uncertainty;Self-adaptive systems have the capability to autonomously modify their behaviour at run-time in response to changes in their environment. Self-adaptation is particularly necessary for applications that must run continuously, even under adverse conditions and changing requirements,, sample domains include automotive systems, telecommunications, and environmental monitoring systems. While a few techniques have been developed to support the monitoring and analysis of requirements for adaptive systems, limited attention has been paid to the actual creation and specification of requirements of self-adaptive systems. As a result, self-adaptivity is often constructed in an ad-hoc manner. In this paper, we argue that a more rigorous treatment of requirements explicitly relating to self-adaptivity is needed and that, in particular, requirements languages for self-adaptive systems should include explicit constructs for specifying and dealing with the uncertainty inherent in self-adaptive systems. We present RELAX, a new requirements language for self-adaptive systems and illustrate it using examples from the smart home domain.
IEEE;2008;PEM fuel cells versus diesel generators — which solution to pick?;;Hurricane Katrina and the pursuant ruling by the FCC has caused many Telecom operators to review and find solutions to provide extended runtime in critical areas of their network. In the past, lead acid batteries have dominated the backup power market for these sites and the power grid quality was considered robust. Katrina forced operators to look past this and it revealed that lead acid batteries typically only provide up to eight hours of backup for a site. For those sites where operators require extended runtime, a diesel generator was typically the only option. This paper will provide an overview of not only the diesel solution but also for a new solution that utilizes a proton exchange membrane fuel cell (PEMFC) and premium lead acid batteries to provide extended runtime at the site using a 5 kW base station plant, we will examine the architecture and design solutions that will give operators well over 48 hours of backup and in some cases over 160 hours. The PEMFC solution can utilize advanced lead acid front terminal batteries to ease maintenance as well as a robust and cost effective PEMFC engine. Critical areas for the mainstream success of fuel cells include total solution costs, reliability, fuel supply, maintenance and hydrogen storage. The comparative diesel will be the embedded 30 kW solution that is normally found in sites throughout these applications. Issues associated with generator operation include noise, emissions, permitting, fuel supply and maintenance costs will all be examined. The goal will be to show a total cost of ownership for both solutions and educate operators about new approaches for their extended runtime requirements.
IEEE;2012;Privacy arguments: Analysing selective disclosure requirements for mobile applications;mobile applications,,privacy arguments,,privacy requirements;Privacy requirements for mobile applications offer a distinct set of challenges for requirements engineering. First, they are highly dynamic, changing over time and locations, and across the different roles of agents involved and the kinds of information that may be disclosed. Second, although some general privacy requirements can be elicited a priori, users often refine them at runtime as they interact with the system and its environment. Selectively disclosing information to appropriate agents is therefore a key privacy management challenge, requiring carefully formulated privacy requirements amenable to systematic reasoning. In this paper, we introduce privacy arguments as a means of analysing privacy requirements in general and selective disclosure requirements (that are both content- and context-sensitive) in particular. Privacy arguments allow individual users to express personal preferences, which are then used to reason about privacy for each user under different contexts. At runtime, these arguments provide a way to reason about requirements satisfaction and diagnosis. Our proposed approach is demonstrated and evaluated using the privacy requirements of BuddyTracker, a mobile application we developed as part of our overall research programme.
IEEE;2012;Towards Dynamic Evolution of Self-Adaptive Systems Based on Dynamic Updating of Control Loops;control loops,,design,,dynamic evolution,,goal-oriented requirements modeling,,programming framework,,self-adaptation;Self-adaptive systems, which enable runtime adaptation, are promising ways of dealing with environmental changes, including system intrusions or faults. Such software systems must modify themselves to better fit their environment. One of the main approaches to constructing such systems is to introduce multiple control loops. Software evolution is an essential activity for expanding this adaptation capability, and dynamic evolution has been envisaged as a way of systems adapting themselves at runtime. In this paper, we establish a development process to deal with dynamic evolution. We devise a goal model compiler to generate models for designing dynamic evolutions and a programming framework that supports dynamic deployment of control loops. We experimentally applied our approach to a system and discuss how our compiler and framework support dynamic evolution of self-adaptive systems.
IEEE;2010;Fuzzy Goals for Requirements-Driven Adaptation;Fuzzy Goals,,Goals,,KAOS,,Self-Adaptation;Self-adaptation is imposing as a key characteristic of many modern software systems to tackle their complexity and cope with the many environments in which they can operate. Self-adaptation is a requirement per-se, but it also impacts the other (conventional) requirements of the system,, all these new and old requirements must be elicited and represented in a coherent and homogenous way. This paper presents FLAGS, an innovative goal model that generalizes the KAOS model, adds adaptive goals to embed adaptation countermeasures, and fosters self-adaptation by considering requirements as live, runtime entities. FLAGS also distinguishes between crisp goals, whose satisfaction is boolean, and fuzzy goals, whose satisfaction is represented through fuzzy constraints. Adaptation countermeasures are triggered by violated goals and the goal model is modified accordingly to maintain a coherent view of the system and enforce adaptation directives on the running system. The main elements of the approach are demonstrated through an example application.
IEEE;2003;Acquiring and incorporating state-dependent timing requirements;;Some real-time systems are designed to deliver services to objects that are controlled by external sources. Their services must be delivered on a timely basis, and the system fails when some services are delivered too late. Such a system may fail if the timing requirements, which it is designed to meet are erroneous. It may under-utilize resources and, consequently, be costly or unreliable if the requirements are too stringent. In general, the timing requirements of the system may change when the states of the objects monitored by the system change. Hence, one must identify how changes in object states call for changes in system requirements and how these changes should be incorporated in the design and implementation of the system. We first describe a methodology to determine timing requirements and to take into account of requirement changes at runtime. The method is based on several timing requirement determination schemes. Simulation data show that these schemes are effective for applications such as mobile IP hand-offs. We then discuss how to incorporate this methodology in the design of such systems and in the development process.
IEEE;2012;Transparent structural online test for reconfigurable systems;FPGA,,Online Test,,Reconfigurable Architectures;FPGA-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. However, the reliability of modern FPGAs is threatened by latent defects and aging effects. Hence, it is mandatory to ensure the reliable operation of the FPGA's reconfigurable fabric. This can be achieved by periodic or on-demand online testing. In this paper, a system-integrated, transparent structural online test method for runtime reconfigurable systems is proposed. The required tests are scheduled like functional workloads, and thorough optimizations of the test overhead reduce the performance impact. The proposed scheme has been implemented on a reconfigurable system. The results demonstrate that thorough testing of the reconfigurable fabric can be achieved at negligible performance impact on the application.
IEEE;2015;Requirements-Driven Self-Optimization of Composite Services Using Feedback Control;QoS,,earned business value,,process reconfiguration,,quality tradeoffs,,self-optimization,,service selection;In an uncertain and changing environment, a composite service needs to continuously optimize its business process and service selection through runtime adaptation. To achieve the overall satisfaction of stakeholder requirements, quality tradeoffs are needed to adapt the composite service in response to the changing environments. Existing approaches on service selection and composition, however, are mostly based on quality preferences and business processes decisions made statically at the design time. In this paper, we propose a requirements-driven self-optimization approach for composite services. It measures the quality of services (QoS), estimates the earned business value, and tunes the preference ranks through a feedback loop. The detection of unexpected earned business value triggers the proposed self-optimization process systematically. At the process level, a preference-based reasoner configures a requirements goal model according to the tuned preference ranks of QoS requirements, reconfiguring the business process according to its mappings from the goal configurations. At the service level, selection decisions are optimized by utilizing the tuned weights of QoS criteria. We used an experimental study to evaluate the proposed approach. Results indicate that the new approach outperforms both fixed-weighted and floating-weighted service selection approaches with respect to earned business value and adaptation flexibility.
IEEE;2012;DRF4SOA: A Dynamic Reconfigurable Framework for Designing Autonomic Application Based on SOA;Autonomic Computing,,Dynamic Reconfiguration,,Quality of Service,,Service Component Architecture,,Service Oriented Architecture;Service Oriented Architecture (SOA) allows modeling dynamic interaction between heterogeneous providers. Coupling with Service Component Architecture (SCA) enables governing the development of complex applications. The dynamic reconfiguration of such applications is a key feature, since it gives measures to ensure runtime adaptation in order to guarantee Quality of Service (QoS) and manage the performance. For instance, recovering a QoS degradation requires the identification of its sources and the capacity of reconfiguration planning and execution. This paper presents DRF4SOA, a Dynamic Reconfigurable Framework to design autonomic application based on SOA, that implements the autonomic control loop phases (Monitoring, Analysis, Planning, Execution) with SCA in order to provide flexibility to support evolving of itself and to include new non-functional requirements at runtime.
IEEE;2008;Engineering pluripotent information systems;;A pluripotent information system is an open and distributed information system that (i) automatically adapts at runtime to changing operating conditions, and (ii) satisfies both the requirements anticipated at development time, and those unanticipated before but relevant at runtime. Engineering pluripotency into an information system therefore responds to two recurring critical issues: (i) the need for adaptability given the uncertainty in a systempsilas operating environment, and (ii) the difficulty to fully anticipate and account for all possible stakeholderspsila requirements at development time and respond to the change of requirements at runtime. We draw on our grouppsilas research efforts over the last two years to show and discuss how pluripotency can be engineered into information systems.
IEEE;2000;Hierarchical error detection in a software implemented fault tolerance (SIFT) environment;;Proposes a hierarchical error detection framework for a software-implemented fault tolerance (SIFT) layer of a distributed system. A four-level error detection hierarchy is proposed in the context of Chameleon, a software environment for providing adaptive fault tolerance in an environment of commercial off-the-shelf (COTS) system components and software. The design and implementation of a software-based distributed signature monitoring scheme, which is central to the proposed four-level hierarchy, is described. Both intra-level and inter-level optimizations that minimize the overhead of detection and are capable of adapting to runtime requirements are proposed. The paper presents results from a prototype implementation of two levels of the error detection hierarchy and results of a detailed simulation of the overall environment. The results indicate a substantial increase in availability due to the detection framework and help in understanding the tradeoffs between overhead and coverage for different combinations of techniques
IEEE;2015;On the Efficiency of Nature-Inspired Algorithms for Generation of Fault-Tolerant Graphs;;In this study several algorithms for the generation of inexpensive and fault-tolerant graphs are evaluated with respect to the quality of the found graphs and to the runtime requirements. A special focus lies on the properties of the algorithm that basically is a simulation of the foraging of the slime mold Physarum polycephalum, since in many other works the deployment of this algorithm does not go beyond the conclusion, that the algorithm is capable to generate a graph, while quality of the graph and runtime requirements of the algorithm are not reported. Our results show that the slime mold algorithm has some interesting features, however it is not the best means to construct highly efficient graphs out of large sets of nodes.
IEEE;2003;On the advantages of approximate vs. complete verification: bigger models, faster, less memory, usually accurate;;We have been exploring LURCH, an approximate (not necessarily complete) alternative to traditional model checking based on a randomized search algorithm. Randomized algorithms like LURCH have been known to outperform their deterministic counterparts for search problems representing a wide range of applications. The cost of an approximate strategy is the potential for inaccuracy. If complete algorithms terminate, they find all the features they are searching for. On the other hand, by its very nature, randomized search can miss important features. Our experiments suggest that this inaccuracy problem is not too serious. In the case studies presented here and elsewhere, LURCHS random search usually found the correct results. Also, these case studies strongly suggest that LURCH can scale to much larger models than standard model checkers like NuSMV and SPIN. The two case studies presented in this paper are selected for their simplicity and their complexity. The simple problem of the dining philosophers has been widely studied. By making the dinner more crowded, we can compare the memory and runtimes of standard methods (SPIN) and LURCH. When hundreds of philosophers sit down to eat, both LURCH and SPIN can find the deadlock case. However, SPINS memory and runtime requirements can grow exponentially while LURCHS requirements stay quite low. Success with highly symmetric, automatically generated problems says little about the generality of a technique. Hence, our second example is far more complex: a real-world flight guidance system from Rockwell Collins. Compared to NuSMV, LURCH performed very well on this model. Our random search finds the vast majority of faults (close to 90%),, runs much faster (seconds and minutes as opposed to hours),, and uses very little memory (single digits to 10s of megabytes as opposed to 10s to 100s of megabytes). The rest of this paper is structured as follows. We begin with a theoretical rationale for why random search methods like LURCH can be incomplete, yet still successful. Next, we note that for a class of problems, the complete search of standard model checkers can be overkill. LURCH is then briefly introduced and our two case studies are presented.
IEEE;2008;Improved bounds for a deterministic sublinear-time Sparse Fourier Algorithm;Algorithms,,Discrete Fourier transforms,,Fourier transforms,,Number theory,,Signal processing;This paper improves on the best-known runtime and measurement bounds for a recently proposed Deterministic sublinear-time Sparse Fourier Transform algorithm (hereafter called DSFT). In (Iwen, 2008 ), (Iwen, 2007), it is shown that DSFT can exactly reconstruct the Fourier transform (FT) of an N-bandwidth signal f, consisting of B Lt N non-zero frequencies, using O(B<sup>2</sup>ldrpolylog(N)) time and O(B<sup>2</sup> ldr polylog(N)) f-samples. DSFT works by taking advantage of natural aliasing phenomena to hash a frequency- sparse signal's FT information modulo O(B ldr polylog(N)) pairwise coprime numbers via O(B ldr polylog(N)) small Discrete Fourier Transforms. Number theoretic arguments then guarantee the original DFT frequencies/coefficients can be recovered via the Chinese Remainder Theorem. DSFT's usage of primes makes its runtime and signal sample requirements highly dependent on the sizes of sums and products of small primes. Our new bounds utilize analytic number theoretic techniques to generate improved (asymptotic) bounds for DSFT. As a result, we provide better bounds for the sampling complexity/number of low-rate analog-to-digital converters (ADCs) required to deterministically recover frequency-sparse wideband signals via DSFT in signal processing applications (Laska, 2006), (Kirolos et al., 2006).
IEEE;2006;Goal-Oriented Development of BDI Agents: The PRACTIONIST Approach;;The representation of goals and the ability to reason about them play an important role in goal-oriented requirements analysis and modelling techniques, especially in agent-oriented software engineering, as goals are more stable than other abstractions (e.g. user stories). In PRACTIONIST, a framework for developing agent systems according to the Belief-Desire-Intention (BDI) model, goals play a central role. Thus, in this paper we describe the structure of the goal model in the PRACTIONIST framework and how agents use their goal model to reason about goals, desires, and intentions during their deliberation process and means-ends reasoning as well as while performing their activities.
IEEE;2007;Pedestrian Navigation Systems: a Case Study of Deep Personalization;;Requirements Engineering (RE) focuses on obtaining the user goals and environmental constraints of a proposed system. In traditional RE, users are treated as a consumer-class: what holds for one member is assumed to hold for the rest. Personalization comes through providing options that a user can set at runtime to tailor features of the system to his or her personal preferences. In our work, we take up the personalization issue in more detail. In particular, we believe that some systems require a "deep personalization" that includes knowledge of an individual user's skills and limitations. In some cases, these skills and limitations might not be self-aware, i.e., a user cannot accurately self-reflect on his or her skills and weaknesses. In this paper, we will demonstrate the notion of deep personalization in the domain of personal navigation systems. We find this an interesting domain for several reasons: (1) There is a domain theory of navigation skills that draws from both Cartography and Psychology. (2) There are individual differences in navigation skills. (3) An individual user may not be self-aware of his or her skills. (4) If a system is delivered that does not match the skills of the user, it may be less than effective, and at worst, abandoned.
IEEE;2007;High Performance Database Searching with HMMer on FPGAs;;Profile hidden Markov models (profile HMMs) are used as a popular bioinformatics tool for sensitive database searching, e.g. a set of not annotated protein sequences is compared to a database of profile HMMs to detect functional similarities. HMMer is a commonly used package for profile HMM-based methods. However, searching large databases with HMMer suffers from long runtimes on traditional computer architectures. These runtime requirements are likely to become even more severe due to the rapid growth in size of both sequence and model databases. In this paper, we present a new reconfigurable architecture to accelerate HMMer database searching. It is described how this leads to significant runtime savings on off-the-shelf field-programmable gate arrays (FPGAs).
IEEE;2009;From Requirements to Embedded Software - Formalising the Key Steps;;Failure of a design to satisfy a system's requirements can result in schedule and cost overruns. When using current approaches, ensuring requirements are satisfied is often delayed until late in the development process during a cycle of testing and debugging. This paper introduces a more rigorous approach to design using Behavior Engineering, which has previously been applied primarily to requirements analysis and specification development. To support design with Behavior Engineering we introduce the embedded Behavior Runtime Environment, a virtual machine created to execute a Behavior Engineering design on an embedded system. The result is a model-driven development approach that can create embedded system software that satisfies its requirements, as a result of applying the development process.
IEEE;2005;Dynamic preemption threshold scheduling for specific real-time control systems;;Application specific operating systems (ASOS) are developing quickly as a new trend in real-time control systems development. It often belongs to system on chip. The scheduling for ASOS should satisfy two basic demands (a) context switching overheads are not significant,, (b) the scheduling should use small amount of RAM memory. According to characteristics of ASOS, we present a novel scheduling algorithm, named dynamic preemption threshold (DPT) scheduling, which integrates preemption threshold scheduling into the EDF (earliest deadline first). The scheduling can achieve greater processor utilization, theoretically even up to all of a processor capacity. Meanwhile, the preemption times between tasks can be effectively decreased using DPT scheduling by two ways: 1) threads allocating,, 2) dynamic thresholds regularly adjusting at runtime. With the reduction of task preemptions, memory requirements are also decreased. In addition, the DPT gives an approach to transform a static model to dynamic model seamlessly. The DPT algorithm can perfectly schedule a mixed task set with preemptive and non-preemptive tasks, and subsumes both as special cases. Thus it remains the scheduling flexibility and also decreases unnecessary context switching and memory requirements at runtime.
IEEE;2005;Monitoring with behavior view diagrams for debugging;;UML sequence diagrams are widely used during requirements analysis and design for specifying the expected message exchanges among a set of objects in various scenarios for the program to perform a certain task. In this paper, we present the behavior view diagrams, a type of extended sequence diagrams, to facilitate execution monitoring during debugging. Using a behavior view diagram, software developers can precisely specify the runtime objects whose behaviors will be monitored during debugging. Software developers can also specify the important message exchanges to be observed among these objects during the progress of various scenarios, and may further define the monitoring actions to be performed for inspecting the program state when a message exchange is observed. We also present a debugger that can automatically monitor the program execution using the information specified in a behavior view diagram. Through this monitoring, the debugger can not only check whether the scenarios are progressed as intended, but also check whether the actions performed by the program have the the desired effects on the program states. Therefore, it will be useful for detecting and localizing bugs.
IEEE;2014;Rationalism with a dose of empiricism: Case-based reasoning for requirements-driven self-adaptation;;Requirements-driven approaches provide an effective mechanism for self-adaptive systems by reasoning over their runtime requirements models to make adaptation decisions. However, such approaches usually assume that the relations among alternative behaviours, environmental parameters and requirements are clearly understood, which is often simply not true. Moreover, they do not consider the influence of the current behaviour of an executing system on adaptation decisions. In this paper, we propose an improved requirements-driven self-adaptation approach that combines goal reasoning and case-based reasoning. In the approach, past experiences of successful adaptations are retained as adaptation cases, which are described by not only requirements violations and contexts, but also currently deployed behaviours. The approach does not depend on a set of original adaptation cases, but employs goal reasoning to provide adaptation solutions when no similar cases are available. And case-based reasoning is used to provide more precise adaptation decisions that better reflect the complex relations among requirements violations, contexts, and current behaviours by utilizing past experiences. Our experimental study with an online shopping benchmark shows that our approach outperforms both requirements-driven approach and case-based reasoning approach in terms of adaptation effectiveness and overall quality of the system.
IEEE;2004;Goal and scenario driven product line development;;Product line development has proven a successful approach to achieve strategic and large-grained reuse and hence time-to-market and productivity. A key to successful software product lines is to identify and analyze the right functionality for reusable implementation, and thus perform detailed requirements analysis for product lines to exploit commonality and variability (C&V) within a family of related systems. In this paper, we describe the goal and scenario driven approach for developing software product lines, which elicits product line requirements and analyzes C&V in products of a product line, as well as supports developing a particular product in the product line. We also discuss our ultimate goal that is to develop a dynamic software product line, which can produce new products at runtime by dynamic reconfiguration of the product line based on goals and scenarios.
IEEE;2013;Using goals and customizable services to improve adaptability of process-based service compositions;;When implementing (semi-)automatic business processes with services, engineers are facing two sources of variability. One source of variability are alternative refinements and decompositions of requirements. The other source of variability is that various (combinations of) services can be used to satisfy the same requirements. We suggest a method based on the use of a goal model and customizable services able to exploit these variabilities to design executable business process. This method improves the adaptability of the business process at runtime. We illustrate the contribution of our method with an example.
IEEE;2006;Using product line techniques to build adaptive systems;;Adaptive systems are able to adapt their properties and resource requirements at runtime in response to dynamically varying user needs and resource constraints. With the emergence of mobile and service oriented computing, such variation is becoming increasingly common, and the need for adaptivity is increasing accordingly. Software product line engineering has proved itself as an efficient way to deal with varying user needs and resource constraints. In this paper we present an approach to building adaptive systems based on product line oriented techniques such as variability modeling and component based architectures. By representing the product line architecture at runtime, we are able to delegate much of the complexity of adaptation to a reusable adaptation platform. To validate our approach we have built a prototype adaptation platform and developed a few pilot applications exploiting the platform to achieve adaptivity
IEEE;2012;OTERA: Online test strategies for reliable reconfigurable architectures — Invited paper for the AHS-2012 special session “Dependability by reconfigurable hardware”;;FPGA-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. However, the reliability of FPGAs, which are manufactured in latest technologies, is threatened not only by soft errors, but also by aging effects and latent defects. To ensure reliable reconfiguration, it is mandatory to guarantee the correct operation of the underlying reconfigurable fabric. This can be achieved by periodic or on-demand online testing. The OTERA project develops and evaluates components and strategies for reconfigurable systems that feature reliable reconfiguration. The research focus ranges from structural online tests for the FPGA infrastructure and functional online tests for the configured functionality up to the resource management and test scheduling. This paper gives an overview of the project tasks and presents first results.
IEEE;1998;A sequential detailed router for huge grid graphs;;Sequential routing algorithms using maze-running are very suitable for general over-the-cell-routing but suffer often from the high memory or runtime requirements of the underlying path search routine. A new algorithm for this subproblem is presented that computes shortest paths in a rectangular grid with respect to euclidean distance. It achieves performance and memory requirements similar to fast line-search algorithms while still being optimal. An additional application for the computation of minimal rip-up sets is presented. Computational results are shown for a detailed router based on these algorithms that is used for the design of high performance CMOS processors at IBM
IEEE;2007;A Process Splitting Transformation For Kahn Process Networks;;This paper presents a process splitting transformation for Kahn process networks. Running applications written in this parallel program specification on a multiprocessor architecture does not guarantee that the runtime requirements are met. Therefore, it may be necessary to further analyze and optimize Kahn process networks. This paper presents a four-step transformation that result in a functionally equivalent process network, but with a changed and optimized network structure. The class of networks that can be handled is not restricted to static networks. The novelty of this approach is that it can also handle processes with dynamic program statements. The authors illustrate the transformation prototyped in GCC for a JPEG decoder, showing a 21% performance improvements
IEEE;2004;A configurable XForms implementation;;XForms is a new language for defining dynamic forms and user interfaces for the World Wide Web. In order to take advantage of the user interaction related features in the language, a client side processor is needed. This paper describes a configurable open source software implementation of XForms. The main goal of the implementation is to conform to the World Wide Web Consortium's XForms Recommendation. The other goals are external to the XForms specification and are related to the portability and configurability of the processor. The important questions are related to implementing an XForms processor for diverse environments, and the integration of XForms and other XML languages with different layout models. In the paper, more detailed requirements are gathered from these goals. Also, the design and implementation are presented in detail, in order to give insight to the more difficult and nonobvious parts of the software. The results of the paper cover the runtime requirements of the XForms processor.
IEEE;2007;A Fault Detection Mechanism for Fault-Tolerant SOA-Based Applications;Artificial neural network,,Fault detection,,Performance requirement,,Probability change point analysis,,Soa;Fault tolerance is an important capability for SOA-based applications, since it ensures the dynamic composition of services and improves the dependability of SOA-based applications. Fault detection is the first step of fault detection, so this paper focuses on fault detection, and puts forward a fault detection mechanism, which is based on the theories of artificial neural network and probability change point analysis rather than static service description, to detect the services that fail to satisfy performance requirements at runtime. This paper also gives reference model of fault-tolerance control center of enterprise services bus.
IEEE;2010;A Map-Reduce System with an Alternate API for Multi-core Environments;Data-intensive computing,,Map-Reduce,,Middleware,,Multi-Core Architectures;Map-reduce framework has received a significant attention and is being used for programming both large-scale clusters and multi-core systems. While the high productivity aspect of map-reduce has been well accepted, it is not clear if the API results in efficient implementations for different subclasses of data-intensive applications. In this paper, we present a system MATE (Map-reduce with an Alternate API), that provides a high-level, but distinct API. Particularly, our API includes a programmer-managed reduction object, which results in lower memory requirements at runtime for many data-intensive applications. MATE implements this API on top of the Phoenix system, a multi-core map-reduce implementation from Stanford. We evaluate our system using three data mining applications, and compare its performance to that of both Phoenix and Hadoop. Our results show that for all the three applications, MATE outperforms Phoenix and Hadoop. Despite achieving good scalability, MATE also maintains the easy-to-use API of map-reduce. Overall, we argue that, our approach, which is based on the generalized reduction structure, provides an alternate high-level API, leading to more efficient and scalable implementations.
IEEE;2012;Identifying Test Requirements by Analyzing SLA Guarantee Terms;Service Level Agreements,,Service Monitoring,,Software Testing,,Test Requirements;Service Level Agreements (SLAs) are used to specify the negotiated conditions between the provider and the consumer of services. In this paper we present a stepwise method to identify and categorize a set of test requirements that represent the potential situations that can be exercised regarding the specification of each isolated guarantee term of an SLA. This identification is addressed by means of devising a set of coverage levels that allow grading the thoroughness of the tests. The utilization of these test requirements would focus on twofold objectives: (1) the generation of a test suite that allows exercising the situations described in the test requirements and (2) the support for the derivation of a monitoring plan that checks the compliance of these requirements at runtime. The approach is illustrated over an eHealth case study.
IEEE;2009;Frequent itemsets hiding: A performance evaluation framework;;Sensitive knowledge hiding is an essential requirement to prevent disclosure of any sensitive knowledge holding in shared databases. The security of a database may be risked when it is made public as is: because the data mining tools are so sophisticated that the sensitive knowledge can easily be surfaced by receivers. This gives rise to a sanitization process which transforms the original database into another database, the released one, which does not hold the sensitive knowledge but can substitute the original otherwise. In case the sensitive knowledge is of the form frequent itemsets, the resulting concrete problem is called frequent itemsets hiding. A number of algorithms, exploiting different approaches and techniques, for frequent itemsets hiding problem is proposed in the literature. Since finding optimal solutions is NP-Hard, algorithms resort to certain heuristics having different levels of sophistication, complexity, efficiency and effectiveness. This paper presents an evaluation framework which implements recent algorithms belonging to different approaches and a set of metrics to gauge the performance and problem difficulties. The current work also presents an experimental study and its results where four algorithms and seven datasets are involved. Our results indicate that data distortion levels and runtime requirements are quite high, especially for difficult problem instances. Our conclusion is that there are new rooms for more sophisticated and tuneable (w.r.t. effectiveness/efficiency tradeoff) algorithms.
IEEE;1994;FDTD-Simulation of Waveguide Junctions Using a New Boundary Condition for Rectangular Waveguides;;A new (under practical circumstances ideal) absorbing boundary condition (ABC) to terminate homogenous waveguides for use with the FDTD method is presented. The ABC uses a numerical Green's function to achieve a reflection error of the same order of magnitude as the precision of the used floating point type. The use and implementation of this waveguide termination is straight forward and the storage and runtime requirements are small compared to the overall requirements for FDTD simulations. The FDTD method incorporating the new ABC was applied to true three dimensional waveguide junctions. The obtained results show good agreement with previously published results.
IEEE;2001;Semantic models for knowledge management;;We explore the use of a semantic model to support a group of strategic business analysts in their daily work. In particular, we present a set of modeling constructs for representing goals, events and actors that are relevant to the work of analysts. We also describe a qualitative goal analysis procedure which makes it possible to reason about a goal model under different assumptions. The paper also reports on an incremental document classification scheme that can be used to classify relevant documents with respect to the concepts constituting the semantic model.
IEEE;2010;Template based SOA framework for dynamic and adaptive composition of web services;SOA,,Web service,,context,,dynamic composition;Automated Web service composition is currently one of the major research problems in the area of service oriented computing. Web services facilitate seamless business-to-business integration. Whenever it becomes difficult to find a single service for a particular task, a composition of services that can together perform the given task, is required. In general, this is accomplished using Service Oriented Architecture (SOA). However, the requirements of the users are frozen before the system locates, composes and executes the required services. The response is not personalized to the user environment. Further, conventional web services cannot handle the context and the context aware web services need to contain the context processing logic. Hence, we propose a framework for dynamic composition of Web services using templates in SOA. This framework allows maximum flexibility for the users to change their requirements at runtime and provides adaptive composition irrespective of whether a web service is context enabled or not.
IEEE;2005;Construction of weighted finite state transducers for very wide context-dependent acoustic models;;A previous paper by the authors described an algorithm for efficient construction of weighted finite state transducers for speech recognition when high-order context-dependent models of order K > 3 (triphones) with tied state observation distributions are used, and showed practical application of the algorithm up to K = 5 (quinphones). In this paper we give additional details of the improved implementation and analyze the algorithm's practical runtime requirements and memory footprint for context-orders up to K = 13 (+/-6 phones context) when building fully cross-word capable WFSTs for large vocabulary speech recognition tasks. We show that for typical systems it is possible to use any practical context-order K les 13 without having to fear an exponential explosion of the search space, since the necessary state ID to phone transducer (resembling a phone-loop observing all possible K-phone constraints) can be built in a few minutes at most. The paper also gives some implementation details of how we efficiently collect context statistics and build phonetic decision trees for very wide context-dependent acoustic models
IEEE;2010;Two-Staged Approach for Semantically Annotating and Brokering TV-related Services;Digital TV,,Linked Data,,Linked Services,,Semantic Web,,Semantic Web Services;Nowadays, more and more distributed digital TV and TV-related resources are published on the Web, such as Electronic Personal TV Guide (EPG) data. To enable applications to access these resources easily, the TV resource data is commonly provided by Web service technologies. The huge variety of data related to the TV domain and the wide range of services that provide it, raises the need to have a broker to discover, select and orchestrate services to satisfy the runtime requirements of applications that invoke these services. The variety of data and heterogeneous nature of the service capabilities makes it a challenging domain for automated web-service discovery and composition. To overcome these issues, we propose a two-stage service annotation approach, which is resolved by integrating Linked Services and IRS-III semantic web services framework, to complete the lifecycle of service annotating, publishing, deploying, discovering, orchestration and dynamic invocation. This approach satisfies both developer's and application's requirements to use Semantic Web Services (SWS) technologies manually and automatically.
IEEE;2014;On requirements representation and reasoning using answer set programming;;We describe an approach to the representation of requirements using answer set programming and how this leads to a vision for the role of artificial intelligence techniques in software engineering with a particular focus on adaptive business systems. We outline how the approach has developed over several years through a combination of commercial software development and artificial intelligence research, resulting in: (i) a metamodel that incorporates the notion of runtime requirements, (ii) a formal language for their representation and its supporting computational model (InstAL), and (iii) a software architecture that enables monitoring of distributed systems. The metamodel is the result of several years experience in the development of business systems for e-tailing, while InstAL and the runtime monitor is on-going research to support the specification, verification and application of normative frameworks in distributed intelligent systems. Our approach derives from the view that in order to build agile systems, the components need to be structured more like software that controls robots, in that it is designed to be relatively resilient in the face of a non-deterministic, dynamic, complex environment about which there is incomplete information. Thus, degrees of autonomy become a strength and an opportunity, but must somehow be constrained by informing these autonomous components what should be done in a certain situation or what system state ought to be achieved through norms as expressions of requirements. Because such a system made up of autonomous components is potentially behaviourally complex and not just complicated, it becomes essential to monitor both whether norms/requirements are being fulfilled and if not why not. Finally, because control over the system can be expressed through requirements in the form of data that can be changed, a route is opened to adjustment and dynamic re-direction of running systems.
IEEE;2002;SIMOO-RT-an object-oriented framework for the development of real-time industrial automation systems;;Presents SIMOO-RT, an object-oriented framework designed to support the whole development cycle of real-time industrial automation systems. It is based on the concept of distributed active objects, which are autonomous execution entities that have their own thread of control, and that interact with each other by means of remote methods invocation. SIMOO-RT covers most of the development phases, from requirements engineering to implementation. It starts with the construction of an object model of the technical plant to be automated, on which user and problem-domain requirements are captured. Here, emphasis on modeling timing constraints is given. The technical details involved in the process of mapping problem-domain objects to design specific entities as well as the automatic code generation for the runtime application are discussed in the paper. Furthermore, details are given on how to monitor the runtime applications and to evaluate its timing restrictions.
IEEE;1999;On-chip multimedia real-time OS and its MPEG-2 applications;;Proposes a very small on-chip multimedia real-time operating system (OS) for embedded system LSIs and demonstrates its usefulness on MPEG-2 multimedia applications. The real-time OS, which has a new cyclic task with `suspend' and `resume' for the interacting hardware/software of embedded system LSIs, implements the minimum set of task, interrupt and semaphore management on the basis of an analysis of embedded software requirements. It requires only about 2.5 KBytes memory at run-time, reduces redundant conventional cyclic task execution steps to about 1/2 for hardware/software interactions and provides sufficient performance in real time by implementing two typical embedded software packages for practical multimedia system LSIs. This on-chip multimedia real-time OS can be easily integrated on many embedded-system LSIs and provides an efficient embedded software design environment
IEEE;1997;The integrated specification and analysis of functional, temporal, and resource requirements;;The Graphical Communicating Shared Resources, GCSR, is a specification language with a precise, operational semantics for the specification and analysis of real-time systems. GCSR allows a designer to integrate the functional and temporal requirements of a real-time system along with its run-time resource requirements. The integration is orthogonal in the sense that it produces system models that are easy to modify, e.g., to reflect different resource requirements, allocations and scheduling disciplines. In addition, it renders the verification of resource related requirements natural and straightforward. The formal semantics of GCSR allows the simulation of a system model and the thorough verification of system requirements through equivalence checking and state space exploration. This paper reviews GCSR and reports our experience with the production cell case study
IEEE;2011;Modelling adaptability and variability in requirements;adaptive systems,,feature models,,modelling,,natural language processing;The requirements and design level identification and representation of dynamic variability for adaptive systems is a challenging task. This requires time and effort to identify and model the relevant elements as well as the need to consider the large number of potentially possible system configurations. Typically, each individual variability dimension needs to identified and modelled by enumerating each possible alternative. The full set of requirements needs to be reviewed to extract all potential variability dimensions. Moreover, each possible configuration of an adaptive system needs to be validated before use. In this demonstration, we present a tool suite that is able to manage dynamic variability in adaptive systems and tame such system complexity. This tool suite is able to automatically identify dynamic variability attributes such as variability dimensions, context, adaptation rules, and soft/hard goals from requirements documents. It also supports modelling of these artefacts as well as their run-time verification and validation.
IEEE;2011;Scenario-Based Specification of Automotive Requirements With Quantitative Constraints and Synthesis of SL/SF Monitors;Monitor synthesis,,Simulink/Stateflow,,scenario-based specification;Requirements of embedded systems often describe the system behavior with quantitative constraints over parameters such as timing, memory, and other resources. In this letter, we present a visual language suited for scenario-based specification of requirements with quantitative constraints. Our language, known as event sequence charts with quantitative constraints (ESC-QC), is inspired by message sequence charts (MSC) and its variants. We introduce ESC-QC notations through an example from automotive requirements and then describe the formal syntax and semantics. Besides being useful for formal documentation and analysis of system requirements, ESC-QC specifications can be translated into monitors and used for run-time verification of designs. In automotive systems Simulink/Stateflow (SL/SF) is widely used for design of control systems. We have developed an algorithm for automatic synthesis of SL/SF monitors from ESC-QC specifications. We have used this algorithm for generating monitors for verification of controller models from active safety and body control applications.
IEEE;2012;[Title page];;The following topics are dealt with: requirements engineering,, uncertainty handling,, requirements process,, requirements management and tracing,, legal and regulatory requirements,, RE@Runtime,, feature models,, requirements communication,, goal modeling,, product management,, aspect oriented RE,, natural language,, formalized specification,, and prioritization.
IEEE;2014;Optimizing Multi-objective Evolutionary Algorithms to Enable Quality-Aware Software Provisioning;Cloud,,Hyper-heuristics,,MOEAs,,Optimization,,Software Deployment;Elasticity is a key feature for cloud infrastructures to continuously align allocated computational resources to evolving hosted software needs. This is often achieved by relaxing quality criteria, for instance security or privacy because quality criteria are often conflicting with performance. As an example, software replication could improve scalability and uptime while decreasing privacy by creating more potential leakage points. The conciliation of these conflicting objectives has to be achieved by exhibiting trade-offs. Multi-Objective Evolutionary Algorithms (MOEAs) have shown to be suitable candidates to find these trade-offs and have been even applied for cloud architecture optimizations. Still though, their runtime efficiency limits the widespread adoption of such algorithms in cloud engines, and thus the consideration of quality criteria in clouds. Indeed MOEAs produce many dead-born solutions because of the Darwinian inspired natural selection, which results in a resources wastage. To tackle MOEAs efficiency issues, we apply a process similar to modern biology. We choose specific artificial mutations by anticipating the optimization effect on the solutions instead of relying on the randomness of natural selection. This paper introduces the Sputnik algorithm, which leverages the past history of actions to enhance optimization processes such as cloud elasticity engines. We integrate Sputnik in a cloud elasticity engine, dealing with performance and quality criteria, and demonstrate significant performance improvement, meeting the runtime requirements of cloud optimization.
IEEE;2014;Supporting Elasticity in OpenMP Applications;OpenMP,,cloud computing,,elasticity,,parallel applications;Elasticity can be seen as the ability of a system to increase or decrease the computing resources allocated in a dynamic and on demand way. In order to explore this feature, several works addressed the development of frameworks and platforms focusing the construction of elastic parallel and distributed applications for IaaS clouds. However, none of these works addressed the exploration of elasticity in multithreaded applications. In this paper, we propose a mechanism to provide elasticity support for OpenMP applications, making possible the dynamic provisioning of cloud resources taking into account the program structure and runtime requirements. In our proposal, the OpenMP directives were modified to support the dynamic adjustment of resources and a set of routines were included to the user-level library in order to enable the configuration of the the elastic execution. Dynamic memory allocation support was also included in elastic OpenMP library. We also present the architecture and implementation of the proposed mechanism. The experiments validate our approach and show some possibilities to use the elastic OpenMP.
IEEE;2009;Model transformation of dependability-focused requirements models;;Recent research has focused on extending standard requirements elicitation processes to address potential abnormal situations that can interrupt normal system interaction at run-time. We proposed a process, DREP, that extends use case-driven modelling with elements that allow the modelling of system behaviour in exceptional situations. This paper discusses the challenge of using the notions of exceptional behaviour and outcomes defined in use cases within a MDE process. In order to create a more formal specification model with activity diagrams, the use cases have to be well-formed to begin with. We describe precise transformation rules to systematically create an activity diagram corresponding to each use case. Special stereotypes are introduced to document partial or degraded outcomes and handling activities. The model resulting from the transformation unambiguously specifies the system interactions required to satisfy the user, as well as exceptional interactions that can lead to degraded service provision.
IEEE;2010;An Agent-Based System to Support Assurance of Security Requirements;Secure Tropos,,Security assurance,,Security requirements,,multi-agents systems,,security verification;Current approaches to evaluating security assurance either focus on the software development stage or at the end product software. However, most often, it is after the deployment or implementation phase that specified security requirements may be violated. This may be due to improper deployment of the security measures, environmental hazards or to the fact that the assumptions under which the security requirements have been specified have become invalid. As such, this paper proposes an approach (supported by a system) which will complement security requirements engineering methodologies by gathering continuous evidence to inform on whether the security requirements elucidated during system development stage have been correctly implemented and as such, they can be relied upon to effectively protect system assets at runtime. We use Secure Tropos methodology to highlight the security assurance case and elicit the features of our security assurance evaluation system. We further depict the security assurance evaluation through an example based on firewalls configurations.
IEEE;2012;Dynamic phase-based tuning for embedded systems using phase distance mapping;Cache tuning,,configurable architectures,,configurable hardware,,dynamic reconfiguration,,energy savings,,phase-based tuning;Phase-based tuning specializes a system's tunable parameters to the varying runtime requirements of an application's different phases of execution to meet optimization goals. Since the design space for tunable systems can be very large, one of the major challenges in phase-based tuning is determining the best configuration for each phase without incurring significant tuning overhead (e.g., energy and/or performance) during design space exploration. In this paper, we propose phase distance mapping, which directly determines the best configuration for a phase, thereby eliminating design space exploration. Phase distance mapping applies the correlation between a known phase's characteristics and best configuration to determine a new phase's best configuration based on the new phase's characteristics. Experimental results verify that our phase distance mapping approach determines configurations within 3% of the optimal configurations on average and yields an energy delay product savings of 26% on average.
IEEE;2008;Mulit-level system integration based on AUTOSAR;automatic code-generation,,automotive,,embedded software,,rapid prototyping;The design of distributed embedded real-time system is a challenging task. Besides solving the control-engineering issues, one has to consider real-time scheduling, reliability and production requirements w.r.t. production cost of the electronic control unit (ECU). This has a considerable impact on the employed software design techniques. These design techniques are well known in the automotive software industry, but are applied with different flavors at each vehicle manufacturer and their suppliers. This situation has changed considerably with the results of the AUTOSAR development partnership, which unifies the flavors of automotive software design. Automotive software design is embedded in the so-called V-Cycle of embedded automotive system development[1]. It starts with the requirements analysis which results later on in a model of the control algorithm. The control algorithm is tested against a vehicle model and establishes the topmost level in system integration. The second system integration level is the adaptation of the control algorithm to be run on a rapid-prototyping system. The rapid-prototyping system is integrated into an existing E/Earchitecture. The E/E-architecture consists of the ECUs connected by networks like CAN or FlexRay and gateways. Sensors- and actuators being used by several control algorithms are coupled to an ECU, which might propagate signals to other ECUs via a vehicle network. From the software point of view, the controlalgorithm has now to respect real-time scheduling and the quantization of the sensor- and actuator signals, no matter whether these signals are generated on the rapid-prototyping system or exchanged via the bus with other ECUs. Further development steps in the V-cycle are the software implementation, the ECU- and the network integration. The integrated ECUs and networks are tested against vehicle models running on Hardware-in-the-loop (HiL) systems. If this works fine, the ECUs are integrated in the real vehicle for - alibration. The software implementation and the ECU integration are deeply influenced by AUTOSAR. AUTOSAR is a development partnership of all stakeholders in the automotive software development (e.g. vehicle manufacturers and their suppliers) which unifies several software implementation techniques[2]. It describes a common ECU software architecture[3] consisting of configurable basic software modules (BSW), a runtime environment (RTE) and a software component description[4]. The software component description describes the interfaces for dataexchange as well as the access points for the RTE. The basic idea of consisting of interconnected software components which are later mapped to an E/E-architecture. Currently, the VFB structure of AUTOSAR software architectures is mainly driven by the next generation E/E-architectures. The VFB structure forms the third system integration level. The next integration step into an AUTOSAR environment is the integration of the rapid-prototyping tested control algorithm to AUTOSAR software components. Since most VFB descriptions use fixed-point interfaces, the control algorithm has to be transformed to fixed-point arithmetic. If the control algorithm is modelled in tools like ASCET, this conversion can be achieved by code-generation. The same holds true for control algorithms already being used in E/E-architectures without an AUTOSAR software architecture. The VFB-description with the control-algorithms forms the fourth system integration level, which can be simulated with plant-models on a PC by tools like INTECRIO-VP. The fifth system integration level is given by the mapping process as defined in the AUTOSAR methodology[5] and requires the configuration of the RTE and the BSW modules for a single ECU. At this integration level, one can perform HiL testing and calibration in the same way as for non-AUTOSAR systems. Several evaluation projects, e.g. [6] and [7], have shown that the multi-level integration approach is feasible to g
IEEE;2015;CloudFreq: Elastic Energy-Efficient Bag-of-Tasks Scheduling in DVFS-Enabled Clouds;bag-of-tasks,,cloud computing,,dynamic voltage and frequency scaling,,elastic,,energy-efficient,,scheduling;Energy consumption imposes a significant cost for data centers in providing cloud services. Many studies explore the opportunities to save power by energy-efficient task scheduling based on the technique of dynamic voltage and frequency scaling (DVFS). However, most of them assume that energy budgets and/or deadline constraints are known in advance. But these information can hardly be acquired in general computing environments, such as cloud computing, and job rejections caused by restricted constraints are intolerable to guarantee the service-level agreement (SLA). Moreover, previous works prefer to provide “black-box” algorithms with little consideration on adjustability, and cannot satisfy runtime requirements in performance and energy-saving. This paper proposes an elastic energy-efficient algorithm called CloudFreq for bag-of-tasks scheduling in DVFS-enabled clouds. CloudFreq enables a model of elastic, adjustable energy-efficient scheduling without any prior knowledge of constraints, and then eliminates job rejections accordingly. CloudFreq also provides an entry for operators to scale system performance at runtime. Experimental results demonstrate that the proposed algorithm can effectively perform energy-efficient scheduling without constraints, and has the capability of making an appropriate tradeoff to improve the weighted balance between schedule length and energy-saving.
IEEE;2009;Robust on-line model-based object detection from range images;Object detection,,point clouds,,range images;A mobile robot that accomplishes high level tasks needs to be able to classify the objects in the environment and to determine their location. In this paper, we address the problem of online object detection in 3D laser range data. The object classes are represented by 3D point-clouds that can be obtained from a set of range scans. Our method relies on the extraction of point features from range images that are computed from the point-clouds. Compared to techniques that directly operate on a full 3D representation of the environment, our approach requires less computation time while retaining the robustness of full 3D matching. Experiments demonstrate that the proposed approach is even able to deal with partially occluded scenes and to fulfill the runtime requirements of online applications.
IEEE;2010;Utilizing parallelism of TMR to enhance power efficiency of reliable ASIC designs;Circuit design,,Power Consumption,,Power-Aware design,,Reliability,,Triple Modular Redundancy;Due to aggressive scaling, reliability issues influence the design process of integrated circuits more and more. A well known technique to tackle these issues represents Triple Modular Redundancy (TMR). It strongly improves reliability of a design at the expense of at least tripled area and power consumption. In this contribution, we propose an enhanced TMR approach that significantly decreases the power overhead of conventional TMR designs. Therefore, the control logic was modified so as to switch between a TMR mode and a parallel mode. This parallel mode allows the circuit to operate with decreased frequency without losing performance by taking advantage of the parallelism offered by the tripled design. Achieved results of investigations on the ISCAS benchmark circuits show power savings of up to 50% with a small reliability penalty compared to a conventional TMR approach for permanent failures. We also propose strategies how to utilize both operating modes in order to balance the design concerning reliability and power consumption requirements at runtime.
IEEE;2013;Towards preventing error propagation in a real-time Ethernet switch;;Flexible Time-Triggered communication (FTT) allows a distributed embedded system (DES) to adapt to changing real-time requirements at runtime. This facilitates the continuous operation of the DES under dynamic environments that change over time. However, for continuous operation, high reliability in the nodes of the DES is also crucial. This can be achieved using node replication, as long as failure independence between replicas is ensured, which calls for preventing the propagation of errors. Our goal is to prevent the propagation of Byzantine node behaviours and to ensure that local errors in the channel cannot disturb the global communication. For this, we construct the HaRTES/PG switch, a new switch based on the HaRTES implementation of FTT for Ethernet. This paper presents as a first step a study of the possible errors that may lead to Byzantine node behaviours and a global communication disturbance in HaRTES, as well as some ideas on how to prevent the propagation of these errors in HaRTES/PG.
IEEE;2014;Computation scalable disparity estimation for delay sensitive 3D video surveillance system;;Disparity estimation is an important task in many 3D video surveillance applications. How to generate the disparity information at the front end under limited delay budget is very challenging in a real-time surveillance system. In this paper, we tackle this problem through adopting multiresolution strategy in the disparity estimation process. Our contribution is twofold. First, unlike existing coarse-to-fine strategies based on uniform sampling, we present a fast disparity estimation algorithm based on nonuniform sampling at the coarse level. The disparity values of the non-samples are initially interpolated through Delaunay Triangulation, and then are refined through bilateral filtering. The content-aware nonuniform sampling provides better disparity approximation in the triangulated interpolation, and consequently leads to faster convergence in the refinement procedure. Second, we model the computation time in the disparity estimation process for execution assistance in the delay sensitive surveillance system. Both the sampling cell size and the data resolution are considered in this model, in order to accommodate different runtime requirements. Experimental results demonstrate the efficiency of the proposed scheme.
IEEE;2011;A fast real-time rendering method of 3D terrain using out-of-core visualization;Real-time rendering,,out-of-core,,terrain;In this paper, we propose an improved algorithm for real-time rendering of large 3D scene, in which we combine quad-tree hierarchy, level of detail (LOD) and out-of-core algorithm. In order to get an efficient rendering method, we construct a scene hierarchy to maintain the rendering scene tree, use quad-tree to simplify terrain meshes, and import the out-of-core algorithm to reduce memory requirements. There are two key steps in his algorithm: Firstly, we will simplify the scene data using quad-tree algorithm. Then we could get a set of simplified meshes for rendering. Secondly, View-dependent out-of-core method is imported to determine which part of meshes should be rendering or delete from the memory. Then we could reduce the memory requirements at runtime. In the last part of the paper, we use terrain data to test out algorithm. Comparing to traditional quad-tree algorithm our method run faster and need less memory requirements.
IEEE;2001;Software implementation of synchronous programs;;Synchronous languages allow a high level, concurrent, and deterministic description of the behavior of reactive systems. Thus, they can be used advantageously for the programming of embedded control systems. The runtime requirements of synchronous code are light, but several critical properties must be fulfilled. In this paper, we address the problem of the software implementation of synchronous programs. After a brief introduction to reactive systems, this paper formalizes the notion of "execution machine" for synchronous code. Then, a generic architecture for centralized execution machines is introduced. Finally, several effective implementations are presented
IEEE;2012;Middleware for Differentiated Quality in Spontaneous Networks;middleware,,mobile applications,,mobile computing,,support services;Spontaneous-network management requires application-driven middleware to address differentiated application-specific requirements at runtime. Real Ad hoc Multihop Peer-to-peer (RAMP) middleware easily deploys over existing and heterogeneous wireless networks, supporting adaptive and per-application strategies even in challenging scenarios, such as multimedia streaming.
IEEE;1990;IEEE Standard Glossary of Software Engineering Terminology;Definitions,,Software engineering,,dictionary,,glossary,,terminology;This IEEE Standards product is part of the family on Software Engineering. This standard identifies terms currently in use in the field of Software Engineering. Standard definitions for those terms are established.
IEEE;2009;Runtime monitoring of cross-cutting policy;;In open systems, certain unfavorable situations due to unanticipated user behavior may be seen, which results in a violation of cross-cutting policy. This paper proposes a runtime monitoring method to check such problems. Since there is a large gap, a certain link is needed between the policy and runtime execution method. We employ a two-step checking approach,, an offline symptom checking and a runtime monitoring. The ingredient to tie the two steps is a Linear-time Temporal Logic formula for the cross-cutting policy to look at.
IEEE;2015;Maintaining consistency of file system by monitoring file system parameters at runtime using consistency checking rules;Consistency checking rules,,File system checker,,Journaling,,Runtime checking,,Transaction commit points;Current research in the file system domain has shown the existence of bugs that disrupt the working and the consistency of file system operations. File systems today, though have grown more robust to various bugs,, however, still they are vulnerable to crashes and corruptions due to bugs that are unknown to the system. The metadata information is an important part of a file system since it keeps the meta information of all the data within it. It is thus necessary to handle this meta information effectively for smooth functioning of the file system. Solutions such as the file system checker tool `fsck', `SQCK' tool, and the technique such as journaling, provide safety measures for maintaining consistency by performing checks against the file system bugs. But, however, these solutions suffer from the limitation that these checks can only be performed while the system is either not in use or is unmounted. This limitation can be overcome with the concept of runtime checking, where files and directories are observed even when the user is still using the system. Thus, in this paper, a runtime checking system is presented that checks various file system parameters that are likely to have a corruption during a working file system.
IEEE;2011;Goal-Driven Adaptation of Service-Based Systems from Runtime Monitoring Data;goal-oriented requirements engineering,,iStar,,runtime adaptation,,service-oriented system,,variability modelling;Service-based systems need to provide flexibility to adapt both to evolving requirements from multiple, often conflicting, ephemeral and unknown stakeholders, as well as to changes in the runtime behavior of their component services. Goal-oriented models allow representing the requirements of the system whilst keeping information about alternatives. We present the MAESoS approach which uses i* diagrams to identify quality of service requirements over services. The alternatives are extracted and kept in a variability model. A monitoring infrastructure identifies changes in runtime behavior that can propagate up to the level of stakeholder goals and trigger the required adaptations. We illustrate the approach with a scenario of use.
IEEE;2008;Towards Requirements Engineering in a Service-Oriented Environment--Extending the SOA Interaction Triangle;Requirements Engineering,,Service-oriented development;In this paper we present an approach towards requirements engineering for the development of software systems in a service-oriented context. It differs from conventional requirements engineering as it includes additional stakeholders and considers specifics of service-oriented environments. Thus, our approach integrates methods and practices from traditional requirements engineering to extend the SOA interaction triangle. A case study will be presented to illustrate the applicability of the method.
IEEE;2011;Addressing requirements engineering challenges in the context of Emergent Systems;Emergent Systems,,cloud computing,,perspective-based views,,prioritization,,requirements elicitation,,requirements specification,,reuse,,service orientation;Dominant trends in today's IT research such as service orientation and cloud computing will enable novel business information systems, called Emergent Systems. However, the introduction of Emergent Systems will have a significant impact and pose challenges relating to requirements engineering. In this position paper we propose and describe the so-called SoMBRERO framework that aims to address practical problems in the area of requirements engineering that will arise during the development process of future information systems. Furthermore, we provide a research agenda that guides our ongoing work to address research questions in this area.
IEEE;2008;Service-Centric Systems and Requirements Engineering;Requirements engineering,,service-oriented architectures,,web services;This mini-tutorial introduces Web services and service-centric systems, identifies their impact on requirements engineering, and introduces new tools and techniques for engineer requirements for service-centric systems.
IEEE;2013;Intention-oriented modelling support for GORE in elastic cloud applications;Cloud Applications,,GORE,,Intention,,Neptune;Businesses started to exploit the forthcoming value from deployment of cloud computing as a new caterpillar paradigm to reach out more diversified customer slices. Although the general concepts they practically focus on are: viability, survivability, adaptability, etc., however, on the ground, there is still a lack for forming mechanisms to sustain viability with adaptation of new types of requirements that pertain to other un-tackled aspects of the echosystem. Such aspects like social intentionality are of actors and their goals. This paper introduces modern dynamic software programming environment aided with modelling support to achieve operationalization and adaptation of abstract object,, goals and their properties as formation of new type of requirements into service based applications distributed over the cloud. This will in turn provide system runtime components to interactively confer to guarantee self-adaptive behaviour with respect to its functional and non-functional characteristics.
IEEE;2009;Model-based Runtime Verification of Web Service Interface Contracts;A General Literature,,A.0 General,,Contracts,,D.2 Software Engineering,,D.2.0 General,,D.2.4 Software/Program Verification,,D.2.5.g Monitors,,D.2.5.r Testing tools,,F.4.1.b Computational logic,,F.4.1.k Temporal logic,,F.4.2 Grammars and Other Rewriting Systems,,H.3.5 Online Information Services,,H.3.5.e Web-based services,,Monitoring,,Protocols,,Runtime,,Servers,,Software Engineering,,Web services,,XML,,project management,,requirements engineering,,services computing,,software architecture;Web applications are required to follow an interface contract that specifies their expected behaviour when they communicate with a web service. Using the Amazon E-&#x00AD,,Commerce Service as an example, we show how we can automatically test an implementation for conformance as well as monitor at runtime that each partner fulfills its part of the contract.
IEEE;2014;Engineering topology aware adaptive security: Preventing requirements violations at runtime;;Adaptive security systems aim to protect critical assets in the face of changes in their operational environment. We have argued that incorporating an explicit representation of the environment's topology enables reasoning on the location of assets being protected and the proximity of potentially harmful agents. This paper proposes to engineer topology aware adaptive security systems by identifying violations of security requirements that may be caused by topological changes, and selecting a set of security controls that prevent such violations. Our approach focuses on physical topologies,, it maintains at runtime a live representation of the topology which is updated when assets or agents move, or when the structure of the physical space is altered. When the topology changes, we look ahead at a subset of the future system states. These states are reachable when the agents move within the physical space. If security requirements can be violated in future system states, a configuration of security controls is proactively applied to prevent the system from reaching those states. Thus, the system continuously adapts to topological stimuli, while maintaining requirements satisfaction. Security requirements are formally expressed using a propositional temporal logic, encoding spatial properties in Computation Tree Logic (CTL). The Ambient Calculus is used to represent the topology of the operational environment - including location of assets and agents - as well as to identify future system states that are reachable from the current one. The approach is demonstrated and evaluated using a substantive example concerned with physical access control.
IEEE;2009;Requirements Analysis of Real-Time Systems by Rational-Rose UML;;The unified modeling language (UML) is one of the most commonly used tools for analyzing and expressing software requirements. Considerably, UML overcomes ambiguity limitation which results from infamous shortcoming of national language. However, how to express requirements of real-time systems properly is still a very challenging problem. In this paper, main aspects of UML for real-time system are highlighted, and typical implementation of real-time system by UML is investigated as well. At the end, a guideline generated for financial account platform of Zhejiang Provincial Finance Department is presented by UML use cases, activity diagrams, and interface diagrams in details.
IEEE;2007;A Refined Goal Model for Semantic Web Services;;The idea of service orientation envisions dynamic detection and execution of suitable Web services for solving a particular request. Most realization approaches pay only little attention to the client side of such architectures. We therefore promote a goal-driven approach: a client merely specifies the objective to be achieved in terms of a goal, and the system resolves this by automated detection, composition, and execution of Web services. Extending the WSMO framework, we present a model for describing goals as formalized client objectives that carry all information relevant for automated detection and execution of Web services.
IEEE;2011;Social Software Product Lines;Models at Runtime,,Social Software Engineering,,Software Product Lines,,Users feedback;Software product lines are an engineering paradigm to systematically configure software products from reusable assets so that development effort and time are minimized. Configuring a high quality product is challenging, because quality is a dynamic property and can be difficult to determine at design time. In this paper, we propose Social Software Product Lines paradigm (SSPL) which exploits users' perception in judging products quality and guiding the configuration process at runtime. The SSPL paradigm advocates two principles. First, quality has to be evaluated iteratively during the product operation so that quality evaluation is kept up-to-date. Second, users are the primary evaluators of quality and their feedback is a primitive driver of configuration. At runtime, SSPL obtains users' quality feedback and reconfigures repeatedly in order to deliver the product found to be most adequate by the users' community. We discuss motivation and foundations of SSPL, and outline a set of research challenges.
IEEE;2008;Requirements Capture with RCAT;model checking,,requirements,,runtime verification,,state machines,,temporal logic;NASA spends millions designing and building spacecraft for its missions. The dependence on software is growing as spacecraft become more complex. With the increasing dependence on software comes the risk that bugs can lead to the loss of a mission. At NASApsilas Jet Propulsion Laboratory new tools are being developed to address this problem. Logic model checking and runtime verification can increase the confidence in a design or an implementation. A barrier to the application of such property-based checks is the difficulty in mastering the requirements notations that are currently available. For these techniques to be easily usable, a simple but expressive requirement specification method is essential. This paper describes a requirements capture notation and supporting tool that graphically captures formal requirements and converts them into automata that can be used in model checking and for runtime verification.
IEEE;2013;End-to-End Formal Specification, Validation, and Verification Process: A Case Study of Space Flight Software;Astronautics,,behavior,,formal methods,,metrics,,process,,requirements engineering,,runtime execution monitoring,,software,,statechart assertions,,verification and validation (V&V);The quality of requirements and the effectiveness of verification and validation (V&V) techniques in guaranteeing that a final system reflects its established requirements have a direct influence on the quality and dependability of the delivered system. The V&V process can be efficient from a managerial point of view, but ineffective from a technical perspective, and vice versa. This paper presents an end-to-end formal computer-aided specification, validation, and verification (SV&V) process, whose feasibility and effectiveness were evaluated against the flight software for the Brazilian Satellite Launcher. Unified modeling language (UML) statechart assertions, scenario-based validation, and runtime verification are used to formally specify and verify the system, and metrics of the ongoing process and its V&V results are collected during the application of the process. The results of the case study indicate that the process and its computer-aided environment were both technically feasible to apply and managerially effective, will likely scale well to cater to SV&V of mission-critical systems that have a larger number of behavioral requirements, and can be used for V&V in a distributed development environment.
IEEE;2014;Eliciting contextual requirements at design time: A case study;adaptive systems,,design time,,exploratory,,requirements engineering,,socio-technical systems;The need to consider context in order to understand requirements is established in requirements engineering. Recently, this has been discussed more intensively for sociotechnical systems, which offer a rich spectrum of different operating contexts. Contextual requirements proved valuable to model requirements together with the context they are valid in, but there is a lack of research on how to derive them from stakeholder needs. Our goal in this paper is to explore the usefulness of existing requirements elicitation techniques for the identification of contextual requirements early, i.e. at design time. In a case study we investigate end-user viewpoints, together with interviews, scenarios, prototyping, goal-based analysis, and groupwork as a means to elicit and clarify contextual requirements already at design time. In our case study a certain combination of the applied requirements elicitation techniques stood out as most beneficial for the identification of contextual requirements. In addition, we discovered valuable indicators of differences in the operative context, for example when end-users cannot agree on refinements of specific requirements. Designers and operators of adaptive systems might benefit by taking such conflicts and resulting contextual requirements into account.
IEEE;2012;(Requirement) evolution requirements for adaptive systems;Requirements engineering,,adaptive systems,,evolution,,modeling,,requirements;It is often the case that stakeholders want to strengthen/weaken or otherwise change their requirements for a system-to-be when certain conditions apply at runtime. For example, stakeholders may decide that if requirement R is violated more than N times in a week, it should be relaxed to a less demanding one R-. Such evolution requirements play an important role in the lifetime of a software system in that they define possible changes to requirements, along with the conditions under which these changes apply. In this paper we focus on this family of requirements, how to model them and how to operationalize them at runtime. In addition, we evaluate our proposal with a case study adopted from the literature.
IEEE;2012;Intention-oriented modelling support for socio-technical driven elastic cloud applications;Cloud Applications,,GORE,,Intention,,Neptune;Businesses have already started to exploit potential uses of cloud computing as a new paradigm for promoting their services. Although the general concepts they practically focus on are: viability, survivability, adaptability, etc., however, there is still a lack of forming mechanisms to sustain viability with adaptation of new requirements in cloud-based applications. This has inspired a pressing need to adopt new methodologies and abstract models which support system acquisition for self-adaptation, thus guaranteeing autonomic cloud application behaviour. In this paper we use state-of-the-art Neptune framework as runtime adaptive software development environment developed mainly for distributed computing supported with intention-oriented modelling language. The representation and adaptation of goal based model artifacts and their intrinsic properties requirements will in turn support distributed service based applications running in the cloud. This harnesses obedient system behaviour with respect to its functional and non-functional characteristics.
IEEE;2013;Support for intention driven cloud government entitlement services;Elastic Clouds,,GORE,,Intention Modeling,,PAA,,XACML;This paper examines the role of judicial cloud based services in assurance of law enforcement and access to justice for citizen social networks. We are using open service oriented model standard XACML as dynamic runtime based business process modeling, while citizens can dynamically express their instant needs using intentions. We foresee these intentions as temporal goal oriented models by automatically reasoning the requirements driven analysis for these goals. Our approach provides complete business model architecture that stimulates citizen needs. The new contribution of our approach shows how temporal changes of citizen's intentions shall provide different role based entitlement as forced by government laws. We examine judicial entitlement services like notarization services and how different purpose based citizen-to-citizen, citizen-to-business scenarios for each service shall be helpful to anticipate context interactions, thus to avoid business conflict of interest between each of citizens, their societies and government laws and policies. Failing to fulfill minimum requirement for such law enforcement, will reason for automatic re-routing of such intention model to use litigation services, which also contain automatic escalation and management to court based litigation services.
IEEE;2010;Engineering a Sound Assertion Semantics for the Verifying Compiler;Software verification,,assertions,,logics of programs,,programming by contract,,requirements engineering.;The Verifying Compiler (VC) project is a core component of the Dependable Systems Evolution Grand Challenge. The VC offers the promise of automatically proving that a program or component is correct, where correctness is defined by program assertions. While several VC prototypes exist, all adopt a semantics for assertions that is unsound. This paper presents a consolidation of VC requirements analysis (RA) activities that, in particular, brought us to ask targeted VC customers what kind of semantics they wanted. Taking into account both practitioners' needs and current technological factors, we offer recovery of soundness through an adjusted definition of assertion validity that matches user expectations and can be implemented practically using current prover technology. For decades, there have been debates concerning the most appropriate semantics for program assertions. Our contribution here is unique in that we have applied fundamental software engineering techniques by asking primary stakeholders what they want and, based on this, proposed a means of efficiently realizing the semantics stakeholders want using standard tools and techniques. We describe how support for the new semantics has been added to ESC/Java2, one of the most fully developed VC prototypes. Case studies demonstrate the effectiveness of the new semantics at uncovering previously indiscernible specification errors.
IEEE;2015;Bridging the gap between requirement analysis and architecture design of self-adaptive systems;MAPE-K control loop,,goal model,,requirement model,,self-adaptive system,,software architecture model;Modern software systems often run on a changing and unpredictable environment, they need to adapt itself at runtime in response to changing requirements of users and running environment. Therefore, numerous methods have been proposed to support the development of self-adaptive system. Some approaches are requirements-based. Other approaches are architecture-based. By considering only a partial view of software systems, such proposals are limited in specifying the adaptivity of a software system. Aiming at this problem, this paper will discuss how to bridge the gap between the requirement analysis and architecture design of self-adaptive systems.
IEEE;2009;Specifying and Monitoring Interactions and Commitments in Open Business Processes;Web services,,business processes,,open software systems,,requirements analysis,,runtime monitoring;This paper shows the developments in Web services and associated standards which offer the possibility of making composite, service-based systems a reality for many enterprises. This approach specifies and monitors interactions among heterogeneous services by tracking their commitments. Such an approach can support service and process evolution.
IEEE;2015;Revisiting Goal-Oriented Models for Self-Aware Systems-of-Systems;goal-awareness,,goals,,missions,,requirements engineering,,self-awareness,,systems-of-systems;Systems-of-systems (SoS) are systems resulted from the interaction among other independent constituent systems that collaborate to offer new functionalities towards accomplishing global missions. Each of these constituent systems accomplishes its individual missions and is able to contribute to the achievement of the global missions of the SoS, both being viewed as a set of associated goals. In the perspective of self-aware systems, SoS need to exhibit goal-awareness, i.e., They need to be aware of their own goals and of how their constituent systems contribute to their accomplishment. In this paper, we revisit goal-oriented concepts aiming at identifying and modeling goals at both SoS level and the constituent systems level. Moreover, we take advantage of such goal-oriented models to express the relationship among goals at these levels as well as to define how each constituent system can contribute to the accomplishment of global goals of an SoS. In addition, we shed light on important issues related to goal modeling in self-aware SoS to be addressed in future research.
IEEE;2010;A Roadmap for Comprehensive Requirements Modeling;Requirements analysis,,Requirements monitoring,,Runtime validation;Most large software systems result from weaving together many independently developed systems. Like Shelley's Frankenstein, such systems risk inheriting undesirable properties. Requirements monitoring can sound the alert should these creations fail to meet their obligations.
IEEE;2016;Situation Analytics: A Foundation for a New Software Engineering Paradigm;aware computing,,cognitive science,,context awareness,,human-computer interaction,,machine learning,,requirements engineering,,runtime adaptation,,services computing,,situation awareness,,software engineering;Advances in cognitive science along with modern-day smart technologies and software services that take into account our mental state will enable a software industry that is poised to meet customers' needs on the fly in new and truly individualized ways.
IEEE;2012;A taxonomy of uncertainty for dynamically adaptive systems;Dynamically adaptive systems,,design,,requirements engineering,,runtime,,taxonomy,,uncertainty;Self-reconfiguration enables a dynamically adaptive system (DAS) to satisfy requirements even as detrimental system and environmental conditions arise. A DAS, especially one intertwined with physical elements, must increasingly reason about and cope with unpredictable events in its execution environment. Unfortunately, it is often infeasible for a human to exhaustively explore, anticipate, or resolve all possible system and environmental conditions that a DAS will encounter as it executes. While uncertainty can be difficult to define, its effects can hinder the adaptation capabilities of a DAS. The concept of uncertainty has been extensively explored by other scientific disciplines, such as economics, physics, and psychology. As such, the software engineering DAS community can benefit from leveraging, reusing, and refining such knowledge for developing a DAS. By synthesizing uncertainty concepts from other disciplines, this paper revisits the concept of uncertainty from the perspective of a DAS, proposes a taxonomy of potential sources of uncertainty at the requirements, design, and execution phases, and identifies existing techniques for mitigating specific types of uncertainty. This paper also introduces a template for describing different types of uncertainty, including fields such as source, occurrence, impact, and mitigating strategies. We use this template to describe each type of uncertainty and illustrate the uncertainty source in terms of an example DAS application from the intelligent vehicle systems (IVS) domain.
IEEE;2013;Active and Adaptive Services Resource Provisioning with Personalized Customization;ATOM,,personalized customization,,requirements engineering,,services resource provisioning;Software as a service(SaaS), we are moving to the age of service-oriented software engineering(SOSE). But for the goal of services computing, namely on-demand service, it has not been able to achieved by far, especially the active provisioning approach for services resource. In view of these facts of services aggregation, i.e. The relative deficient services resource, single provision structure and passive selection mode for service requesters. Active provisioning of services resource with personalized customization will be focused. In this paper, we have established software architecture with personalized active custom for services resource. Customization of the active provisioning of services resource mainly includes two aspects: Firstly, for unmatched services resource of service composition, sliced or segmentation method should be chosen to acquire the individual needs of services resource according to the overall requirements. Secondly, a huge amount of legacy software will be comprehensively reused, namely servicelization. Through personalized customization of services resource, it will achieve on-demand active provisioning to furnish adequate material for dynamic services aggregation. It will also provide the production framework, process guidance and engineering support of CASE tools for services resource provisioning in runtime based on personalized customization. The feasibility and efficiency of the proposed approach are verified by a series of experiments.
IEEE;2010;Adaptive Goals for Self-Adaptive Service Compositions;Goals,,Service Compositions,,supervision;Service compositions need to continuously self- adapt to cope with unexpected failures. In this context adaptation becomes a fundamental requirement that must be elicited along with the other functional and non functional requirements. Beside modelling, effective adaptation also demands means to trigger it at runtime as soon as the actual behavior of the composition deviates from stated requirements. This paper extends traditional goal models with adaptive goals to support continuous adaptation. Goals become live, runtime entities whose satisfaction level is dynamically updated. Furthermore, boundary infringement triggers adaptation capabilities. The paper also provides a methodology to trace goals onto the underlying composition, assess goals satisfaction at runtime, and activate adaptation consequently. All the key elements are demonstrated on the definition of the process to control an advanced washing machine.
IEEE;2011;Monitoring fuzzy temporal requirements for service compositions: Motivations, challenges and experimental results;Fuzzy Goals,,Requirements Monitoring,,Self-Adaptive Systems,,Service-Compositions;Service compositions are an important family of self-adaptive systems, which need to cope with the variability of the environment (e.g., heterogeneous devices, changing context), and react to unexpected events (e.g., changing components) that may take place at runtime. To this aim, it is fundamental to continuously assess requirements while the system is executing and detect partial mismatches or handle uncertainty. Detecting the entity of a violation is very helpful, since it can guide the way applications adapt at runtime. This paper is based on the FLAGS language we already proposed in our previous work to represent requirements as fuzzy temporal formulas and identify partial violations at the temporal level. The paper illustrates the advantages of using the FLAGS language to express the requirements of service compositions, and proposes a technique to monitor them at runtime. The experimental evaluation demonstrates that the monitoring technique is feasible and the overhead introduced in the running system is negligible.
IEEE;2008;Building Contingencies into Specifications;Arbitration,,Contingencies,,Feature Interaction;We propose an approach to runtime feature composition and conflict resolution that combines arbitration and contingencies. By arbitration we mean the resolution of conflicts between features using priorities. Contingency means having several specifications per feature, satisfying the same requirement, depending on the current state of the shared resource. Evaluation of our approach shows that combining arbitration and contingencies ensures that in the event of a conflict, requirements of the conflicting features are eventually satisfied.
IEEE;2001;XML-based method and tool for handling variant requirements in domain models;;A domain model describes common and variant requirements for a system family. UML notations used in requirements analysis and software modeling can be extended with variation points to cater for variant requirements. However, UML models for a large single system are already complicated enough. With variants UML domain models soon become too complicated to be useful. The main reasons are the explosion of possible variant combinations, complex dependencies among variants and inability to trace variants from a domain model down to the requirements for a specific system, member of a family. We believe that the above mentioned problems cannot be solved at the domain model description level alone. We propose a novel solution based on a tool that interprets and manipulates domain models to provide analysts with customized, simple domain views. We describe a variant configuration language that allows us to instrument domain models with variation points and record variant dependencies. An interpreter of this language produces customized views of a domain model, helping analysts understand and reuse software models. We describe the concept of our approach and its simple implementation based on XML and XMI technologies
IEEE;2009;Towards a Unified Framework for Contextual Variability in Requirements;;Context is a significant factor in deciding the set of requirements relevant to a system (i.e., software product construction), the alternatives the system can adopt to satisfy these requirements, and the quality assessment of each alternative. By context we mean the conditions in the operating environment of an system that influences how the system should behave in different situations. However, the relationship between context and requirements can be challenging to capture and analyze. Presently this area of requirements engineering is largely under-researched. In this position paper, we discuss several ways by which context can be related to requirements and subsequently used for product derivation. We outline an approach that facilitates better understanding and use of contextual information in requirements. Our approach integrates three requirements engineering approaches - goal modeling, feature modeling, and problem frames - and is aimed at facilitating treatment of contextual variability in requirements.
IEEE;2012;Stateful requirements monitoring for self-repairing socio-technical systems;goal models,,requirements monitoring,,self-repair;Socio-technical systems consist of human, hardware and software components that work in tandem to fulfill stakeholder requirements. By their very nature, such systems operate under uncertainty as components fail, humans act in unpredictable ways, and the environment of the system changes. Self-repair refers to the ability of such systems to restore fulfillment of their requirements by relying on monitoring, reasoning, and diagnosing on the current state of individual requirements. Self-repair is complicated by the multi-agent nature of socio-technical systems, which demands that requirements monitoring and self-repair be done in a decentralized fashion. In this paper, we propose a stateful requirements monitoring approach by maintaining an instance of a state machine for each requirement, represented as a goal, with runtime monitoring and compensation capabilities. By managing the interactions between the state machines, our approach supports hierarchical goal reasoning in both upward and downward directions. We have implemented a customizable Java framework that supports experimentation by simulating a socio-technical system. Results from our experiments suggest effective and precise support for a wide range of self-repairing decisions in a socio-technical setting.
IEEE;2015;A requirements monitoring model for systems of systems;System of systems,,requirements monitoring;Many software systems today can be characterized as systems of systems (SoS) comprising interrelated and heterogeneous systems developed by diverse teams over many years. Due to their scale, complexity, and heterogeneity engineers face significant challenges when determining the compliance of SoS with their requirements. Requirements monitoring approaches are a viable solution for checking system properties at runtime. However, existing approaches do not adequately consider the characteristics of SoS: different types of requirements exist at different levels and across different systems,, requirements are maintained by different stakeholders,, and systems are implemented using diverse technologies. This paper describes a three-dimensional requirements monitoring model (RMM) for SoS providing the following contributions: (i) our approach allows modeling the monitoring scopes of requirements with respect to the SoS architecture,, (ii) it employs event models to abstract from different technologies and systems to be monitored,, and (iii) it supports instantiating the RMM at runtime depending on the actual SoS configuration. To evaluate the feasibility of our approach we created a RMM for a real-world SoS from the automation software domain. We evaluated the model by instantiating it using an existing monitoring framework and a simulator running parts of this SoS. The results indicate that the model is sufficiently expressive to support monitoring SoS requirements of a directed SoS. It further facilitates diagnosis by discovering violations of requirements across different levels and systems in realistic monitoring scenarios.
IEEE;2001;Requirements-based dynamic metrics in object-oriented systems;;Because early design decisions can have a major long term impact on the performance of a system, early evaluation of the high-level architecture can be an important risk mitigation technique. This paper proposes a technique for predicting the volume of data that will flow across a network in a distributed system. The prediction is based upon anticipated execution of scenarios and can be applied at an extremely early stage of the design. It is driven by requirements specifications and captures dynamic metrics by defining typical usage patterns in terms of scenarios. Scenarios are then mapped to architectural components, and dataflow across inter-partition links is estimated. The feasibility of the approach is demonstrated through an experiment in which predicted metrics are compared to runtime measurements
IEEE;2010;Self-Tuning of Software Systems Through Goal-based Feedback Loop Control;control theory,,goal reasoning,,self-tuning;Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators rather than high-level requirements. By linking the overall satisfaction to a business value indicator as feedback, we propose a control theoretic self-tuning method that can dynamically adjust the tradeoff decisions among different quality requirements. A preference-based reasoning algorithm is involved to configure hard goals accordingly to guide the following architecture reconfiguration.
IEEE;2006;Discovering early aspects;architecture,,aspect orientation,,design,,life cycle,,requirements;Aspect-oriented software development has focused on the software life cycle's implementation phase: developers identify and capture aspects mainly in code. But aspects are evident earlier in the life cycle, such as during requirements engineering and architecture design. Early aspects are concerns that crosscut an artifact's dominant decomposition or base modules derived from the dominant separation-of-concerns criterion, in the early stages of the software life cycle. In this article, we describe how to identify and capture early aspects in requirements and architecture activities and how they're carried over from one phase to another. We'll focus on requirements and architecture design activities to illustrate the points, but the same ideas apply in other phases as well, such as domain analysis or in the fine-grained design activities that lie between architecture and implementation
IEEE;2012;Software Product Line Engineering for Developing Self-Adaptive Systems: Towards the Domain Requirements;domain requirements,,self-adaptative system,,software product line;Self-adaptive systems are now facing the anticipation of mass customization. Therefore, the Software Product Line (SPL) engineering for developing Self-Adaptive systems (SPL4SA) can be an effective way. At the first sight, SPL4SA is the straightforward combination of the two methodologies of SPL engineering and self-adaptive systems. However, the direct and unsystematic combination will bring difficulty in the domain requirements analysis and in the customization process. In this paper, in order to give a solution to the practical problems, we propose a domain requirements meta-model in SPL4SA. It is described with different point of views and the variability binding constraints inside are emphasized. Based on it, a guidance is concluded to support the consistent customization towards the domain model. In addition, an experimental study about a web-based business product line involving self-adaptation capability is conducted to evaluate the model.
IEEE;2007;Dynamic Requirements Specification for Adaptable and Open Service Systems;;The Dynamic Requirements Adaptation Method (DRAM) is suggested to assist existing RE methodologies in updating requirements specifications at runtime for adaptable and open service-oriented systems. Updates are needed because an adaptable and open system continually changes how and to what extent initial requirements are achieved.
IEEE;1996;Archiving system states by persistent objects;;The paper describes one specific aspect of the software component construction in the life cycles of computer based systems. The construction is located following the requirements analysis, conception and design. Systems are designed as within the ECBS process so called services which consist of a set of objects working together, keeping the states of the system. To ensure an efficient and rapid construction of systems an easy to use mechanism to store and retrieve objects together with their relations is necessary. This demands an embedded method of keeping objects in a database-the persistence. The described mechanism is implemented using C++ and verified in some projects. A man machine service is used as an example to show the application of this approach
IEEE;2012;From use cases and their relationships to code;;Use cases are used in many methodologies to drive the software engineering process. Though, their transition to code was usually a mostly manual process. In the context of MDD, use cases gain attention as first-class artifacts with representation notations allowing for automatic transformations to analysis and design models. The paper concentrates on an important problem of constructing transformations that cater for use case relationships. It presents a notation that unifies the ambiguous “include” and “extend”, and allows for representing them within textual use case scenarios. This notation, equipped with runtime semantics, is used to construct a direct transformation into working code. The code is placed within method bodies of the Controller/Presenter and View layers within the MVC/MVP framework. Based on this transformation, an agile use-case-driven development process is possible.
IEEE;2007;A Sound Assertion Semantics for the Dependable Systems Evolution Verifying Compiler;;The verifying compiler (VC) project is a core component of the dependable systems evolution grand challenge. The VC offers the promise of automatically proving that a program or component is correct, where correctness is defined by program assertions. While several VC prototypes exist, all adopt a semantics for assertions that is unsound. This paper presents a consolidation of VC requirements analysis activities that, in particular, brought us to ask targeted VC customers what kind of semantics they wanted. Taking into account both practitioners' needs and current technological factors, we offer recovery of soundness through an adjusted definition of assertion validity that matches user expectations and can be implemented practically using current prover technology. We describe how support for the new semantics has been added to ESC/Java2. Preliminary results demonstrate the effectiveness of the new semantics at uncovering previously indiscernible specification errors.
IEEE;2006;Towards an Integrated Approach for Aspectual Requirements;;This paper presents an aspectual requirements approach to handle separation, modularization, representation and composition of crosscutting concerns. The approach includes a process model, a metamodel to define rigorously the main concepts, and a tool to support the approach
IEEE;2009;Intellectual Property Rights Requirements for Heterogeneously-Licensed Systems;;Heterogeneously-licensed systems pose new challenges to analysts and system architects. Appropriate intellectual property rights must be available for the installed system, but without unnecessarily restricting other requirements, the system architecture, and the choice of components both initially and as it evolves. Such systems are increasingly common and important in e-business, game development, and other domains. Our semantic parameterization analysis of open-source licenses confirms that while most licenses present few roadblocks, reciprocal licenses such as the GNU General Public License produce knotty constraints that cannot be effectively managed without analysis of the system's license architecture. Our automated tool supports intellectual property requirements management and license architecture evolution. We validate our approach on an existing heterogeneously-licensed system.
IEEE;2011;Toward Inconsistency Awareness in Collaborative Software Development;Change Support Model,,Change Support Workflow,,Collaborative Software Development,,Inconsistency Awareness,,Patterns of Inconsistency;Change management is a key issue in collaborative software development. In a collaborative work, the fact that many change processes applied to shared artifacts are executed concurrently leads to the inconsistency problem. Most of previous studies addressed only conflicts, a type of inconsistency caused by concurrent change activities on shared artifacts. In this paper, we define patterns of inconsistency, including conflict patterns, concerning the change context instead of only concurrent change activities. To deal with these inconsistencies, we propose an approach that is a combination of the process support approach and awareness support approach. We model change processes as Change Support Workflows (CSWs) and manage their execution. We then introduce a Change Support Model (CSM) based on this approach. CSM is a model of a dynamic workflow management system. In CSM, inconsistency awareness is implemented using workspace awareness and context awareness techniques. Requirements, static model, architecture, and dynamic model of CSM are also specified. Our approach will help workers to implement changes more safely and efficiently in collaborative environments.
IEEE;2006;Using Aspects to Simplify iModels;;This poster outlines a set of rules to systematically identify crosscutting concerns present in i* models, and modularizing them using aspects. The result is the reduction of the complexity of those models
IEEE;2008;Characterizing maintainability concerns in autonomic element design;;Autonomic computing has become more prevalent in recent years for its vision of developing applications with self-adaptive and self-managing behavior. Due to the inherent complexity of such applications and the nature of the built-in closed-loop feedback control, maintainability issues of autonomic systems are emerging as significant concerns in autonomic system designs. This paper identifies and categorizes types of common forms of autonomic element patterns and reveals the inherent relationships among them as well as their particular maintainability concerns. The key to maintainability of self-managing systems is their embedded control loops. Good software engineering practice calls for making the control loops as independent as possible to achieve loose coupling and separate concerns. However, typical self-managing systems solutions feature arrangements of interdependent, collaborative autonomic elements. This paper outlines selected autonomic element patterns derived from requirements goal models and attribute-based architectural styles for self-adaptive systems and then identifies their particular maintainability concerns based on the characteristics of the solutionpsilas control loops. Maintainability issues for the various autonomic element patterns are illustrated using a book store example.
IEEE;1995;Requirements monitoring in dynamic environments;;We propose requirements monitoring to aid in the maintenance of systems that reside in dynamic environments. By requirements monitoring we mean the insertion of code into a running system to gather information from which it can he determined whether, and to what degree, that running system is meeting its requirements. Monitoring is a commonly applied technique in support of performance tuning, but the focus therein is primarily on computational performance requirements in short runs of systems. We wish to address systems that operate in a long lived, ongoing fashion in nonscientific enterprise applications. We argue that the results of requirements monitoring can be of benefit to the designers, maintainers and users of a system-alerting them when the system is being used in an environment for which it was not designed, and giving them the information they need to direct their redesign of the system. Studies of two commercial systems are used to illustrate and justify our claims.
IEEE;2006;Making Sense of Requirements, Sooner;requirements modeling,,software technologies;Making decisions during early requirements formulations is like racing through a dark forest without a road map or compass. The available options or goals are often unclear. Different participants in the formulation process have different points of view and frequently end up making decisions based on political rather than technical grounds. Using AI-based simulation tools can cut back the forest to reveal clear paths to possibilities and useful requirements. Researchers have long used modeling and simulation to analyze complex processes. In this approach, we describe the system's properties and behavior symbolically and numerically, and then let the computer unleash its brute force to grind through the combinations. We have been developing our own model-based technique for making early requirements decisions. The technique uses two curious properties seen in many models, clumps and collars, which make it easier to search quickly through a seemingly vast range of options
IEEE;2015;Modularity for Uncertainty;Known Unknown,,Partial Model,,Uncertainty;Uncertainty can appear in all aspects of software development: uncertainty in requirements analysis, design decisions, implementation, and testing. As the research on uncertainty is so young, there are many issues to be tackled. Modularity for Uncertainty is one of them. If uncertainty can be dealt with modularly, we can add or delete uncertain concerns to/from models, code, and tests whenever these concerns arise or are fixed to certain concerns. To deal with this challenging issue, we propose a modularization mechanism for uncertainty. Agile methods embrace change to accept changeable user requirements. On the other hand, our approach embraces uncertainty to support exploratory development. This paper sets out a focused research agenda for uncertainty in terms of the new modularity vision.
IEEE;2005;Using black-box persistent state manifest for dependency management in patching and upgrading J2EE based applications;;This paper describes a new approach to J2EE based software upgrade and patch management using black-box persistent state manifests which helps in self-managing the upgrade and patching process of Java based applications. We created a tool called object version checker which uses black-box based persistent state manifests to aid in software upgrades and patches to check the versions of installed components and identify components that are not installed correctly and cause potential problems to the whole enterprise. We provide a theoretical insight of the requirements, usage and usefulness of the tool.
IEEE;2009;Evaluating Organizational Configurations;Formal Framework for Multi-Agent Systems,,Graph Theory,,Organization Oriented Modeling;A Multi-Agent System is often conceived as an organization of autonomous software agents that participate into social and evolving structures (e.g., organizational configurations) suitable to deal with highly dynamic environments. Nevertheless, systems based on agent technologies rarely capitalize on their potentials since their systemic properties— e.g., flexibility, robustness and efficiency—are typically only the byproduct of the (AI) techniques deployed at the implementation level, and are neither explicit object of study nor are taken into consideration at a requirements engineering phase. The paper presents a method, based on graph theory, to exactly compare and evaluate software design system configurations in the engineering of multiagent systems. The theoretical results are presented and validated on a crisis management scenario.
IEEE;2014;MapReduce-based warehouse systems: A survey;Big data,,MapReduce,,Mastiff,,ORC File,,RCFile,,column store,,data storage structure,,hybrid-store,,row-store;Today's world is of data explosion, the size of data sets is increasing continuously with the huge amount of volume, where various data sets being managed and examined are called “Big data”. The analysis of Big data is growing rapidly in the industry for business intelligence making traditional warehousing solution prohibitively expensive instead of conventional database systems which have difficulty to manage Big data. MapReduce is a computing paradigm that has not only gained a lot of response now days from research and industry, but also recognized as a more effective tool for large-scale data analysis. The MapReduce framework and its open-source implementation Hadoop provide an accessible and fault-tolerant structure for large scale data analysis and it has been successfully built in social network websites and major Web service providers. This paper overviews MapReduce-based data placement structure for Big data analysis, namely row store, column-store, hybrid store, RCFile, Mastiff and ORC File.
IEEE;2012;Tool support for combined rule-based and goal-based reasoning in Context-Aware systems;Context-aware,,User Requirements Notation,,goal-based reasoning,,reasoning,,rule-based,,systems;Context-aware systems often use rule-based reasoning engines for decision making without involving explicit interaction with the user. While rule-based systems excel in filtering out unsuitable solutions based on clear criteria, it is difficult to rank suitable solutions based on vague, qualitative criteria with a rule-based approach. Moreover, the description of such systems is typically ad-hoc without well-defined modeling tasks. CARGO (Context-Aware Reasoning using Goal-Orientation) aims to address these problems by combining rule-based and goal-based reasoning as well as scenario-based modeling to provide a more comprehensive way to define context-aware systems and to process contextual information. This demo presents CARGO, a modeling, simulation, and execution environment for context-aware systems built on existing tool support for the User Requirements Notation.
IEEE;2010;A method to acquire compliance monitors from regulations;;Developing software systems in heavily regulated industries requires methods to ensure systems comply with regulations and law. A method to acquire finite state machines (FSM) from stakeholder rights and obligations for compliance monitoring is proposed. Rights and obligations define what people are permitted or required to do,, these rights and obligations affect software requirements and design. The FSM allows stakeholders, software developers and compliance officers to trace events through the invocation of rights and obligations as pre- and post-conditions. Compliance is monitored by instrumenting runtime systems to report these events and detect violations. Requirements and software engineers specify the rights and obligations, and apply the method using three supporting tasks: 1) identify under-specifications, 2) balance rights with obligations, and 3) generate finite state machines. Preliminary validation of the method includes FSMs generated from U.S. healthcare regulations and tool support to parse these specifications and generate the FSMs.
IEEE;1953;Crosstalk in radio systems caused by foreground reflections;;The effect of cross polarization was studied on two of three antennas tested. The interfering transwitting antenna was changed from its normal vertical polarization to horizontal polarization, and the effect on the reflections was noted. At Buckingham, the nearby reflections were reduced by 3 db, and the more distant reflections by 8 db. At New Holland, the distant reflection was reduced 10 db. These low numbers are in sharp contrast to the 30 odd db cross polartzation discrimination that can be realized in direct transmission at these frequencies. So this study calls to attention another limitation on selection of sites for radio relay repeater stations, already beset by a multitude of interlocking requirements. Reflection troubles can be minimized by carefully serutinizing foreground geographies of proposed repeater sites for possible reflecting agencies, and by so arranging the transmission paths as to minimize illumination of possible reflectors.
ScienceDirect;2015;Modeling and verification of Functional and Non-Functional Requirements of ambient Self-Adaptive Systems ;Non Functional Requirements, Model Driven Engineering, Relax, Dynamic Adaptive Systems, Properties verification, Goal Oriented Requirements Engineering , ;Abstract Self-Adaptive Systems modify their behavior at run-time in response to changing environmental conditions. For these systems, Non-Functional Requirements play an important role, and one has to identify as early as possible the requirements that are adaptable. We propose an integrated approach for modeling and verifying the requirements of Self-Adaptive Systems using Model Driven Engineering techniques. For this, we use Relax, which is a Requirements Engineering language which introduces flexibility in Non-Functional Requirements. We then use the concepts of Goal-Oriented Requirements Engineering for eliciting and modeling the requirements of Self-Adaptive Systems. For properties verification, we use OMEGA2/IFx profile and toolset. We illustrate our proposed approach by applying it on an academic case study. 
ScienceDirect;2012;Using RELAX, SysML and KAOS for Ambient Systems Requirements Modeling ;Ambient Systems, Non Functional Requirements, Dynamic Adaptive Systems, Domain Speciﬁc Language, Relax, Requirements Engineering, Ambient Assisted Living , ;Ambient Systems are highly adaptive. They modify their behavior at run-time in response to changing environmental conditions. For these systems, Non Functional Requirements (NFR's) play an important role, and one has to identify as early as possible the requirements that are adaptable. Because of the inherent uncertainty in these systems, goal based approaches can help in the development of their requirements. Relax, which is a Requirement Engineering (RE) language for adaptive systems, can introduce ﬂexibility in NFR's to adapt to any changing environmental conditions. We illustrate our proposal through a case study of an Ambient Assisted Living (AAL) system. We use an existing goal oriented approach, based on Kaos, which extends the SysML 11http://www.sysml.org/specs meta-model and our proposed Domain Speciﬁc Language (DSL) for Relax,, that enables to derive requirements in graphical format from textual requirements in the form of SysML requirements diagrams. In this paper we show how we have integrated these two approaches for a better modeling of these systems 
ScienceDirect;2013;Intention-oriented programming support for runtime adaptive autonomic cloud-based applications ;;Abstract The continuing high rate of advances in information and communication systems technology creates many new commercial opportunities but also engenders a range of new technical challenges around maximising systems’ dependability, availability, adaptability, and auditability. These challenges are under active research, with notable progress made in the support for dependable software design and management. Runtime support, however, is still in its infancy and requires further research. This paper focuses on a requirements model for the runtime execution and control of an intention-oriented Cloud-Based Application. Thus, a novel requirements modelling process referred to as Provision, Assurance and Auditing, and an associated framework are defined and developed where a given system’s non/functional requirements are modelled in terms of intentions and encoded in a standard open mark-up language. An autonomic intention-oriented programming model, using the Neptune language, then handles its deployment and execution. 
ScienceDirect;2012;Value models for engineering of complex sustainable systems ;Sustainable Systems, Value Network Models, Requirements Engineering , ;Sustainable Systems are typically complex systems-of-systems spanning across several technical domains and organizations. They could include different kinds of sub-systems such as energy systems, information technology systems, transportation systems, buildings, etc. The organizations (e.g., companies and infrastructure providers) are involved in complex exchanges of goods, services and value. To make such systems sustainable but also economically feasible, it is important to analyze and understand these exchanges. We propose to integrate Value Network Models (VNMs) into analysis and design of complex sustainable systems. VNMs include, e.g., actors (typically organizations but also customers), value objects (goods, services, money), activities that generate value, and their dependencies. They should also include the information about the environmental impact and could be used in the context requirements engineering and high-level systems architecting. In addition, we propose to make VNMs available not only for design but also during system's operations using information and communication technology systems. Thus, exchange of value represented within VNMs could be monitored and evaluated during runtime. We show our approach on the example of Masdar City, a project of a planned city designed to be a sustainable, minimal emissions, and minimal waste urban environment. 
ScienceDirect;2012;Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop ;Feedback control theory, Preference, Goal-oriented reasoning, Self-tuning, Earned business value , ;Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods. 
ScienceDirect;2005;Early Verification and Validation of Mission Critical Systems ;Requirements Engineering, Goal-orientation, Early Verification, Validation, Animation, Monitoring, Acceptance Tests , ;Our world is increasingly relying on complex software and systems. In a growing number of fields such as transportation, finance, telecommunications, medical devices, they now play a critical role and require high assurance. To achieve this, it is imperative to produce high quality requirements. The KAOS goal-oriented requirements engineering methodology provides a rich framework for requirements elicitation and management of such systems. This paper demonstrates the practical industrial application of that methodology. The non-critical parts are modelled semi-formally using a graphical language for goal-oriented requirements engineering. When and where needed (ie. for critical parts of a system) the model can be specified at formal level using a real-time temporal logic. That formal level seamlessly extends the semi-formal level which can also help hide the formality for the non-specialist. To ensure at an early stage that the right system is being built and that the requirements model is right, validation and verification tools are applied on that model. Early verification checks help to discover missing requirements, overlooked assumptions or incorrect goal refinements. State machines generation from operations provides an executable model useful for validation purposes or for deriving an initial design. Acceptance test cases and runtime behavior monitors can also be derived from the model. The process is supported by an integrated toolbox implementing the above tools by a roundtrip mapping of KAOS requirements level notations to the languages of formal technology tools such as model-checkers, SAT engines or constraint solvers. A graphical visualization framework also significantly helps validation using domain-based representations. 
ScienceDirect;2015;A programming-level approach for elasticizing parallel scientific applications ;Cloud computing, Elasticity, Parallel applications , ;Abstract Elasticity is considered one of the fundamental properties of cloud computing. Several mechanisms to provide the feature are offered by public cloud providers and in some academic works. We argue these solutions are inefficient in providing elasticity for scientific applications, since they cannot consider the internal structure and behavior of applications. In this paper we present an approach for exploring the elasticity in scientific applications, in which the elasticity control is embedded in application source code and constructed using elasticity primitives. This approach enables the application itself to request or to release its own resources, taking into account the execution flow and runtime requirements. To support the construction of elastic applications using the presented approach, we developed the Cloudine framework. Cloudine provides all components necessary to construct and execute elastic scientific applications. The Cloudine effectiveness is demonstrated in the experiments where the platform is successfully used to include new features to existing applications, to extend functionalities of other elasticity frameworks and to add elasticity support to parallel programming libraries. 
ScienceDirect;2014;Uncertainty handling in goal-driven self-optimization – Limiting the negative effect on adaptation ;Uncertainty, Goal-driven self-optimization, Requirements goal models , ;Abstract Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling. 
ScienceDirect;2014;GPU Architecture for Unsupervised Surface Inspection Using Multi-scale Texture Analysis ;Surface Inspection, Image Processing, Texture Analysis, GPGPU , ;Abstract Surface inspection in manufacturing scenarios is strongly related to accuracy and runtime requirements. To ensure high accuracy and reliable defect detection results, in many applications the environment will be modified with respect to constant illumination and well defined system behavior. In these cases early vision algorithms like edge detection or thresholds are applied for surface inspection which are not complex and runtime intensive. In more complex scenarios with changing illumination conditions more elaborate image processing techniques are needed to ensure reliable defect detection, which leads to more runtime intensive algorithms. To overcome this challenges time-consuming operations can be transferred to additional hardware to satisfy the strong runtime constrains even for complex image processing techniques. The graphics processing unit (GPU) as a co-processor offers great potential and massively parallel computing power to enable real-time application of complex computing steps in production scenarios. We introduce a GPU based implementation of unsupervised defect detection on textured surfaces. Evaluation on an artificial dataset confirms excellent defect detection results and real-time performance. 
ScienceDirect;2008;Accelerating molecular dynamics simulations using Graphics Processing Units with CUDA ;Graphics processing unit, Molecular dynamics, Advanced computer architecture , ;Molecular dynamics is an important computational tool to simulate and understand biochemical processes at the atomic level. However, accurate simulation of processes such as protein folding requires a large number of both atoms and time steps. This in turn leads to huge runtime requirements. Hence, finding fast solutions is of highest importance to research. In this paper we present a new approach to accelerate molecular dynamics simulations with inexpensive commodity graphics hardware. To derive an efficient mapping onto this type of computer architecture, we have used the new Compute Unified Device Architecture programming interface to implement a new parallel algorithm. Our experimental results show that the graphics card based approach allows speedups of up to factor nineteen compared to the corresponding sequential implementation. 
ScienceDirect;2008;Integrating FPGA acceleration into HMMer ;Bioinformatics, Reconfigurable computing, Hidden Markov models, Viterbi algorithm, Accelerators , ;HMMer is a commonly used package for biological sequence database searching with profile hidden Markov model (HMMs). It allows researchers to compare HMMs to sequence databases or sequences to HMM databases. However, such searches often take many hours on traditional computer architectures. These runtime requirements are likely to become even more severe due to the rapid growth in size of both sequence and model databases. We present a new reconfigurable architecture to accelerate the two HMMer database search procedures hmmsearch and hmmpfam. It is described how this leads to significant runtime savings on off-the-shelf field-programmable gate arrays (FPGAs). 
ScienceDirect;2014;Progress and improvement of KSTAR plasma control using model-based control simulators ;Magnetic fusion, Tokamak, Simulator, KSTAR, Plasma control, Control design , ;Abstract Superconducting tokamaks like KSTAR, EAST and ITER need elaborate magnetic controls mainly due to either the demanding experiment schedule or tighter hardware limitations caused by the superconducting coils. In order to reduce the operation runtime requirements, two types of plasma simulators for the KSTAR plasma control system (PCS) have been developed for improving axisymmetric magnetic controls. The first one is an open-loop type, which can reproduce the control done in an old shot by loading the corresponding diagnostics data and PCS setup. The other one, a closed-loop simulator based on a linear nonrigid plasma model, is designed to simulate dynamic responses of the plasma equilibrium and plasma current (Ip) due to changes of the axisymmetric poloidal field (PF) coil currents, poloidal beta, and internal inductance. The closed-loop simulator is the one that actually can test and enable alteration of the feedback control setup for the next shot. The simulators have been used routinely in 2012 plasma campaign, and the experimental performances of the axisymmetric shape control algorithm are enhanced. Quality of the real-time EFIT has been enhanced by utilizations of the open-loop type. Using the closed-loop type, the decoupling scheme of the plasma current control and axisymmetric shape controls are verified through both the simulations and experiments. By combining with the relay feedback tuning algorithm, the improved controls helped to maintain the shape suitable for longer H-mode (10–16 s) with the number of required commissioning shots largely reduced. 
ScienceDirect;2010;Hybrid automata, reachability, and Systems Biology ;Semi-algebraic automata, Reachability, Systems Biology , ;Hybrid automata are a powerful formalism for the representation of systems evolving according to both discrete and continuous laws. Unfortunately, undecidability soon emerges when one tries to automatically verify hybrid automata properties. An important verification problem is the reachability one that demands to decide whether a set of points is reachable from a starting region. If we focus on semi-algebraic hybrid automata the reachability problem is semi-decidable. However, high computational costs have to be afforded to solve it. We analyse this problem by exploiting some existing tools and we show that even simple examples cannot be efficiently solved. It is necessary to introduce approximations to reduce the number of variables, since this is the main source of runtime requirements. We propose some standard approximation methods based on Taylor polynomials and ad hoc strategies. We implement our methods within the software SAHA-Tool and we show their effectiveness on two biological examples: the Repressilator and the Delta–Notch protein signaling. 
ScienceDirect;2011;Adaptable Decentralized Service Oriented Architecture ;Dynamic software, Self-* systems, Self-adaptive systems, Service Oriented Architecture, Workflow Decentralization, Distributed orchestration engine, BPEL, Process mining , ;In the Service Oriented Architecture (SOA), BPEL specified business processes are executed by non-scalable centralized orchestration engines. In order to address the scalability issue, decentralized orchestration engines are applied, which decentralize BPEL processes into static fragments at design time without considering runtime requirements. The fragments are then encapsulated into runtime components such as agents. There are a variety of attitudes towards workflow decentralization,, however, only a few of them produce adaptable fragments with runtime environment. In this paper, producing runtime adaptable fragments is presented in two aspects. The first one is frequent-path adaptability that is equal to finding closely interrelated activities and encapsulating them in the same fragment to omit the communication cost of the activities. Another aspect is proportional-fragment adaptability, which is analogous to the proportionality of produced fragments with number of workflow engine machines. It extenuates the internal communication among the fragments on the same machine. An ever-changing runtime environment along with the mentioned adaptability aspects may result in producing a variety of process versions at runtime. Thus, an Adaptable and Decentralized Workflow Execution Framework (ADWEF) is introduced that proposes an abstraction of adaptable decentralization in the SOA orchestration layer. Furthermore, ADWEF architectures Type-1 and Type-2 are presented to support the execution of fragments created by two decentralization methods, which produce customized fragments known as Hierarchical Process Decentralization (HPD) and Hierarchical Intelligent Process Decentralization (HIPD). However, mapping the current system conditions to a suitable decentralization method is considered as future work. Evaluations of the ADWEF decentralization methods substantiate both adaptability aspects and demonstrate a range of improvements in response-time, throughput, and bandwidth-usage compared to previous methods. 
ScienceDirect;2011;CleGo: A package for automated computation of Clebsch–Gordan coefficients in tensor product representations for Lie algebras ;Tensor product, Tensor product decomposition, Clebsch–Gordan coefficients, Lie algebra, Multiple tensor product, Model building, Symmetry breaking, GUT , ;We present a program that allows for the computation of tensor products of irreducible representations of Lie algebras A – G based on the explicit construction of weight states. This straightforward approach (which is slower and more memory-consumptive than the standard methods to just calculate dimensions of the tensor product decomposition) produces Clebsch–Gordan coefficients that are of interest for instance in discussing symmetry breaking in model building for grand unified theories. For that purpose, multiple tensor products have been implemented as well as means for analyzing the resulting effective operators in particle physics. Program summary Program title: CleGo Catalogue identifier: AEIQ_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEIQ_v1_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 3641 No. of bytes in distributed program, including test data, etc.: 34 536 Distribution format: tar.gz Programming language: OʼCaml Computer: i386–i686, x86_64 Operating system: Cross-platform, for definiteness though we assume some UNIX system. RAM: ⩾4 GB commendable, though in general memory requirements depend on the size of the Lie algebras and the representations involved. Classification: 4.2, 11.1 Nature of problem: Clebsch–Gordan coefficients are widely used in physics. This program has been written as a means to analyze symmetry breaking in the context of grand unified theories in particle physics. As an example, we computed the singlets appearing in higher-dimensional operators 27 ⊗ 27 ⊗ 27 ⊗ 78 and 27 ⊗ 27 ⊗ 27 ⊗ 650 in an E 6 -symmetric GUT. Solution method: In contrast to very efficient algorithms that also produce tensor product decompositions (as far as outer multiplicities/Littlewood–Richardson coefficients are concerned) we proceed straightforwardly by constructing all the weight states, i.e. the Clebsch–Gordan coefficients. This obviously comes at the expense of high memory and CPU-time demands. Applying Dynkin arithmetic in weight space, the algorithm is an extension of the one for the addition of angular momenta in su ( 2 ) ≈ A 1 , for reference see [1]. Note that, in general, Clebsch–Gordan coefficients are basis-dependent and therefore need to be understood with respect to the chosen basis. However, singlets appearing in (multiple) tensor products are less basis-dependent. Restrictions: Generically, only tensor products of non-degenerate or adjoint representations can be computed. However, the irreps appearing therein can subsequently be used as new input irreps for further tensor product decomposition so in principle there is no restriction on the irreps the tensor product is taken of. In practice, though, it is by the very nature of the explicit algorithm that input is restricted by memory and CPU runtime requirements. Unusual features: Analytic computation instead of float numerics. Additional comments: The program can be used in “notebook style” using a suitable OʼCaml toplevel. Alternatively, an OʼCaml input file can be compiled which results in processing that is approximately a factor of five faster. The latter mode is commendable when large irreps need to be constructed. Running time: Varies depending on the input from parts of seconds to weeks for very large representations (because of memory exhaustion). Reference:[1] J. Fuchs, C. Schweigert, Symmetries, Lie Algebras and Representations, Cambridge Univ. Press, Cambridge, UK, 1997, 438 p. 
ScienceDirect;2003;chapter 10 - Reflection and Attributes ;;Publisher Summary This chapter focuses on the application called reflections and the extensive use of attributes. Attributes are one type of metadata used to annotate code. They may be pre- or user-defined and help guide the developer toward those modules or classes that satisfy certain requirements. Reflection, in this sense, is analogous to searching on keywords except that attributes are derived classes and are replete with their own methods. Certain attributes, such as Obsolete, inform developers when a code is replaced and must be updated. The chapter introduces the process of reflection, creation and the usage of metadata by using these attributes. Reflection is the ability to look into and access information or metadata about an application itself. Applications such as debuggers, builders, and integrated development environments that extract and rely on this information are called meta-applications. For example, the IntelliSense feature of Visual Studio .NET presents to the user a list of available public members whenever a dot (.) is typed after a class name or an object reference. Furthermore, this chapter discusses reflection, hierarchy and description of metadata about assemblies, modules, classes, and its members. 
SCOPUS;2015;From Means-End Analysis to Proactive Means-End Reasoning;Cognition,, Electronic mail,, Ontologies,, Runtime,, Schedules,, Software agents,, Software systems;Self-adaptation is a prominent property for developing complex distributed software systems. Notable approaches to deal with self-adaptation are the runtime goal model artifacts. Goals are generally invariant along the system lifecycle but contain points of variability for allowing the system to decide among many alternative behaviors. This work investigates how it is possible to provide goal models at run-time that do not contain tasks, i.e. The description of how to address goals, thus breaking the design-time tie up between Tasks and Goals, generally outcome of a means-end analysis. In this vision the system is up to decide how to combine its available Capabilities: the Proactive Means-End Reasoning. The impact of this research line is to implement a goal-oriented form of self-adaptation where goal models can be injected at runtime. The paper also introduces MUSA, a Middleware for User-driven Service self-Adaptation. © 2015 IEEE.
SCOPUS;2015;An Application Conflict Detection and Resolution System for Smart Homes;conflict resolution,, model checking,, Requirements at runtime,, Smart Home;One of the applications of Cyber-Physical Systems (CPSs) is the Smart Homes. In Smart Homes, multiple apps operate the sensors and actuators to provide rich user experience in a living environment. Because actuators are entities that affect the surrounding environment, conflicts may occur if two or more apps are running simultaneously, especially when they try to use a single actuator or when they use different actuators causing different effects. There have been attempts to resolve these conflicts at app installation time. However the state-of-the-art solutions can detect conflicts only if the apps actuate on devices with conditions based on time, and resolute conflicts by creating a total order between all apps, regardless of the situations of the conflicts. In this paper, we create a Kripke structure to detect conflicts by model-checking the assertion 'no two appsuse actuators to create different effects at the same location'. Our proposed system, which provides install-time conflict detection, enables detection of application conflicts triggered by conditions based on events. In addition, it supports users in prioritising apps by reducing the number of conflicts by dividing them into groups of the same situation which are meaningful to the users. By prioritising apps for each situation, rather than creating a total order, our system allows the apps to run in a more flexible way. © 2015 IEEE.
SCOPUS;2015;Comparative assessment of methods for the computational inference of transcript isoform abundance from RNA-seq data;;Background: Understanding the regulation of gene expression, including transcription start site usage, alternative splicing, and polyadenylation, requires accurate quantification of expression levels down to the level of individual transcript isoforms. To comparatively evaluate the accuracy of the many methods that have been proposed for estimating transcript isoform abundance from RNA sequencing data, we have used both synthetic data as well as an independent experimental method for quantifying the abundance of transcript ends at the genome-wide level. Results: We found that many tools have good accuracy and yield better estimates of gene-level expression compared to commonly used count-based approaches, but they vary widely in memory and runtime requirements. Nucleotide composition and intron/exon structure have comparatively little influence on the accuracy of expression estimates, which correlates most strongly with transcript/gene expression levels. To facilitate the reproduction and further extension of our study, we provide datasets, source code, and an online analysis tool on a companion website, where developers can upload expression estimates obtained with their own tool to compare them to those inferred by the methods assessed here. Conclusions: As many methods for quantifying isoform abundance with comparable accuracy are available, a user's choice will likely be determined by factors such as the memory and runtime requirements, as well as the availability of methods for downstream analyses. Sequencing-based methods to quantify the abundance of specific transcript regions could complement validation schemes based on synthetic data and quantitative PCR in future or ongoing assessments of RNA-seq analysis methods. © 2015 Kanitz et al.
SCOPUS;2015;Rationalism with a dose of empiricism: Combining goal reasoning and case-based reasoning for self-adaptive software systems;Case-based reasoning,, Goal reasoning,, Requirements goal models,, Self-adaptive systems;Requirements-driven approaches provide an effective mechanism for self-adaptive systems by reasoning over their runtime requirements models to make adaptation decisions. However, such approaches usually assume that the relations among alternative system configurations, environmental parameters and requirements are clearly understood, which is often not true. Moreover, they do not consider the influence of the current configuration of an executing system on adaptation decisions. In this paper, we propose an improved requirements-driven self-adaptation approach that combines goal reasoning and case-based reasoning. In the approach, past experiences of successful adaptations are retained as adaptation cases, which are described by not only requirements violations and contexts, but also currently deployed system configurations. The approach does not depend on a set of original adaptation cases, but employs goal reasoning to provide adaptation solutions when no similar cases are available. Case-based reasoning is used to provide more precise adaptation decisions that better reflect the complex relations among requirements violations, contexts, and current system configurations by utilizing past experiences. To prevent case-based reasoning from getting trapped in suboptimal adaptation solutions, an additional case mutation mechanism is introduced to mutate existing adaptation solutions when necessary. We conduct an experimental study with an online shopping benchmark to evaluate the effectiveness of our approach. The results show that our approach outperforms both a requirements-driven approach and a case-based approach in terms of satisfaction level of quality constraints. The results also confirm the effectiveness of case mutation for producing better adaptation solutions. In addition, we empirically investigate the evolution process of adaptation solutions. The evolution analysis reveals some general evolution trends of adaptation solutions such as different evolution phases. © Springer-Verlag London 2015.
SCOPUS;2015;Iterative solutions to the steady-state density matrix for optomechanical systems;;We present a sparse matrix permutation from graph theory that gives stable incomplete lower-upper preconditioners necessary for iterative solutions to the steady-state density matrix for quantum optomechanical systems. This reordering is efficient, adding little overhead to the computation, and results in a marked reduction in both memory and runtime requirements compared to other solution methods, with performance gains increasing with system size. Either of these benchmarks can be tuned via the preconditioner accuracy and solution tolerance. This reordering optimizes the condition number of the approximate inverse and is the only method found to be stable at large Hilbert space dimensions. This allows for steady-state solutions to otherwise intractable quantum optomechanical systems. © 2015 American Physical Society.
SCOPUS;2013;Requirement Verification and Dependency Tracing during Simulation in Modelica;fault tolerance,, model driven systems engineering,, requirements,, run-time verification,, traceability;Requirement verification is an important part of the development process, and the increasing system complexity has exacerbated the need for integrating this step into a formalized model driven development process, providing a dedicated methodology as well as tool support. In this paper the authors propose an extension for Modelica, an equation-based language for system modeling, that will allow to represent system requirements in the same formalism as the design model, thus reducing the need for transformations between different specialized formalisms, lowering maintenance and modification costs, and benefitting from the expression and simulation capabilities, as well as extensive tool support of Modelica. The object-oriented nature of the approach provides the advantages of modular design and hierarchical structuring of the requirement model. This paper also illustrates, with the help of an example, how requirement verification can be used alongside the simulation process to trace the components responsible for requirement violations. To this end, we introduce a formalism for expressing relationships between components and requirements, as well as a tracing algorithm. © 2013 IEEE.
SCOPUS;2015;Model-based generation of a requirements monitor;Requirements at runtime,, Requirements monitor,, Self-adaptive systems;Runtime representations of requirements have recently gained interested to deal with uncertainty in the environment and the term requirements at runtime has been established. Runtime representations of requirements support reasoning about the requirements at runtime and adapting the configuration of a system according to changes in the environment. Such systems often called self-adaptive systems. Core part of respective approaches in the field is a requirements monitor. That is, an instance which is able to observe the system's environment and to decide whether a requirement is broken, based on assertions. The problem addressed in this paper is how to generate the applicationspecific parts of a requirements monitor. Such a monitor consists of some goal model for decisions at runtime, assertions connected to the goal model, and parameters on which assertions are defined. We present in this paper a model-driven approach to enhance requirements documents by goal models, assertions, and parameters in a way which is (1) understandable to requirements engineers and (2) a sufficient basis for generating the requirements monitor. The contribution is an integrated view on requirements for self-adaptive systems and a concept for code generation. © 2015 by the authors.
SCOPUS;2015;Context uncertainty in requirements engineering: Definition of a search strategy for a systematic review and preliminary results;Context,, Cyber-Physical systems,, Requirements engineering,, Search strategy,, Self-Adaptive systems,, Systematic literature search,, Systematic review,, Uncertainty;[Context and motivation] Cyber-physical systems (CPS) and selfadaptive systems (SAS) strongly rely on the context they are operating in and need to adapt their behavior at run-time based on contextual information. Therefore, it is challenging to completely predict the context of such systems for their entire operating time already at design time. [Question/problem] Since several approaches dealing with uncertainty have been proposed for different research and problem domains in recent years, some might provide valuable insights for the engineering of CPS or SAS in uncertain contexts. However, there is no study so far that provides an overview of them. [Principle ideas/results] Thus, we aim at conducting a systematic literature analysis to create a research landscape of approaches coping with context uncertainty. [Contribution] We manually searched one journal and the proceedings of two conferences in the requirements engineering field to determine and evaluate the adequateness of search strings to be used in an automated search. In doing so, we can furthermore present preliminary findings from the manual search for uncertainty in the requirements engineering field. © 2015 by the authors.
SCOPUS;2015;A lightweight framework for dynamic GUI data verification based on scripts;Aspect-oriented,, Data verification,, Graphical user interfaces,, Requirements engineering,, Runtime verification,, Scripting languages;Summary: Runtime verification (RV) provides essential mechanisms to enhance software robustness and prevent malfunction. However, RV often entails complex and formal processes that could be avoided in scenarios in which only invariants or simple safety properties are verified, for example, when verifying input data in Graphical User Interfaces (GUIs). This paper describes S-DAVER, a lightweight framework aimed at supporting separate data verification in GUIs. All the verification processes are encapsulated in an independent layer and then transparently integrated into an application. The verification rules are specified in separate files and written in interpreted languages to be changed/reloaded at runtime without recompilation. Superimposed visual feedback is used to assist developers during the testing stage and to improve the experience of users during execution. S-DAVER provides a lightweight, easy-to-integrate and dynamic verification framework for GUI data. It is an integral part of the development, testing and execution stages. An implementation of S-DAVER was successfully integrated into existing open-source applications, with promising results. © 2015John Wiley & Sons, Ltd.
SCOPUS;2015;Modeling and verification of Functional and Non-Functional Requirements of ambient Self-Adaptive Systems;Dynamic Adaptive Systems,, Goal Oriented Requirements Engineering,, Model Driven Engineering,, Non Functional Requirements,, Properties verification,, Relax;Self-Adaptive Systems modify their behavior at run-time in response to changing environmental conditions. For these systems, Non-Functional Requirements play an important role, and one has to identify as early as possible the requirements that are adaptable. We propose an integrated approach for modeling and verifying the requirements of Self-Adaptive Systems using Model Driven Engineering techniques. For this, we use Relax, which is a Requirements Engineering language which introduces flexibility in Non-Functional Requirements. We then use the concepts of Goal-Oriented Requirements Engineering for eliciting and modeling the requirements of Self-Adaptive Systems. For properties verification, we use OMEGA2/IFx profile and toolset. We illustrate our proposed approach by applying it on an academic case study. © 2015 Elsevier Inc. All rights reserved.
SCOPUS;2015;MiDAS: A model-driven approach for adaptive software;Adaptation,, Design,, MDE,, Requirements,, Transformations;Some of the main problems in software engineering for adaptive software are: the lack of mechanisms to specify adaptive characteristics in software requirements,, the difficulty to obtain a functional adaptive system based on the elicited requirements,, and the need of maintaining synchronization and traceability between the requirements, design and implementation. To address the above problems, this paper proposes MiDAS, a framework that uses a model-driven approach to develop adaptive software. Specifically, MiDAS provides: (i) a new language for requirements engineering process that takes into account uncertainty in adaptive software,, (ii) a method to derive concrete implementations in specific architectures supporting run-time adaptation,, and, (iii) a mechanism to maintain traceability and synchronization between requirements specifications, design models and implementation architectures.
SCOPUS;2015;Designing an adaptive computer-aided ambulance dispatch system with Zanshin: An experience report;adaptive systems,, case studies,, feedback loops,, requirements awareness,, software evolution,, software reconfiguration,, software requirements,, system identification;Summary We have been witnessing growing interest in systems that can adapt their behavior to deal with deviations between their performance and their requirements at run-time. Such adaptive systems usually need to support some form of a feedback loop that monitors the system's output for problems and carries out adaptation actions when necessary. Being an important feature, adaptivity needs to be considered in early stages of development. Therefore, adopting a requirements engineering perspective, we have proposed an approach and a framework (both called Zanshin) for the engineering of adaptive systems based on a feedback loop architecture. As part of our framework's evaluation, we have applied the Zanshin approach to the design of an adaptive computer-aided ambulance dispatch system, whose requirements were based on a well-known case study from the literature. In this paper, we report on the application of Zanshin for the design of an adaptive computer-aided ambulance dispatch system, presenting elements of the design, as well as the results from simulations of run-time scenarios. Copyright © 2013 John Wiley & Sons, Ltd. Copyright © 2013 John Wiley & Sons, Ltd.
SCOPUS;2015;Designing adaptive systems;Architecture-driven software adaptation,, Goal-oriented requirements models,, Model-driven development,, Requirements-driven software adaptation;In this work, we investigate the interplay between requirements and architecture in the context of adaptive systems. Furthermore, we propose the Multi-Level Adaptation for Software Systems (MULAS) framework. It is centred on the iterative and incremental refinement of a goal model, towards the creation of a design goal model, which can be used at runtime to drive adaptation on a system that is properly instrumented. Moreover, the framework includes a toolsupported process for generating statechart behavioural models from a design goal model. Copyright © 2015 for this paper by its authors.
SCOPUS;2015;Pragmatic requirements for adaptive systems: A goal-driven modeling and analysis approach;Context dependency,, Goal-models,, Requirements engineering;Goal-models (GM) have been used in adaptive systems engineering for their ability to capture the different ways to fulfill the requirements. Contextual GM (CGM) extend these models with the notion of context and context-dependent applicability of goals. In this paper, we observe that the interpretation of a goal achievement is itself contextdependent. Thus, we introduce the notion of Pragmatic Goals which have a dynamic satisfaction criteria. However, the specification of contextdependent goals’ applicability as well as their interpretations make it hard for stakeholders to decide whether the model is achievable for all possible context combinations. Thus we also developed and evaluated an algorithm to decide on the Pragmatic CGM’s achievability.We performed several experiments to evaluate our algorithm regarding correctness and performance and concluded that it can be used for deciding at runtime the tasks to execute under a given context to achieve a quality constraint as well as for pinpointing context sets in which the model is intrinsically unachievable. © Springer International Publishing Switzerland 2015.
SCOPUS;2015;27th International Conference on Advanced Information Systems Engineering, CAiSE 2015;;The proceedings contain 31 papers. The special focus in this conference is on Collaborative Computing, Business Process Modeling, Languages, Complex Information Management, Requirements Elicitation and Enterprise Data Management. The topics include: An approach to separation of concerns in crowdsourced data management,, run-time and task-based performance of event detection techniques for twitter,, a graphical notation for resource assignments in business processes,, revising the vocabulary of business process element labels,, the requirements and needs of global data usage in product lifecycle management,, a clustering approach for protecting GIS vector data,, need-to-share and non-diffusion requirements verification in exchange policies,, leveraging familiar environments to prime stakeholder memory during process elicitation,, handling regulatory goal model families as software product lines,, managing data warehouse traceability,, specification and incremental maintenance of linked data mashup views,, a model-driven approach to enterprise data migration,, detecting complex changes during metamodel evolution,, completing workflow traces using action languages,, extracting decision logic from process models,, equivalence transformations for the design of interorganizational data-flow,, automatic generation of optimized process models from declarative specifications,, towards the automated annotation of process models,, discovery and validation of queueing networks in scheduled processes,, verification and validation of UML artifact-centric business process models,, empirical challenges in the implementation of IT portfolio management,, deriving artefact-centric interfaces for overloaded web services,, fundamental systems thinking concepts for is engineering and a practical tutorial on the framework for evaluation in design science research.
SCOPUS;2015;An Empirical Analysis of Providing Assurance for Self-Adaptive Systems at Different Levels of Abstraction in the Face of Uncertainty;assurance,, search-based software engineering,, self-adaptive systems;Self-adaptive systems (SAS) must frequently continue to deliver acceptable behavior at run time even in the face of uncertainty. Particularly, SAS applications can self-reconfigure in response to changing or unexpected environmental conditions and must therefore ensure that the system performs as expected. Assurance can be addressed at both design time and run time, where environmental uncertainty poses research challenges for both settings. This paper presents empirical results from a case study in which search-based software engineering techniques have been systematically applied at different levels of abstraction, including requirements analysis, code implementation, and run-time validation, to a remote data mirroring application that must efficiently diffuse data while experiencing adverse operating conditions. Experimental results suggest that our techniques perform better in terms of providing assurance than alternative software engineering techniques at each level of abstraction. © 2015 IEEE.
SCOPUS;2015;Challenges in rendering and maintaining trustworthiness for long-living software systems;;Trustworthiness plays a key role in acceptance and adoption of software by the end-users. When maintaining long-living software systems, trustworthiness has to be addressed since trust of the end-user is volatile and can change over time. In this paper, we discuss the challenges regarding trustworthiness of long-living software systems. Trustworthiness should be considered in the whole life-cycle of a long-living system, i.e., in all development phases aiming at building trustworthiness into the core of the system at design-time and later maintaining it during run-time. But, our focus in this paper is on challenges in requirements engineering and also planning for the run-time activities, e.g., what are the needed monitor interfaces, what are the planned actions and how are the execution interfaces for performing those actions.
SCOPUS;2015;Towards an ontology-based approach to safety management in cooperative intelligent transportation systems;Cooperative Intelligent Transportation System,, Knowledge,, Ontology,, Safety,, Safety loop,, Vehicle;The expected increase in transports of people and goods across Europe will aggravate the problems related to traffic congestion, accidents and pollution. As new road infrastructure alone would not solve such problems, Intelligent Transportation Systems (ITS) has been considered as new initiatives. Due to the complexity of behaviors, novel methods and tools for the requirements engineering, correct-by-construction design, dependability, product variability and lifecycle management become also necessary. This chapter presents an ontology-based approach to safety management in Cooperative ITS (C-ITS), primarily in an automotive context. This approach is supposed to lay the way for all aspects of ITS safety management, from simulation and design, over run-time risk assessment and diagnostics. It provides the support for ontology driven ITS development and its formal information model. Results of approach validation in CarMaker are also given in this Chapter. The approach is a result of research activities made in the framework of Swedish research initiative, referred to as SARMITS (Systematic Approach to Risk Management in ITS Context). © Springer International Publishing Switzerland 2015.
SCOPUS;2015;Instruction-cache locking for improving embedded systems performance;Binary rewriting,, Cache locking,, Caches,, Embedded systems;Cache memories in embedded systems play an important role in reducing the execution time of applications. Various kinds of extensions have been added to cache hardware to enable software involvement in replacement decisions, improving the runtime over a purely hardware-managed cache. Novel embedded systems, such as Intel's XScale and ARM Cortex processors, facilitate locking one or more lines in cache,, this feature is called cache locking. We present a method in for instruction-cache locking that is able to reduce the average-case runtime of a program. We demonstrate that the optimal solution for instruction cache locking can be obtained in polynomial time. However, a fundamental lack of correlation between cache hardware and software program points renders such optimal solutions impractical. Instead, we propose two practical heuristics-based approaches to achieve cache locking. First, we present a static mechanism for locking the cache, in which the locked contents of the cache are kept fixed over the execution of the program. Next, we present a dynamic mechanism that accounts for changing program requirements at runtime. We devise a cost-benefit model to discover the memory addresses that should be locked in the cache. We implement our scheme inside a binary rewriter, widening the applicability of our scheme to binaries compiled using any compiler. Results obtained on a suite of MiBench benchmarks show that our static mechanism results in 20% improvement in the instruction-cache miss rate on average and up to 18% improvement in the execution time on average for applications having instruction accesses as a bottleneck, compared to no cache locking. The dynamic mechanism improves the cache miss rate by 35% on average and execution time by 32% on instruction-cache-constrained applications. © 2015 ACM.
SCOPUS;2015;The e-mobility case study;Autonomic systems,, Ensemble-oriented systems,, Requirements analysis,, Scheduling,, Self-organization,, Software engineering methodologies;Electro-mobility (e-mobility) is one of the promising technologies being considered by automotive OEMs as an alternative to internal combustion engines as a means of propulsion. The e-mobility case study provides a novel example of a relevant industry application with in the ASCENS framework. An overview of the system design is given which describes how e-mobility is conceptualized and then transformed using the ensemble development life cycle (EDLC) approach into a distributed autonomic (i.e self-aware, self-adaptive) component-based software system.The system requirements engineering is based on the state-of-the affairs (SOTA) approach and the invariant refinement method (IRM)which are both revisited and applied. Regarding the implementation and deployment of the system, a dependable emergent ensembles of components(DEECo) approach is utilized. The DEECo components and ensembles are coded and deployed using the Java-based jDEECo runtime environment. The runtime environment integrates the multi-agent transport simulation tool (MATSim), which is used to predict the effects of the physical interactions of users, vehicles and infrastructure resources.jDEECo handles multiple MATSim instances to allow for different belief states between components and ensembles. © Springer International Publishing Switzerland2015.
SCOPUS;2015;Cyber-physical systems design for runtime trustworthiness maintenance supported by tools;Adaptation,, Mitigation,, Runtime monitoring,, Threats,, Trustworthiness requirements,, Trustworthiness-by-Design;The trustworthiness of cyber-physical systems is a critical factor for establishing wide-spread adoption of these systems. Hence, especially the behavior of safety-critical software components needs to be monitored and managed during system operation. Runtime trustworthiness maintenance should be planned and prepared in early requirements and design phases. This involves the identification of threats that may occur and affect user's trust at runtime, as well as related controls that can be executed to mitigate the threats. Furthermore, observable and measureable system quality properties have to be identified as indicators of threats, and interfaces for reporting these properties as well as for executing controls have to be designed and implemented. This paper presents a process model for preparing and designing systems for runtime trustworthiness maintenance, which is supported by several tools that facilitate the tasks to be performed by requirements engineers and system designers. Copyright © 2015 by the authors.
SCOPUS;2015;A tool for monitoring and maintaining system trustworthiness at runtime;Adaptation,, Mitigation,, Monitoring,, Runtime,, Trustworthiness;Trustworthiness of software systems is a key factor in their acceptance and effectiveness. This is especially the case for cyber-physical systems, where incorrect or even sub-optimal functioning of the system may have detrimental effects. In addition to designing systems with trustworthiness in mind, monitoring and maintaining trustworthiness at runtime is critical to identify issues that could negatively affect a system's trustworthiness. In this paper, we present a fully operational tool for system trustworthiness maintenance, covering a comprehensive set of quality attributes. It automatically detects, and in some cases mitigates, trustworthiness threatening events. The use of such a tool can enable complex software systems to support runtime adaptation and self-healing, thus reducing the overall upkeep cost and complexity. © 2015 by the authors.
SCOPUS;2015;Requirements-driven self-optimization of composite services using feedback control;earned business value,, process reconfiguration,, QoS,, quality tradeoffs,, self-optimization,, service selection;In an uncertain and changing environment, a composite service needs to continuously optimize its business process and service selection through runtime adaptation. To achieve the overall satisfaction of stakeholder requirements, quality tradeoffs are needed to adapt the composite service in response to the changing environments. Existing approaches on service selection and composition, however, are mostly based on quality preferences and business processes decisions made statically at the design time. In this paper, we propose a requirements-driven self-optimization approach for composite services. It measures the quality of services (QoS), estimates the earned business value, and tunes the preference ranks through a feedback loop. The detection of unexpected earned business value triggers the proposed self-optimization process systematically. At the process level, a preference-based reasoner configures a requirements goal model according to the tuned preference ranks of QoS requirements, reconfiguring the business process according to its mappings from the goal configurations. At the service level, selection decisions are optimized by utilizing the tuned weights of QoS criteria. We used an experimental study to evaluate the proposed approach. Results indicate that the new approach outperforms both fixed-weighted and floating-weighted service selection approaches with respect to earned business value and adaptation flexibility. © 2008-2012 IEEE.
SCOPUS;2015;Modelling and facilitating user-generated feedback for enterprise information systems evaluation;Enterprise Information Systems Evaluation,, Feedback Analysis,, Requirements Models,, User Involvement,, Users' Feedback;Most enterprises operate within a complex and ever-changing context. Users understand the software as a means to meet their requirements and needs, thus, giving them a voice in the continuous runtime evaluation of software would naturally fit this level of abstraction to ensure that requirements keep pace with changing context. However, this evaluation knowledge is often provided in an ad-hoc manner, which endures a great deal of impression and ambiguity leading to another problem, which is how engineers can extract meaningful and useful information from such feedback to inform their maintenance and evolution decisions. This doctoral work is novel in providing classifications of users' feedback constituents and how they could be structured, which can be employed for a formal feedback acquisition method. Also, capturing structured feedback using systematic means can aid engineers in obtaining useful knowledge for evaluating enterprise information systems in order to maintain and evolve their requirements.
SCOPUS;2015;Adaptive software-based Feedback Acquisition: A Persona-based design;Persona,, Requirement engineering,, User Feedback;Users' feedback is vital to improve software quality and it provides developers with a rich knowledge on how software meets users' requirements in practice. Feedback informs how software should adapt, or be adapted, at runtime and what evolutionary actions to take in the next release. However, studies have noted that accommodating the different preferences of users on how feedback should be requested is a complex task and requires a careful engineering process. This calls for an adaptive feedback acquisition mechanisms to cater for such variability. In this paper, we tackle this problem by employing the concept of Persona to aid software engineers understand the various users' behaviours and improve their ability to design feedback acquisition techniques more efficiently. We create a set of personas based on a mixture of qualitative and quantitative studies and propose PAFA, a Persona-based method for Adaptive Feedback Acquisition. © 2015 IEEE.
SCOPUS;2015;A programming-level approach for elasticizing parallel scientific applications;Cloud computing,, Elasticity,, Parallel applications;Elasticity is considered one of the fundamental properties of cloud computing. Several mechanisms to provide the feature are offered by public cloud providers and in some academic works. We argue these solutions are inefficient in providing elasticity for scientific applications, since they cannot consider the internal structure and behavior of applications. In this paper we present an approach for exploring the elasticity in scientific applications, in which the elasticity control is embedded in application source code and constructed using elasticity primitives. This approach enables the application itself to request or to release its own resources, taking into account the execution flow and runtime requirements. To support the construction of elastic applications using the presented approach, we developed the Cloudine framework. Cloudine provides all components necessary to construct and execute elastic scientific applications. The Cloudine effectiveness is demonstrated in the experiments where the platform is successfully used to include new features to existing applications, to extend functionalities of other elasticity frameworks and to add elasticity support to parallel programming libraries. © 2015 Elsevier Inc. All rights reserved.
SCOPUS;2015;Evaluating heuristic optimization, bio-inspired and graph-theoretic algorithms for the generation of fault-tolerant graphs with minimal costs;Bio-inspired algorithm,, Fault-tolerant graphs,, Graph optimization;The construction of fault-tolerant graphs is a trade-off between costs and degree of fault-tolerance. Thus the construction of such graphs can be viewed as a two-criteria optimization problem. Any algorithm therefore should be able to generate a Pareto-front of graphs so that the right graph can be chosen to match the application and the user’s need. In this work algorithms from three different domains for the construction of fault-tolerant graphs are evaluated. Classical graph-theoretic algorithms, optimization and bio-inspired approaches are compared regarding the quality of the generated graphs as well as concerning the runtime requirements. As result, recommendations for the application of the right algorithm for a certain problem class can be concluded. © Springer-Verlag Berlin Heidelberg 2015.
SCOPUS;2014;Swarm intelligence for dimensionality reduction: How to improve the non-negative matrix factorization with nature- inspired optimization methods;;Low-rank approximations allow for compact representations of data with reduced storage and runtime requirements and reduced redundancy and noise. The Non-Negative Matrix Factorization (NMF) is a special low-rank approximation that allows for additive parts-based, interpretable representation of the data. Various properties of NMF are similar to Swarm Intelligence (SI) methods: indeed, most NMF objective functions and most SI fitness functions are non-convex, discontinuous, and may possess many local minima. This chapter summarizes efforts on improving convergence, approximation quality, and classification accuracy of NMF using five different meta-heuristics based on SI and evolutionary computation. The authors present (1) new initialization strategies for NMF, and (2) an iterative update strategy for NMF. The applicability of the approach is illustrated on data sets coming from the areas of spam filtering and email classification. Experimental results show that both optimization strategies are able to improve NMF in terms of faster convergence, lower approximation error, and/or better classification accuracy. © 2015 by IGI Global. All rights reserved.
SCOPUS;2014;Run-time verification of MSMAS norms using event calculus;;Modelling Self-managing Multi Agent Systems (MSMAS) is a software development methodology that facilitates designing and developing complex distributed systems based on the multivalent systems paradigm. MSMAS uses a declarative modelling style to capture system requirements by specifying four types of what we call system norms over: the system goals, the system roles, the business activities, and communications. MSMAS utilises system norms to capture system requirements in a formal language which can subsequently be monitored and verified at runtime. In this paper we present the main elements of MSMAS and introduce MSMAS defined norm types. We model the life cycle of MSMAS norms as non-atomic activities and formally express them as Event Calculus (EC) theories. Our acclimatisation of MSMAS system norms as first-order EC allows for reasoning with a metric time representation which we illustrate through a monitoring example of two execution traces to verify the system compliance with its intended design requirements and show how to detect any violation of norms. © 2014 IEEE.
SCOPUS;2014;A self-tuning scientific framework using model-driven engineering for heterogeneous execution platforms;Coprocessors,, Load-balancing,, Model-driven engineering,, Scheduling,, Self-adaptation;This article presents an ongoing work towards the extension of Sm@rtConfig - A dynamic scheduling tool with self-tuning load-balancing functionalities targeting CPUs, GPUs, and other co-processors. This extension is based on the introduction of a high-level modeling phase for scientific applications. These applications are commonly complex, use (heterogeneous) high performance execution platforms, and require stakeholders of several disciplines. This way, it is important to raise abstraction level in earlier stages of development in order to deal with such complexities in an efficient way. By using Model-Driven Engineering, we propose an approach to transform Sm@rtConfig into a scientific framework comprising requirements engineering up to code generation for the target Processing Unit in which a task is scheduled at runtime. We advocate that our envisioned methodology facilitates not just cross-stakeholders development, but also replication of experimentations by the research community.
SCOPUS;2014;Uncertainty handling in goal-driven self-optimization - Limiting the negative effect on adaptation;Goal-driven self-optimization,, Requirements goal models,, Uncertainty;Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling. © 2014 Elsevier Inc.
SCOPUS;2014;Phase distance mapping: a phase-based cache tuning methodology for embedded systems;Cache tuning,, Configurable architectures,, Configurable hardware,, Dynamic reconfiguration,, Energy delay product,, Phase-based tuning;Networked embedded systems typically leverage a collection of low-power embedded systems (nodes) to collaboratively execute applications spanning diverse application domains (e.g., video, image processing, communication, etc.) with diverse application requirements. The individual networked nodes must operate under stringent constraints (e.g., energy, memory, etc.) and should be specialized to meet varying applications’ requirements in order to adhere to these constraints. Phase-based tuning specializes a system’s tunable parameters to the varying runtime requirements of an application’s different phases of execution to meet optimization goals. Since the design space for tunable systems can be very large, one of the major challenges in phase-based tuning is determining the best configuration for each phase without incurring significant tuning overhead (e.g., energy and/or performance) during design space exploration. In this paper, we propose phase distance mapping, which directly determines the best configuration for a phase, thereby eliminating design space exploration. Phase distance mapping applies the correlation between a known phase’s characteristics and best configuration to determine a new phase’s best configuration based on the new phase’s characteristics. Experimental results verify that our phase distance mapping approach, when applied to cache tuning, determines cache configurations within 1 % of the optimal configurations on average and yields an energy delay product savings of 27 % on average. © 2014, Springer Science+Business Media New York.
SCOPUS;2014;Feedback-aware requirements documents for smart devices;Domain-specific Language,, Embedded System,, mbeddr,, Requirements at Runtime,, Self-Adaptivity,, Smart Device;[Context/ Motivation] A smart device is a software-intensive system that operates autonomously and interacts to some degree with other systems over wireless connections. Such systems are often faced with uncertainty in the environment. Runtime representations of requirements have recently gained more interested to deal with this challenge and the term requirements at runtime has been established. Runtime representations of requirements support reasoning about the requirements at runtime and adapting the configuration of a system according to changes in the environment. [Questions/Problems] The research question is how the results of runtime monitoring of requirements and the system's decisions about changes in the configuration are communicated back to the requirements engineer to better understand the environment. There is a gap between the written requirements document and the dynamic requirements model inside the system. This problem is exacerbated by the fact that a requirements document are mostly informal while the dynamic requirements model is formal. [Principal ideas/results] This paper introduces an approach to bridge the gap between development time and runtime representations of requirements in order to keep them consistent and to facilitate better understanding. We propose to weave the feedback from the runtime system into requirements documents using a domain-specific language that largely retain the informal nature of requirements. An annotated requirements document helps get a better understanding of the system's actual behavior in a given environment. The approach is implemented using mbeddr, a novel set of domain-specific languages for developing embedded systems, and illustrated using a running example. © 2014 Springer International Publishing Switzerland.
SCOPUS;2014;Achieving runtime adaptability through automated model evolution and variant selection;dynamic adaptability,, model at runtime,, model evolution,, quality requirements;Dynamically adaptive systems propose adaptation by means of variants that are specified in the system model at design time and allow for a fixed set of different runtime configurations. However, in a dynamic environment, unanticipated changes may result in the inability of the system to meet its quality requirements. To allow the system to react to these changes, this article proposes a solution for automatically evolving the system model by integrating new variants and periodically validating the existing ones based on updated quality parameters. To illustrate this approach, the article presents a BPEL-based framework using a service composition model to represent the functional requirements of the system. The framework estimates quality of service (QoS) values based on information provided by a monitoring mechanism, ensuring that changes in QoS are reflected in the system model. The article shows how the evolved model can be used at runtime to increase the system's autonomic capabilities and delivered QoS. © 2014 Copyright Taylor and Francis Group, LLC.
SCOPUS;2014;Semiautomatic security requirements engineering and evolution using decision documentation, heuristics, and user monitoring;decision documentation,, decision knowledge,, heuristic analysis,, knowledge carrying software,, Security requirements engineering,, software evolution,, user mon-itoring;Security issues can have a significant negative impact on the business or reputation of an organization. In most cases they are not identified in requirements and are not continuously monitored during software evolution. Therefore, the inability of a system to conform to regulations or its endangerment by new vulnerabilities is not recognized. In consequence, decisions related to security might not be taken at all or become obsolete quickly. But to evaluate efficiently whether an issue is already addressed appropriately, software engineers need explicit decision documentation. Often, such documentation is not performed due to high overhead. To cope with this problem, we propose to document decisions made to address security requirements. To lower the manual effort, information from heuristic analysis and end user monitoring is incorporated. The heuristic assessment method is used to identify security issues in given requirements au-tomatically. This helps to uncover security decisions needed to mitigate those issues. We describe how the corresponding security knowledge for each issue can be incorporated into the decision documentation semiautomatically. In addition, violations of security requirements at runtime are monitored. We show how decisions related to those security requirements can be identified through the documentation and updated manually. Overall, our approach improves the quality and completeness of security decision documentation to support the engineering and evolution of security requirements. © 2014 IEEE.
SCOPUS;2014;Self-explanation in adaptive systems based on runtime goal-based models;Claims,, Goals,, Self-adaptive,, Self-explanation;The behaviour of self adaptive systems can be emergent, which means that the system’s behaviour may be seen as unexpected by its customers and its developers. Therefore, a self-adaptive system needs to garner confidence in its customers and it also needs to resolve any surprise on the part of the developer during testing and maintenance. We believe that these two functions can only be achieved if a self-adaptive system is also capable of self-explanation. We argue a self-adaptive system’s behaviour needs to be explained in terms of satisfaction of its requirements. Since self-adaptive system requirements may themselves be emergent, we propose the use of goal-based requirements models at runtime to offer self-explanation of how a system is meeting its requirements. We demonstrate the analysis of run-time requirements models to yield a self-explanation codified in a domain specific language, and discuss possible future work. © Springer-Verlag Berlin Heidelberg 2014.
SCOPUS;2014;The observer-based technique for requirements validation in embedded real-time systems;model-based requirements validation,, observer technique,, run-time monitoring,, systems functional behaviors and non-functional properties,, TASM;Model-based requirements validation is an increasingly attractive approach to discovering hidden flaws in requirements in the early phases of systems development life cycle. The application of using traditional methods such as model checking for the validation purpose is limited by the growing complexity of embedded real-time systems (ERTS). The observer-based technique is a lightweight validation technique, which has shown its potential as a means of validating the correctness of model behaviors. In this paper, the novelty of our contributions is three-fold: 1) we formally define the observer constructs for our formal specification language namely the Timed Abstract State Machine (TASM) language and, 2) we propose the Events Monitoring Logic (EvML) to facilitate the observer specification and, 3) we show how to execute observers to validate the requirements describing the functional behaviors and non-functional properties (such as timing) of ERTS. We also illustrate the applicability of the extended TASM language through an industrial application of a Vehicle Locking-Unlocking system. © 2014 IEEE.
SCOPUS;2014;Study on high-performance simulation computer for large-scale system of systems simulation;High-performance parallel computing,, High-performance simulation computer,, Simulation framework,, System of systems;The high-performance parallel computing (HPPC) has a better overall performance and higher productivity, for a generical large-scale army equipment system of systems (AESoS) simulation, and the runtime efficiency can be multiplied several tenfold to several hundredfold. The requirement analysis of simulation framework of AESoS based on HPPC was proposed. After the simulation framework of AESoS based on HPPC and its key techniques were discussed, the simulation framework of AESoS Based HPPC was designed. it is of great significance to offer certain references for the engineering application in the simulation fields of AESoS based on HPPC. © (2014) Trans Tech Publications, Switzerland.
SCOPUS;2014;Study on simulation framework of army equipment system of systems based on high-performance parallel computing;Army equipment system of systems,, High-performance parallel computing,, Simulation components,, Simulation framework;The high-performance parallel computing (HPPC) has a better overall performance and higher productivity, for a generical large-scale army equipment system of systems (AESoS) simulation, and the runtime efficiency can be multiplied several tenfold to several hundredfold. The requirement analysis of simulation framework of AESoS based on HPPC was proposed. After the simulation framework of AESoS based on HPPC and its key techniques were discussed, the simulation framework of AESoS Based HPPC was designed. it is of great significance to offer certain references for the engineering application in the simulation fields of AESoS based on HPPC. © (2014) Trans Tech Publications, Switzerland.
SCOPUS;2014;Implementation decision making for internetware driven by quality requirements;decision making,, design alternatives,, implementation decision,, internetware,, quality requirements;Internetware is an emerging software paradigm in the open, dynamic and ever-changing Internet environment. A successful internetware must demonstrate acceptable degree of quality when carrying out its functionality. Hence, when internetware is being dynamically constructed, making implementation decisions to satisfice the quality requirements becomes a critical issue. In the traditional software engineering, quality requirements are usually refined stepwise by sub-requirements utilizing goal modeling perspective, until some potential functional design alternatives are identified. The goal-oriented paradigms have adopted graphical goal models to reason about quality requirements and proposed qualitative or quantitative reasoning schemas. However, these techniques may become unviable due to the ever-changing operating environment and demands for run-time decision making. In this paper, we propose an approach for implementation decision making driven by quality requirements for internetware. It focuses on the symbolic formula representation of requirements goal models with the tree structure, which is of well-defined syntax and clear traceability. Furthermore, we explore some reasoning rules which effectively automate each reasoning action on the formulae. This supports multiple-factor decision making. A case study is also provided to illustrate our proposed approach. We have developed a supporting tool based on our theoretical approach that we also present in this paper. © 2014 Science China Press and Springer-Verlag Berlin Heidelberg.
SCOPUS;2014;Adaptive user-centered security;Identity management,, Privacy,, Resilience,, Security,, Usability;One future challenge in informatics is the integration of humans in an infrastructure of data-centric IT services. A critical activity of this infrastructure is trustworthy information exchange to reduce threats due to misuse of (personal) information. Privacy by Design as the present methodology for developing privacy-preserving and secure IT systems aims to reduce security vulnerabilities already in the early requirement analysis phase of software development. Incident reports show, however, that not only an implementation of a model bears vulnerabilities but also the gap between rigorous view of threat and security model on the world and real view on a run-time environment with its dependencies. Dependencies threaten reliability of information, and in case of personal information, privacy as well. With the aim of improving security and privacy during run-time, this work proposes to extend Privacy by Design by adapting an IT system not only to inevitable security vulnerabilities but in particular to their users’ view on an information exchange and its IT support with different, eventually opposite security interests. © IFIP International Federation for Information Processing 2014.
SCOPUS;2014;Rationalism with a dose of empiricism: Case-based reasoning for requirements-driven self-adaptation;;Requirements-driven approaches provide an effective mechanism for self-adaptive systems by reasoning over their runtime requirements models to make adaptation decisions. However, such approaches usually assume that the relations among alternative behaviours, environmental parameters and requirements are clearly understood, which is often simply not true. Moreover, they do not consider the influence of the current behaviour of an executing system on adaptation decisions. In this paper, we propose an improved requirements-driven self-adaptation approach that combines goal reasoning and case-based reasoning. In the approach, past experiences of successful adaptations are retained as adaptation cases, which are described by not only requirements violations and contexts, but also currently deployed behaviours. The approach does not depend on a set of original adaptation cases, but employs goal reasoning to provide adaptation solutions when no similar cases are available. And case-based reasoning is used to provide more precise adaptation decisions that better reflect the complex relations among requirements violations, contexts, and current behaviours by utilizing past experiences. Our experimental study with an online shopping benchmark shows that our approach outperforms both requirements-driven approach and case-based reasoning approach in terms of adaptation effectiveness and overall quality of the system. © 2014 IEEE.
SCOPUS;2014;Worst-case scheduling of software tasks a constraint optimization model to support performance testing;;Real-Time Embedded Systems (RTES) in safety-critical domains, such as maritime and energy, must satisfy strict performance requirements to be deemed safe. Therefore, such systems have to be thoroughly tested to ensure their correct behavior even under the worst operating conditions. In this paper, we address the need of deriving worst case scenarios with respect to three common performance requirements, namely task deadlines, response time, and CPU usage. Specifically, we investigate whether this worst-case analysis can be effectively re-expressed as a Constrained Optimization Problem (COP) over the space of possible inputs to the system. Solving this problem means finding the sets of inputs that maximize the chance to violate performance requirements at runtime. Such inputs can in turn be used to test if the target RTES meets the expected performance even in the worst case. We develop an OPL model for IBM ILOG CP Optimizer that implements a task priority-based preemptive scheduling, and apply it to a case study from the maritime and energy domain. Our validation shows that (1) the input to our model can be provided with reasonable effort in an industrial setting, and (2) the COP effectively identifies test cases that maximize deadline misses, response time, and CPU usage. © 2014 Springer International Publishing Switzerland.
SCOPUS;2014;Managing expectations: Runtime negotiation of information quality requirements in event-based systems;Event-based systems,, Malleability,, Quality of information,, Runtime negotiation,, Self-adaptive systems;Interconnected smart devices in the Internet of Things (IoT) provide fine-granular data about real-world events, leveraged by service-based systems using the paradigm of event-based systems (EBS) for invocation. Depending on the capabilities and state of the system, the information propagated in EBS differs in content but also in properties like precision, rate and freshness. At runtime, consumers have different dynamic requirements about those properties that constitute quality of information (QoI) for them. Current approaches to support quality-related requirements in EBS are either domain-specific or limited in terms of expressiveness, flexibility and scope as they do not allow participants to adapt their behavior. We introduce the generic concept of expectations to express, negotiate and enforce arbitrary requirements about information quality in EBS at runtime. In this paper, we present the model of expectations, capabilities and feedback based on generic properties. Participants express requirements and define individual tradeoffs between them as expectations while system features are expressed as capabilities. We discuss the algorithms to (i) negotiate requirements at runtime in the middleware by matching expectations to capabilities and (ii) adapt participants as well as the middleware. We illustrate the architecture for runtime-support in industry-strength systems by describing prototypes implemented within a centralized and a decentralized EBS. © Springer-Verlag Berlin Heidelberg 2014.
SCOPUS;2014;Software evaluation via users' feedback at runtime;Crowdsourcing,, Requirements engineering,, Software evaluation,, Users feedback;Users' evaluation of software at runtime is a powerful tool which enables us to capture and communicate a richer and updated knowledge on how users view software throughout its life cycle. Users understand the software as a means to meet their requirements and, thus, giving them a voice in the continuous runtime evaluation of software would naturally need to fit this level of abstraction. That is, users' evaluation feedback would mainly communicate their judgment on the role of the system in meeting their requirements. Users' runtime evaluation feedback could be used to take autonomous or semi-autonomous runtime adaptation decisions or to support developers on taking evolution and maintenance decisions. Within this picture, our research focuses on to the development of a modeling and elicitation framework of users' evaluation feedback at runtime. This includes devising mechanisms to structure such an evaluation feedback in a way that makes it easy for users to express and developers to interpret. We motivate our work and articulate the problem and the set of research questions to address in our research and the method to follow and reach them. We also discuss our initial results on the topic. Copyright 2014 ACM.
SCOPUS;2014;Requirements-driven social adaptation: Expert survey;Adaptive Systems,, Requirements Engineering,, Social Adaptation;[Context and motivation] Self-adaptation empowers systems with the capability to meet stakeholders' requirements in a dynamic environment. Such systems autonomously monitor changes and events which drive adaptation decisions at runtime. Social Adaptation is a recent kind of requirements-driven adaptation which enables users to give a runtime feedback on the success and quality of a system's configurations in reaching their requirements. The system analyses users' feedback, infers their collective judgement and then uses it to shape its adaptation decisions. [Question/problem] However, there is still a lack of engineering mechanisms to guarantee a correct conduction of Social Adaptation. [Principal ideas/results] In this paper, we conduct a two-phase Expert Survey to identify core benefits, domain areas and challenges for Social Adaptation. [Contribution] Our findings provide practitioners and researchers in adaptive systems engineering with insights on this emerging role of users, or the crowd, and stimulate future research to solve the open problems in this area. © 2014 Springer International Publishing Switzerland.
SCOPUS;2014;A Requirements Monitoring Infrastructure for Very-Large-Scale Software Systems;Requirements monitoring,, very-large-scale software systems;[Context and motivation] Approaches for requirements monitoring check the compliance of systems with their requirements during operation. [Question/problem] Despite many advances, requirements monitoring remains challenging particularly for very-large-scale software systems (VLSS) with system-of-systems architectures. [Principal ideas/results] In this research preview we describe key characteristics of industrial VLSS and discuss implications for requirements monitoring. Furthermore, we report on our ongoing work of developing a requirements monitoring infrastructure addressing these characteristics. [Contribution] Our infrastructure supports runtime monitoring of requirements across systems,, variability management of requirements-based monitors,, and the integration of monitoring data from different sources in a VLSS. © 2014 Springer International Publishing Switzerland.
SCOPUS;2014;The requirements problem for adaptive systems;Adaptive systems,, Requirements engineering,, Requirements modelling language,, Requirements problem,, Requirements problem for adaptive systems;Requirements Engineering (RE) focuses on eliciting, modeling, and analyzing the requirements and environment of a system-to-be in order to design its specification. The design of the specification, known as the Requirements Problem (RP), is a complex problem-solving task because it involves, for each new system, the discovery and exploration of, and decision making in a new problem space. A system is adaptive if it can detect deviations between its runtime behavior and its requirements, specifically situations where its behavior violates one or more of its requirements. Given such a deviation, an Adaptive System uses feedback mechanisms to analyze these changes and decide, with or without human intervention, how to adjust its behavior as a result. We are interested in defining the Requirements Problem for Adaptive Systems (RPAS). In our case, we are looking for a configurable specification such that whenever requirements fail to be fulfilled, the system can go through a series of adaptations that change its configuration and eventually restore fulfilment of the requirements. From a theoretical perspective, this article formally shows the fundamental differences between standard RE (notably Zave and Jackson [1997]) and RE for Adaptive Systems (see the seminal work by Fickas and Feather [1995], to Letier and van Lamsweerde [2004], and up to Whittle et al. [2010]). The main contribution of this article is to introduce the RPAS as a new RP class that is specific to Adaptive Systems. We relate the RPAS to RE research on the relaxation of requirements, the evaluation of their partial satisfaction, and the monitoring and control of requirements, all topics of particular interest in research on adaptive systems [de Lemos et al. 2013]. From an engineering perspective, we define a protoframework for solving RPAS, which illustrates features needed in future frameworks for adaptive software systems. © 2014 ACM.
SCOPUS;2014;Improving business value assurance in large-scale IT projects - A quantitative method based on founded requirements assessment;Action design research,, Business value of IT,, IT project controlling,, Requirements engineering,, Value assurance;The probability of IT project failures can be mitigated more successfully when discovered early. To support a more insightful management of IT projects, which may also facilitate an early detection of IT project failures, transparency regarding a project's cash flows shall be increased. Therefore, an appropriate analysis of a project's benefits, costs, requirements, their respective risks and interdependencies is inevitable. However, to date, in requirements engineering only few methods exist that appropriately consider these factors when estimating the ex ante project business case. Furthermore, empirical studies reveal that a lot of risk factors emerge during the runtime of projects why the ex ante valuation of IT projects even with respect to requirements seems insufficient. Therefore, using the Action Design Research approach, we design, apply, and evaluate a practicable method for value-based continuous IT project steering especially for large-scale IT projects. © 2014 ACM.
SCOPUS;2014;Flexibility requirements in real-world process scenarios and prototypical realization in the care domain;;Flexibility is a key concern in business process management and mature solutions and systems have been developed during the last years. What can be observed is that the approaches mostly consider process instances that are executed based on a process schema reflecting a process type, e.g., an order process. The process instances might be adapted during runtime in an individual manner (ad-hoc changes) or the process schema evolves due to, for example, new regulations. We studied cases from four different domains for their requirements on flexibility. These use cases are characterized by long running, highly adaptive, and individual instances, i.e., instances that are not based on a common process schema, but develop during runtime based on context and processrelevant data. The requirements analysis shows that can only part of them can be met by existing approaches. To illustrate an initial solution meeting the identified requirements, a prototypical implementation of the care domain use case is demonstrated, followed by a discussion of lessons learned and a research agenda. © Springer-Verlag Berlin Heidelberg 2014.
SCOPUS;2014;Dealing with multiple failures in zanshin: A control-theoretic approach;Adaptive systems,, Multiple failures,, Optimization,, Requirements engineering,, Zanshin;Adaptive software systems monitor the environment to ensure that their requirements are being fulfilled. When this is not the case, their adaptation mechanism proposes an adaptation (a change to the behaviour/configuration) that can lead to restored satisfaction of system requirements. Unfortunately, such adaptation mechanisms don't work very well in cases where there are multiple failures (divergence of system behaviour relative to several requirements). This paper proposes an adaptation mechanism that can handle multiple failures. The proposal consists of extending the Qualia adaptation mechanism of Zanshin enriched with features adopted from Control Theory. The proposed framework supports the definition of requirements for the adaptation process prescribing how to deal at runtime with problems such as conicting requirements and synchronization, enhancing the precision and effectiveness of the adaptation mechanism. The proposed mechanism, named Qualia+ is illustrated and evaluated with an example using the meeting scheduling exemplar.
SCOPUS;2014;6th Workshop on Software Engineering for Resilient Systems, SERENE 2014;;The proceedings contain 13 papers. The special focus in this conference is on Requirements engineering and re-engineering for resilience,, Design of trustworthy and intrusion-safe systems,, Formal and semi-formal techniques for verification and validation,, Quantitative approaches to ensuring resilience and Resilience prediction. The topics include: Community resilience engineering,, enhancing architecture design decisions evolution with group decision making principles,, the role of parts in the system behaviour,, automatic generation of description files for highly available services,, modelling resilience of data processing capabilities of CPS,, formal fault tolerance analysis of algorithms for redundant systems in early design stages,, on applying FMEA to SOAs,, verification and validation of a pressure control unit for hydraulic systems,, simulation testing and model checking,, advanced modelling, simulation and verification for future traffic regulation optimization,, using instrumentation for quality assessment of resilient software in embedded systems,, adaptive domain-specific service monitoring and combined error propagation analysis and runtime event detection in process-driven systems.
SCOPUS;2014;Extending lyapack for the solution of band Lyapunov equations on hybrid CPU–GPU platforms;Banded matrix Lyapunov equations,, GPUs,, lyapack;The solution of large-scale Lyapunov equations is an important tool for the solution of several engineering problems arising in optimal control and model order reduction. In this work, we investigate the case when the coefficient matrix of the equations presents a band structure. Exploiting the structure of this matrix, we can achive relevant reductions in the memory requirements and the number of floating-point operations. Additionally, the new solver efficiently leverages the parallelism of CPU–GPU platforms. Furthermore, it is integrated in the lyapack library to facilitate its use. The new codes are evaluated on the solution of several benchmarks, exposing significant runtime reductions with respect to the original CPU version in lyapack. © 2014, Springer Science+Business Media New York.
SCOPUS;2014;Study on the transformation method of aadl-based reliability model in the embedded system;Architecture analysis and design language,, Embedded system,, Model transformation,, Stochastic petri net;Architecture analysis and design language (AADL) is an important method for architecture modeling, performance analysis and verification in embedded field. And the system reliability is an important attribute of software quality metrics in embedded system. In this paper, we first established an AADL-based reliability model to describe the system functional requirements and the reliability information of the runtime embedded system. Then, by analyzing the differences of syntax, semantics and mathematical presentation between the AADL-based reliability model and the SPN model, we present the rules and methods to automatically transform the AADL-based reliability model to the stochastic Petri net (SPN) model, which is convenient for system designers to assess and measure system reliability in the design phase of system development. Finally, an example of model transformation process for an automaton in aviation systems is shown to verify the effectiveness of model transformation rules and methods. © Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2014.
SCOPUS;2014;A middleware and policy-based adaptation framework to simplify software evolution: An implementation on unit trust enterprise system;Framework,, Middleware,, Policy,, Software adaptation,, Software evolution;Software evolution needs to be properly controlled to avoid huge problems during maintenance phase. Software needs to evolve to ensure it meets its development purpose. One of promising ways to address the issue of software evolution is via software adaptation. There are 4 main approaches to software adaptation i.e. architecture-based, component-based, agent-oriented and middleware-based. This research is adopting middleware-based approach to software adaptation. An adaptation framework called MiPAF, which uses middleware and policy-based concept is proposed to simplify software evolution. MiPAF comprises 6 components namely Service Manager, Adaptation Manager, Service Infrastructure, Device Controller, Policy Repository and Context Monitor. The use of MiPAF will affect 4 software development phases i.e. requirement, analysis, design, and development. MiPAF runtime is developed to enable adaptation of the device layer of a Unit Trust Enterprise System (UTES). A simple, XML-based policy language is developed to specify what action to be taken when certain condition happens. The adaptation requirements of this system is specified and an adaptation policy is developed according to the requirements. In this implementation, MiPAF runtime is developed using C language and it is installed on workstations together with UTES client. There are 2 adaptation requirements for this implementation. The first requirement is when a passbook printer fail, the system can proceed with printing using another passbook printer without interruption. The second requirement is that when the type of passbook printer is changed, the system should not be impacted. The evaluation is done against 6 evaluation criteria,, scalability, context-awareness, performance, usability, heterogeneity, and dynamic-evolveability. MiPAF meets all the mentioned criteria. © 2005 - 2014 JATIT & LLS. All rights reserved.
SCOPUS;2014;Progress and improvement of KSTAR plasma control using model-based control simulators;Control design,, KSTAR,, Magnetic fusion,, Plasma control,, Simulator,, Tokamak;Superconducting tokamaks like KSTAR, EAST and ITER need elaborate magnetic controls mainly due to either the demanding experiment schedule or tighter hardware limitations caused by the superconducting coils. In order to reduce the operation runtime requirements, two types of plasma simulators for the KSTAR plasma control system (PCS) have been developed for improving axisymmetric magnetic controls. The first one is an open-loop type, which can reproduce the control done in an old shot by loading the corresponding diagnostics data and PCS setup. The other one, a closed-loop simulator based on a linear nonrigid plasma model, is designed to simulate dynamic responses of the plasma equilibrium and plasma current (Ip) due to changes of the axisymmetric poloidal field (PF) coil currents, poloidal beta, and internal inductance. The closed-loop simulator is the one that actually can test and enable alteration of the feedback control setup for the next shot. The simulators have been used routinely in 2012 plasma campaign, and the experimental performances of the axisymmetric shape control algorithm are enhanced. Quality of the real-time EFIT has been enhanced by utilizations of the open-loop type. Using the closed-loop type, the decoupling scheme of the plasma current control and axisymmetric shape controls are verified through both the simulations and experiments. By combining with the relay feedback tuning algorithm, the improved controls helped to maintain the shape suitable for longer H-mode (10-16 s) with the number of required commissioning shots largely reduced. © 2013 Elsevier B.V.
SCOPUS;2014;On requirements representation and reasoning using answer set programming;;We describe an approach to the representation of requirements using answer set programming and how this leads to a vision for the role of artificial intelligence techniques in software engineering with a particular focus on adaptive business systems. We outline how the approach has developed over several years through a combination of commercial software development and artificial intelligence research, resulting in: (i) a metamodel that incorporates the notion of runtime requirements, (ii) a formal language for their representation and its supporting computational model (InstAL), and (iii) a software architecture that enables monitoring of distributed systems. The metamodel is the result of several years experience in the development of business systems for e-tailing, while InstAL and the runtime monitor is on-going research to support the specification, verification and application of normative frameworks in distributed intelligent systems. Our approach derives from the view that in order to build agile systems, the components need to be structured more like software that controls robots, in that it is designed to be relatively resilient in the face of a non-deterministic, dynamic, complex environment about which there is incomplete information. Thus, degrees of autonomy become a strength and an opportunity, but must somehow be constrained by informing these autonomous components what should be done in a certain situation or what system state ought to be achieved through norms as expressions of requirements. Because such a system made up of autonomous components is potentially behaviourally complex and not just complicated, it becomes essential to monitor both whether norms/requirements are being fulfilled and if not why not. Finally, because control over the system can be expressed through requirements in the form of data that can be changed, a route is opened to adjustment and dynamic re-direction of running systems. © 2014 IEEE.
SCOPUS;2014;Improving OLAM with cloud elasticity;Cloud Computing,, Cloudine Framework,, Elasticity,, Load Balancing,, Ocean-Land-Atmosphere Model;Elasticity is considered one of the fundamental properties of cloud computing, and can be seen as the ability of a system to increase or decrease the computing resources allocated in a dynamic and on demand way. This feature is suitable for dynamic applications, whose resources requirements cannot be determined exactly in advance, either due to changes in runtime requirements or in application structure. A good candidate for using cloud elasticity is the Ocean-Land-Atmosphere Model (OLAM), since it presents a significant load variation during its execution and due to online mesh refinement (OMR), that causes load unbalancing problems. In this paper, we present our efforts to adapt OLAM to use the elasticity offered in cloud environments to dynamic allocate resources according to the demands of each execution phase, and to minimize the load unbalancing caused by OMR. The results show that elasticity was successfully used to provide these features, improving the OLAM performance and providing a better use of resources. © 2014 Springer International Publishing.
SCOPUS;2014;A catalogue of optimization techniques for Triple Graph Grammars;;Bidirectional model transformation languages are typically declarative, being able to provide unidirectional operationalizations from a common specification automatically. Declarative languages have numerous advantages, but ensuring runtime efficiency, especially without any knowledge of the underlying transformation engine, is often quite challenging. Triple Graph Grammars (TGGs) are a prominent example for a completely declarative, bidirectional language and have been successfully used in various application scenarios. Although an optimization phase based on profiling results is often a necessity to meet runtime requirements, there currently exists no systematic classification and evaluation of optimization strategies for TGGs, i.e., the optimization process is typically an ad-hoc process. In this paper, we investigate the runtime scalability of an exemplary bidirectional model-to-text transformation. While systematically optimizing the implementation, we introduce, classify and apply a series of optimization strategies. We provide in each case a quantitative measurement and qualitative discussion, establishing a catalogue of current and future optimization techniques for TGGs in particular and declarative rule-based model transformation languages in general.
SCOPUS;2014;Supporting elasticity in OpenMP applications;cloud computing,, elasticity,, OpenMP,, parallel applications;Elasticity can be seen as the ability of a system to increase or decrease the computing resources allocated in a dynamic and on demand way. In order to explore this feature, several works addressed the development of frameworks and platforms focusing the construction of elastic parallel and distributed applications for IaaS clouds. However, none of these works addressed the exploration of elasticity in multithreaded applications. In this paper, we propose a mechanism to provide elasticity support for OpenMP applications, making possible the dynamic provisioning of cloud resources taking into account the program structure and runtime requirements. In our proposal, the OpenMP directives were modified to support the dynamic adjustment of resources and a set of routines were included to the user-level library in order to enable the configuration of the the elastic execution. Dynamic memory allocation support was also included in elastic OpenMP library. We also present the architecture and implementation of the proposed mechanism. The experiments validate our approach and show some possibilities to use the elastic OpenMP. © 2014 IEEE.
SCOPUS;2014;Automated assessment of pleural thickening: Towards an early diagnosis of pleuramesothelioma;;Assessment of the growth of pleural thickenings is crucial for an early diagnosis of pleuramesothelioma. The presented automatic system supports the physician in comparing two temporally consecutive CT data-sets to determine this growth. The algorithms perform the determination of the pleural contours. After surface-based smoothing, anisotropic diffusion, a model-oriented probabilistic classification specifies the thickening’s tissue. The volume of each detected thickening is determined. While doctors still have the possibility to supervise the detection results, a full automatic registration carries out the matching of the same thickenings in two consecutive datasets to fulfill the change follow-up, where manual control is still possible thereafter. All algorithms were chosen and designed to meet runtime requirements, which allow an application in the daily routine. © Springer-Verlag Berlin Heidelberg 2014.
SCOPUS;2014;Parallel fast multipole boundary element method for crustal dynamics;;Crustal faults and sharp material transitions in the crust are usually represented as triangulated surfaces in structural geological models. The complex range of volumes separating such surfaces is typically three-dimensionally meshed in order to solve equations that describe crustal deformation with the finite-difference (FD) or finite-element (FEM) methods. We show here how the Boundary Element Method, combined with the Multipole approach, can revolutionise the calculation of stress and strain, solving the problem of computational scalability from reservoir to basin scales. The Fast Multipole Boundary Element Method (Fast BEM) tackles the difficulty of handling the intricate volume meshes and high resolution of crustal data that has put classical Finite 3D approaches in a performance crisis. The two main performance enhancements of this method: the reduction of required mesh elements from cubic to quadratic with linear size and linear-logarithmic runtime,, achieve a reduction of memory and runtime requirements allowing the treatment of a new scale of geodynamic models. This approach was recently tested and applied in a series of papers by [1, 2, 3] for regional and global geodynamics, using KD trees for fast identification of near and far-field interacting elements, and MPI parallelised code on distributed memory architectures, and is now in active development for crustal dynamics. As the method is based on a free-surface, it allows easy data transfer to geological visualisation tools where only changes in boundaries and material properties are required as input parameters. In addition, easy volume mesh sampling of physical quantities enables direct integration with existing FD/FEM code. © 2010 IOP Publishing Ltd.
SCOPUS;2014;Computation scalable disparity estimation for delay sensitive 3D video surveillance system;;Disparity estimation is an important task in many 3D video surveillance applications. How to generate the disparity information at the front end under limited delay budget is very challenging in a real-time surveillance system. In this paper, we tackle this problem through adopting multiresolution strategy in the disparity estimation process. Our contribution is twofold. First, unlike existing coarse-to-fine strategies based on uniform sampling, we present a fast disparity estimation algorithm based on nonuniform sampling at the coarse level. The disparity values of the non-samples are initially interpolated through Delaunay Triangulation, and then are refined through bilateral filtering. The content-aware nonuniform sampling provides better disparity approximation in the triangulated interpolation, and consequently leads to faster convergence in the refinement procedure. Second, we model the computation time in the disparity estimation process for execution assistance in the delay sensitive surveillance system. Both the sampling cell size and the data resolution are considered in this model, in order to accommodate different runtime requirements. Experimental results demonstrate the efficiency of the proposed scheme. © 2014 IEEE.
SCOPUS;2014;Computational lithography platform for 193i-guided directed self-assembly;Compact model,, Computational Lithography,, Directed Self Assembly,, DSA model,, Lithography Optimization,, Mask Decomposition,, Optical extension,, Optical Lithography,, Optical Proximity Correction,, Source Mask Optimization;We continue to study the feasibility of using Directed Self Assembly (DSA) in extending optical lithography for High Volume Manufacturing (HVM). We built test masks based on the mask datatprep flow we proposed in our prior year's publication [1]. Experimental data on circuit-relevant fin and via patterns based on 193nm graphoepitaxial DSA are demonstrated on 300mm wafers. With this computational lithography (CL) flow we further investigate the basic requirements for full-field capable DSA lithography. The first issue is on DSA-specific defects which can be either random defects due to material properties or the systematic DSA defects that are mainly induced by the variations of the guiding patterns (GP) in 3 dimensions. We focus in studying the latter one. The second issue is the availability of fast DSA models to meet the full-chip capability requirements in different CL component's need. We further developed different model formulations that constitute the whole spectrum of models in the DSA CL flow. In addition to the Molecular Dynamic/Monte Carlo (MD/MC) model and the compact models we discussed before [2], we implement a 2D phenomenological phase field model by solving the Cahn-Hilliard type of equation that provide a model that is more predictive than compact model but much faster then the physics-based MC model. However simplifying the model might lose the accuracy in prediction especially in the z direction so a critical question emerged: Can a 2D model be useful fro full field? Using 2D and 3D simulations on a few typical constructs we illustrate that a combination of 2D mode with pre-characterized 3D litho metrics might be able to approximate the prediction of 3D models to satisfy the full chip runtime requirement. Finally we conclude with the special attentions we have to pay in the implementation of 193nm based lithography process using DSA. © 2014 SPIE.
SCOPUS;2014;Optimizing multi-objective evolutionary algorithms to enable quality-aware software provisioning;Cloud,, Hyper-heuristics,, MOEAs,, Optimization,, Software Deployment;Elasticity is a key feature for cloud infrastructures to continuously align allocated computational resources to evolving hosted software needs. This is often achieved by relaxing quality criteria, for instance security or privacy because quality criteria are often conflicting with performance. As an example, software replication could improve scalability and uptime while decreasing privacy by creating more potential leakage points. The conciliation of these conflicting objectives has to be achieved by exhibiting trade-offs. Multi-Objective Evolutionary Algorithms (MOEAs) have shown to be suitable candidates to find these trade-offs and have been even applied for cloud architecture optimizations. Still though, their runtime efficiency limits the widespread adoption of such algorithms in cloud engines, and thus the consideration of quality criteria in clouds. Indeed MOEAs produce many dead-born solutions because of the Darwinian inspired natural selection, which results in a resources wastage. To tackle MOEAs efficiency issues, we apply a process similar to modern biology. We choose specific artificial mutations by anticipating the optimization effect on the solutions instead of relying on the randomness of natural selection. This paper introduces the Sputnik algorithm, which leverages the past history of actions to enhance optimization processes such as cloud elasticity engines. We integrate Sputnik in a cloud elasticity engine, dealing with performance and quality criteria, and demonstrate significant performance improvement, meeting the runtime requirements of cloud optimization. © 2014 IEEE.
SCOPUS;2014;Accurate crop classification using hierarchical genetic fuzzy rule-based systems;crop classification,, embedded feature selection,, Genetic fuzzy rule-based classification systems (GFRBCS),, hierarchical classifier,, higher-order spectral and textural features,, per-class feature selection;This paper investigates the effectiveness of an advanced classification system for accurate crop classification using very high resolution (VHR) satellite imagery. Specifically, a recently proposed genetic fuzzy rule-based classification system (GFRBCS) is employed, namely, the Hierarchical Rule-based Linguistic Classifier (HiRLiC). HiRLiC's model comprises a small set of simple IF-THEN fuzzy rules, easily interpretable by humans. One of its most important attributes is that its learning algorithm requires minimum user interaction, since the most important learning parameters affecting the classification accuracy are determined by the learning algorithm automatically. HiRLiC is applied in a challenging crop classification task, using a SPOT5 satellite image over an intensively cultivated area in a lake-wetland ecosystem in northern Greece. A rich set of higher-order spectral and textural features is derived from the initial bands of the (pan-sharpened) image, resulting in an input space comprising 119 features. The experimental analysis proves that HiRLiC compares favorably to other interpretable classifiers of the literature, both in terms of structural complexity and classification accuracy. Its testing accuracy was very close to that obtained by complex state-of-the-art classification systems, such as the support vector machines (SVM) and random forest (RF) classifiers. Nevertheless, visual inspection of the derived classification maps shows that HiRLiC is characterized by higher generalization properties, providing more homogeneous classifications that the competitors. Moreover, the runtime requirements for producing the thematic map was orders of magnitude lower than the respective for the competitors. © 2014 SPIE.
SCOPUS;2014;Towards monitoring cloud services using Models@run.time;Cloud computing,, Model driven engineering,, Models@run.time,, Monitoring,, SaaS,, SLA;Cloud computing represents a new trend to provide software services. In order to deliver these services there are certain quality levels that should be considered. The provided services need to comply with a set of contract terms and non-functional requirements specified by a service level agreement (SLA). In addition, to support the fulfillment of the SLA a monitoring process should be defined. This allows service providers to determine the actual quality level of services in the cloud. In this paper, we define a monitoring process for the usage of models at runtime, specifying low- and high-level nonfunctional requirements contained in a SLA. Models at runtime provide flexibility to the monitoring infrastructure due to their reflection mechanisms,, the modification of non-functional requirements may dynamically change the monitoring computation, avoiding the need to adjust the monitoring infrastructure. In our approach, models at runtime are part of a monitoring middleware that interacts with cloud services,, it retrieves data in the model at runtime, analyzes the information, and provides a report detailing the issues of non-compliance of non-functional requirements.
SCOPUS;2013;Towards preventing error propagation in a real-time Ethernet switch;;Flexible Time-Triggered communication (FTT) allows a distributed embedded system (DES) to adapt to changing real-time requirements at runtime. This facilitates the continuous operation of the DES under dynamic environments that change over time. However, for continuous operation, high reliability in the nodes of the DES is also crucial. This can be achieved using node replication, as long as failure independence between replicas is ensured, which calls for preventing the propagation of errors. Our goal is to prevent the propagation of Byzantine node behaviours and to ensure that local errors in the channel cannot disturb the global communication. For this, we construct the HaRTES/PG switch, a new switch based on the HaRTES implementation of FTT for Ethernet. This paper presents as a first step a study of the possible errors that may lead to Byzantine node behaviours and a global communication disturbance in HaRTES, as well as some ideas on how to prevent the propagation of these errors in HaRTES/PG. © 2013 IEEE.
SCOPUS;2013;Requirements models for design- and runtime: A position paper;;In this position paper we review the history of requirements models and conclude that a goal-oriented perspective offers a suitable abstraction for requirements analysis. We stake positions on the nature of modelling languages in general, and requirements modelling languages in particular. We then sketch some of the desirable features (... 'requirements') of design-time and runtime requirements models and draw conclusions about their similarities and differences. © 2013 IEEE.
SCOPUS;2013;From GMoDS models to object-oriented specifications in Event-B;;This paper represents a first attempt to express in Event-B the models of the GMoDS (The Goal Model for Dynamic Systems) methodology. GMoDS is a major result of the research related to Organization-Based Multiagent System Engineering methodology (O-MaSE), allowing to specify goals during requirements engineering process and then to use them throughout the system development and at runtime. We choose Event-B as a modelling language for specifying the GMoDS models for two reasons: (a) Event-B has the concept of proving correctness, which supports the accuracy of software development, and (b) its supporting tool, RODIN, is open-source, and it is used in many software industrial applications. Because Event-B is not object-oriented, and because several concepts used in GMoDS are related to object-orientation, we have included in the corresponding Event-B specifications the object-oriented concepts used in the GMoDS models, without changing the syntax and semantics of Event-B, and without using the UML-B tool. © 2013 IEEE.
SCOPUS;2013;'Computing' requirements in open source software projects;Distributed cognition,, Open source software,, Requirements computation,, Requirements engineering;Due to high dissimilarity with traditional software development, Requirements Engineering (RE) in Open Source Software (OSS) remains poorly understood, despite the visible success of many OSS projects. In this study, we approach OSS RE as a sociotechnical and distributed cognitive activity where multiple actors deploy heterogeneous artifacts to 'compute' requirements as to reach a collectively-held understanding of what the software is going to do. We conduct a case study of a popular OSS project, Rubinius (a Ruby programming language runtime environment). Specifically, we investigate the ways in which this project exhibits distribution of cognitive efforts along social, structural, and temporal dimensions and how its requirements computation takes place accordingly. In particular, we seek to generalize to a theoretical framework that explains how three temporally-ordered processes of distributed cognition in OSS projects, denoted excavation, instantiation, and testing-inthe- wild, tie together to form a powerful distributed computational structure to manage requirements. © (2013) by the AIS/ICIS Administrative Office All rights reserved.
SCOPUS;2013;2013 5th International Workshop on Modeling in Software Engineering, MiSE 2013 - Proceedings;;The proceedings contain 14 papers. The topics discussed include: lightweight analysis of software design models at the whiteboard,, complementing model-driven development for the detection of software architecture erosion,, enhancing version control with domain-specific semantics,, model based control for multi-cloud applications,, design module: a modularity vision beyond code: not only program code but also a design model is a module,, highlighting the challenges of model-based engineering for spaceflight software systems,, a UML profile for dynamic execution persistence with monitoring purposes,, requirements models for design- and runtime: a position paper,, model oriented programming: bridging the code-model divide,, a model-based approach to language integration,, and prioritizing software anomalies with software metrics and architecture blueprints: a controlled experiment.
SCOPUS;2013;Requirements-driven software evolution;Adaptive systems,, Evolution,, Modeling,, Requirements,, Requirements engineering;It is often the case that stakeholders want to strengthen/weaken or otherwise change their requirements for a system-to-be when certain conditions apply at runtime. For example, stakeholders may decide that if requirement R is violated more than N times in a week, it should be relaxed to a less demanding one R-. Such evolution requirements play an important role in the lifetime of a software system in that they define possible changes to requirements, along with the conditions under which these changes apply. In this paper we focus on this family of requirements, how to model them and how to operationalize them at runtime. In addition, we evaluate our proposal with a case study adopted from the literature. © 2012 Springer-Verlag Berlin Heidelberg.
SCOPUS;2013;Aspect interactions: A requirements engineering perspective;;The principle of Separation of Concerns encourages developers to divide complex problems into simpler ones and solve them individually. Aspect-Oriented Programming (AOP) languages provide mechanisms to modularise concerns that affect several software components, by means of joinpoints, advice and aspect weaving. In a software system with multiple aspects, a joinpoint can often be matched with advice from several aspects, thus giving rise to emergent behaviours that may be unwanted. This issue is often known as the aspect interaction problem. AOP languages provide various composition operators: the precedence operator of AspectJ, for instance, instructs the aspect weaver about the ordering of aspects when advice from several of them match one joinpoint. This ordering of conflicting aspects is usually done at compile-time. This chapter discusses a type of problem where conflicting aspects need to be ordered according to runtime conditions. Extending previous work on Composition Frames, this chapter illustrates an AOP technique to compose aspects in a non-intrusive way so that precedence can be decided at runtime. © Springer-Verlag Berlin Heidelberg 2013. All rights are reserved.
SCOPUS;2013;Semantic issues in model-driven management of information system interoperability;information system,, interoperability,, model-driven engineering,, semantic reconciliation;The Mediation Information System Engineering (MISE Project) aims at providing collaborating organisations with a mediation information system (MIS) in charge of supporting interoperability of a collaborative network. The MISE proposes an overall MIS design method according to a model-driven approach, based on model transformations. This MIS is in charge of managing (i) information, (ii) functions and (iii) processes among the information systems (IS) of partner organisations involved in the network. Semantic issues are accompanying this triple objective: How to deal with information reconciliation? How to ensure the matching between business functions and technical services? How to identify workflows among business processes? This article aims first at presenting the MISE approach, second at defining the semantic gaps along the MISE approach and third at describing some past, current and future research works that deal with these issues. Finally and as a conclusion, the very design-oriented previous considerations are confronted with 'runtime' requirements. © 2013 Copyright Taylor and Francis Group, LLC.
SCOPUS;2013;Requirements-driven self-repairing against environmental failures;;Self-repairing approaches have been proposed to alleviate the runtime requirements satisfaction problem by switching to appropriate alternative solutions according to the feedback monitored. However, little has been done formally on analyzing the relations between specific environmental failures and corresponding repairing decisions, making it a challenge to derive a set of alternative solutions to withstand possible environmental failures at runtime. To address these challenges, we propose a requirements-driven self-repairing approach against environmental failures, which combines both development-time and runtime techniques. At the development phase, in a stepwise manner, we formally analyze the issue of self-repairing against environmental failures with the support of the model checking technique, and then design a sufficient and necessary set of alternative solutions to withstand possible environmental failures. The runtime part is a runtime self-repairing mechanism that monitors the operating environment for unsatisfiable situations, and makes self-repairing decisions among alternative solutions in response to the detected environmental failures. © 2013 IEEE.
SCOPUS;2013;Design on sports meeting management system of university based on .NET framework;.NET Framework,, Database design,, Function design,, Management system,, Requirement analysis,, Sports meeting;Information management is an indispensable part of modern sports meeting management, is to adapt to the requirements of the modern sports meeting management, and promote the sports meeting management to the scientific, standardization of the necessary conditions. According to the status of sports meeting management of university, this paper design is based on. NET Framework Management System. First, according to the software development process has carried on requirement analysis, function design, and database design. Requirement analysis is mainly carried on function analysis and performance analysis, Function design is designed in the description of the basic functions of the system function structure model, database design concept of structural design and the physical structure of the design. Then, research the .NET Framework Application, mainly research .NET Framework two components .NET Framework class library and Common language runtime. The content of this paper for the university build sports meeting management system provides a practical solution. © (2013) Trans Tech Publications, Switzerland.
SCOPUS;2013;Intention-oriented programming support for runtime adaptive autonomic cloud-based applications;;The continuing high rate of advances in information and communication systems technology creates many new commercial opportunities but also engenders a range of new technical challenges around maximising systems' dependability, availability, adaptability, and auditability. These challenges are under active research, with notable progress made in the support for dependable software design and management. Runtime support, however, is still in its infancy and requires further research. This paper focuses on a requirements model for the runtime execution and control of an intention-oriented Cloud-Based Application. Thus, a novel requirements modelling process referred to as Provision, Assurance and Auditing, and an associated framework are defined and developed where a given system's non/functional requirements are modelled in terms of intentions and encoded in a standard open mark-up language. An autonomic intention-oriented programming model, using the Neptune language, then handles its deployment and execution. © 2013 Elsevier Ltd. All rights reserved.
SCOPUS;2013;A Non-factualist defense of the Reflection principle;Nonfactualism,, Rational requirement,, Reflection principle,, van Fraassen;Are there plausible synchronic constraints on how a subject thinks of herself extended over time? At first glance, Bas van Fraassen's principle of Reflection seems to prescribe the sort of epistemic authority one's future self should be taken by one to have over one's current epistemic states. (The gist of this principle is that I should now believe what I'm convinced I will believe tomorrow.) There has been a general consensus that, as a principle concerning epistemic authority, Reflection does not apply to epistemically non-ideal agents. I agree with this, but argue here that it misses the point of Reflection. Rather than an epistemic principle concerning reasons for belief, Reflection concerns the semantics of belief avowal. I present a non-factual interpretation of Reflection, argue that the principle provides a constraint on the ways in which one can reflectively endorse one's future epistemic self, and say something about the logic governing such an interpretation. © 2012 Springer Science+Business Media B.V.
SCOPUS;2013;Runtime goal models: Keynote;Goal reasoning,, Requirements at runtime,, Runtime goal models,, Self-adaptive systems;Goal models capture stakeholder requirements for a system-to-be, but also circumscribe a space of alternative specifications for fulfilling these requirements. Recent proposals for self-adaptive software systems rely on variants of goal models to support monitoring and adaptation functions. In such cases, goal models serve as mechanisms in terms of which systems reflect upon their requirements during their operation. We argue that existing proposals for using goal models at runtime are using design artifacts for purposes they were not intended, i.e., for reasoning about runtime system behavior. In this paper, we propose a conceptual distinction between Design-time Goal Models (DGMs)-used to design a system-and Runtime Goal Models (RGMs)-used to analyze a system's runtime behavior with respect to its requirements. RGMs extend DGMs with additional state, behavioral and historical information about the fulfillment of goals. We propose a syntactic structure for RGMs, a method for deriving them from DGMs, and runtime algorithms that support their monitoring. © 2013 IEEE.
SCOPUS;2013;Using goals and customizable services to improve adaptability of process-based service compositions;;When implementing (semi-)automatic business processes with services, engineers are facing two sources of variability. One source of variability are alternative refinements and decompositions of requirements. The other source of variability is that various (combinations of) services can be used to satisfy the same requirements. We suggest a method based on the use of a goal model and customizable services able to exploit these variabilities to design executable business process. This method improves the adaptability of the business process at runtime. We illustrate the contribution of our method with an example. © 2013 IEEE.
SCOPUS;2013;7th International Conference on Research Challenges in Information Science, RCIS 2013 - Conference Proceedings;;The proceedings contain 71 papers. The topics discussed include: towards a proposal to capture usability requirements through guidelines,, collaborative creativity in requirements engineering: analysis and practical advice,, handling requirements dependencies in agile projects: a focus group with agile software development practitioners,, goal formalization and classification for requirements engineering, fifteen years later,, pattern based methodology for UML profiles evolution management,, supporting organizational evolution by means of model-driven reengineering frameworks,, M2Flex: a process metamodel for flexibility at runtime,, model driven development of a generic data control engine: an industrial experience,, modeling temporality and subjectivity in ConML,, a framework for question answering and its application to business intelligence,, and adapting to uncertain and evolving enterprise requirements: the case of business-driven business intelligence.
SCOPUS;2013;DYNAMICO: A reference model for governing control objectives and context relevance in self-adaptive software systems;;Despite the valuable contributions on self-adaptation, most implemented approaches assume adaptation goals and monitoring infrastructures as non-mutable, thus constraining their applicability to systems whose context awareness is restricted to static monitors. Therefore, separation of concerns, dynamic monitoring, and runtime requirements variability are critical for satisfying system goals under highly changing environments. In this chapter we present DYNAMICO, a reference model for engineering adaptive software that helps guaranteeing the coherence of (i) adaptation mechanisms with respect to changes in adaptation goals,, and (ii) monitoring mechanisms with respect to changes in both adaptation goals and adaptation mechanisms. DYNAMICO improves the engineering of self-adaptive systems by addressing (i) the management of adaptation properties and goals as control objectives,, (ii) the separation of concerns among feedback loops required to address control objectives over time,, and (iii) the management of dynamic context as an independent control function to preserve context-awareness in the adaptation mechanism. © 2013 Springer-Verlag.
SCOPUS;2013;Synthesis of observers for autonomic evolutionary systems from requirements models;Adaptive Systems,, Autonomous agents,, Fault Detection,, Software Engineering;Monitoring the behaviour of autonomous evolutionary systems is a necessity to control their behaviour at runtime and react to undesired changes or developments. We propose an approach that derives observers from system requirements annotated with OCL-constraints in a model-driven way that can easily be integrated into iterative incremental design processes. © 2013 IFIP.
SCOPUS;2013;Early accessibility evaluation in web application development;;Existing accessibility guidelines are mainly focused on runtime behavior and do not provide recommendations and evaluation for conceptual design of Web applications. Our approach aims to support more abstract principles for analysis and design of accessible Web applications. Combined with a prototype evaluation, it provides early integration of accessibility requirements into the process of Web application development. The approach is based on a model-driven user interface design method. Analysis of tasks and workflow is used to design a prototype which is evaluated with a simple screening technique to get fast and efficient results on selected accessibility requirements. The longtime objective of this work is a general concept for software development which bridges the gap between user requirements and developers needs in the field of accessibility. © 2013 Springer-Verlag Berlin Heidelberg.
SCOPUS;2013;Test strategies for reliable runtime reconfigurable architectures;FPGA,, Online test,, Reconfigurable architectures;Field-programmable gate array (FPGA)-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. The reliability of FPGAs, being manufactured in latest technologies, is threatened by soft errors, as well as aging effects and latent defects. To ensure reliable reconfiguration, it is mandatory to guarantee the correct operation of the reconfigurable fabric. This can be achieved by periodic or on-demand online testing. This paper presents a reliable system architecture for runtimereconfigurable systems, which integrates two nonconcurrent online test strategies: preconfiguration online tests (PRET) and postconfiguration online tests (PORT). The PRET checks that the reconfigurable hardware is free of faults by periodic or on-demand tests. The PORT has two objectives: It tests reconfigured hardware units after reconfiguration to check that the configuration process completed correctly and it validates the expected functionality. During operation, PORT is used to periodically check the reconfigured hardware units for malfunctions in the programmable logic. Altogether, this paper presents PRET, PORT, and the system integration of such test schemes into a runtime-reconfigurable system, including the resource management and test scheduling. Experimental results show that the integration of online testing in reconfigurable systems incurs only minimum impact on performance while delivering high fault coverage and low test latency. © 2013 IEEE.
SCOPUS;2013;Support for intention driven cloud government entitlement services;Elastic Clouds,, GORE,, Intention Modeling,, PAA,, XACML;This paper examines the role of judicial cloud based services in assurance of law enforcement and access to justice for citizen social networks. We are using open service oriented model standard XACML as dynamic runtime based business process modeling, while citizens can dynamically express their instant needs using intentions. We foresee these intentions as temporal goal oriented models by automatically reasoning the requirements driven analysis for these goals. Our approach provides complete business model architecture that stimulates citizen needs. The new contribution of our approach shows how temporal changes of citizen's intentions shall provide different role based entitlement as forced by government laws. We examine judicial entitlement services like notarization services and how different purpose based citizen-to-citizen, citizen-to-business scenarios for each service shall be helpful to anticipate context interactions, thus to avoid business conflict of interest between each of citizens, their societies and government laws and policies. Failing to fulfill minimum requirement for such law enforcement, will reason for automatic re-routing of such intention model to use litigation services, which also contain automatic escalation and management to court based litigation services. © 2013 IEEE.
SCOPUS;2013;Intention-oriented modelling support for GORE in elastic cloud applications;Cloud Applications,, GORE,, Intention,, Neptune;Businesses started to exploit the forthcoming value from deployment of cloud computing as a new caterpillar paradigm to reach out more diversified customer slices. Although the general concepts they practically focus on are: viability, survivability, adaptability, etc., however, on the ground, there is still a lack for forming mechanisms to sustain viability with adaptation of new types of requirements that pertain to other un-tackled aspects of the echosystem. Such aspects like social intentionality are of actors and their goals. This paper introduces modern dynamic software programming environment aided with modelling support to achieve operationalization and adaptation of abstract object,, goals and their properties as formation of new type of requirements into service based applications distributed over the cloud. This will in turn provide system runtime components to interactively confer to guarantee self-adaptive behaviour with respect to its functional and non-functional characteristics. © 2013 IEEE.
SCOPUS;2013;An agent-based requirements monitoring framework for internetware;Agent,, Internetware,, Requirements monitoring,, Self-adaptation,, Self-healing,, Self-repairing;Running in a complicated, open and highly-dynamic environment, Internetware systems are likely to deviate from their requirements specification. In recent years, there have been a series of researches on runtime requirements monitoring and self-repairing based on goal-oriented requirements models and goal reasoning. However, a practical implementation framework for requirements monitoring and repairing, which supports typical Internetware characteristics like distribution and sociality, is still missing. In this paper, we propose an agent-based requirements monitoring framework for Internetware. The monitoring agents in the framework are able to monitor host systems on internal goal satisfaction and cross-agent goal delegation at runtime, and perform actuate repairing actions based on customized policies when requirements deviations are detected in a non-intrusive manner. The framework organizes monitoring agents in a decentralized way and supports cross-system goal delegation, requirements monitoring and self-repairing with inter-agent communication and interaction. To evaluate the effectiveness of our framework, we've conducted a case study with an online product booking system. The results show that the framework can effectively alleviate potential system failures in various self-reparing scenarios.
SCOPUS;2013;Supporting decision-making for self-adaptive systems: From goal models to dynamic decision networks;bayesian decision theory,, dynamic decision networks,, goal models,, requirements,, specification-methodologies;[Context/Motivation] Different modeling techniques have been used to model requirements and decision-making of self-adaptive systems (SASs). Specifically, goal models have been prolific in supporting decision-making depending on partial and total fulfilment of functional (goals) and non-functional requirements (softgoals). Different goalrealization strategies can have different effects on softgoals which are specified with weighted contribution-links. The final decision about what strategy to use is based, among other reasons, on a utility function that takes into account the weighted sum of the different effects on softgoals. [Questions/Problems] One of the main challenges about decisionmaking in self-adaptive systems is to deal with uncertainty during runtime. New techniques are needed to systematically revise the current model when empirical evidence becomes available from the deployment. [Principal ideas/results] In this paper we enrich the decision-making supported by goal models by using Dynamic Decision Networks (DDNs). Goal realization strategies and their impact on softgoals have a correspondence with decision alternatives and conditional probabilities and expected utilities in the DDNs respectively. Our novel approach allows the specification of preferences over the softgoals and supports reasoning about partial satisfaction of softgoals using probabilities. We report results of the application of the approach on two different cases. Our early results suggest the decision-making process of SASs can be improved by using DDNs. © 2013 Springer-Verlag.
SCOPUS;2013;Context-awareness for self-adaptive applications in ubiquitous computing environments;context-awareness,, middleware,, self-adaptation,, socio-technical requirements;Context-awareness is a prerequisite for self-adaptive applications that are able to react and adapt to their runtime context. We have built and evaluated a comprehensive development framework for context-aware, self-adaptive applications in dynamic ubiquitous computing scenarios. The framework consists of a middleware and an associated model-driven development methodology. In this paper we focus on the context-awareness part of the framework. We discuss design objectives, design decisions, and lessons learnt. The main contributions of this paper are generally applicable insights into the design and seamless integration of context-awareness, dynamic service landscapes, and application adaptation management for applications in highly dynamic execution environments. These insights not only relate to the functional requirements and constraints, but also to non-functional aspects that have a strong influence on the user acceptance of such applications. © 2013 ICST Institute for Computer Science, Social Informatics and Telecommunications Engineering.
SCOPUS;2013;A theory of multi-bounce energy-beam-supporting displaced geo-synchronous-orbit satellites;;In an effort to place geo-synchronous-orbit (GSO) satellites outside of conventional Keplerian orbits, the ways of producing a constellation of satellites levitated by multi-bounce energy beams are proposed. The proposed satellite systems, typically consisting of a dual or triplet of satellites, named multi-bounce energy-beam-supported satellite(s) (MEBS), utilize reflection forces generated by a momentum change in beams of matter or photon propulsions by photonic laser thrusters. To intensify the beam density by a million times, a set of nearly perfect reflectors with high aiming accuracy for multiple beam-bouncing is used. Two types of MEBS should be particularly useful: the geo-synchronous low-Earth-orbit type (LEO MEBS) and the nonequatorial geo-stationary-orbit type (GEO MEBS). Orbit construction and orbit transfer at launch and early operation phases (LEOP) and contingency plans (COPs) for orbit failures for the geo-synchronous LEO and GEO MEBS are analyzed. The LEO MEBS require only the same amount of launch energy as the corresponding conventional satellites, and their orbit radii should be larger than 30,72032,417 km for the successful LEOP and COP. The twin geo-synchronous GEO MEBS has the same altitude as the conventional equatorial GEO satellite and shows mitigated power requirement and reflector size. © 1965-2011 IEEE.
SCOPUS;2013;Self-adaptive systems requirements modelling: Four related approaches comparison;;When developing Self Adaptive Systems (SAS), their highly adaptiveness has to be taken into account as early as the requirements elicitation. Because such systems modify their behaviour at run-time in response to changing environmental conditions, Non Functional Requirements (NFR's) play an important role. One has to identify as early as possible the requirements that are adaptable. Because of the inherent uncertainty in these systems, goal based approaches can help in the development of their requirements. In order to cope with this purpose, we have defined a combined approach based on several requirements modelling techniques. In this paper we use a common case study and well defined comparison criteria to illustrate the way those techniques can benefit from each other. This submission is a synthesis and hence make some reference of more specific requirements models submissions. © 2013 IEEE.
SCOPUS;2013;End-to-end formal specification, validation, and verification process: A case study of space flight software;Astronautics,, behavior,, formal methods,, metrics,, process,, requirements engineering,, runtime execution monitoring,, software,, statechart assertions,, verification and validation (V&V);The quality of requirements and the effectiveness of verification and validation (V&V) techniques in guaranteeing that a final system reflects its established requirements have a direct influence on the quality and dependability of the delivered system. The V&V process can be efficient from a managerial point of view, but ineffective from a technical perspective, and vice versa. This paper presents an end-to-end formal computer-aided specification, validation, and verification (SV&V) process, whose feasibility and effectiveness were evaluated against the flight software for the Brazilian Satellite Launcher. Unified modeling language (UML) statechart assertions, scenario-based validation, and runtime verification are used to formally specify and verify the system, and metrics of the ongoing process and its V&V results are collected during the application of the process. The results of the case study indicate that the process and its computer-aided environment were both technically feasible to apply and managerially effective, will likely scale well to cater to SV&V of mission-critical systems that have a larger number of behavioral requirements, and can be used for V&V in a distributed development environment. © 2007-2012 IEEE.
SCOPUS;2013;Social Adaptation at Runtime;Requirements at Runtime,, Requirements-driven Adaptation,, Social Adaptation;One of the main goals of software adaptation is that users get their dynamic requirements met efficiently and correctly. Adaptation is traditionally driven by changes in the system internally and its operational environment. An adaptive system has to monitor and analyse such changes and, if needed, switch to the right behaviour to meet its requirements. In this paper, we advocate another essential driver for adaptation which is the collective judgement of users on the different behaviours of a system. This judgement is based on the feedback iteratively collected from users at run-time. Users feedback should be related to their main interest which is the ability and quality of the system in reaching their requirements. We propose a novel approach to requirements-driven adaptation that gives the collective judgement of users, inferred from their individual feedback, a primary role in planning and guiding adaptation. We apply our approach on a case study and report on the results. © Springer-Verlag Berlin Heidelberg 2013.
SCOPUS;2013;Harnessing evolutionary computation to enable dynamically adaptive systems to manage uncertainty;design,, Dynamically adaptive systems,, model-based development,, requirements engineering,, run-time,, uncertainty;This keynote talk and paper intend to motivate research projects that investigate novel ways to model, analyze, and mitigate uncertainty arising in three different aspects of the cyber-physical systems. First, uncertainty about the physical environment can lead to suboptimal, and sometimes catastrophic, results as the system tries to adapt to unanticipated or poorly-understood environmental conditions. Second, uncertainty in the cyber environment can have lead to unexpected and adverse effects, including not only performance impacts (load, traffic, etc.) but also potential threats or overt attacks. Finally, uncertainty can exist with the components themselves and how they interact upon reconfiguration, including unexpected and unwanted feature interactions. Each of these sources of uncertainty can potentially be identified at different stages, respectively run time, design time, and requirements, but their mitigation might be done at the same or a different stage. Based on the related literature and our preliminary investigations, we argue that the following three overarching techniques are essential and warrant further research to provide enabling technologies to address uncertainty at all three stages: model-based development, assurance, and dynamic adaptation. Furthermore, we posit that in order to go beyond incremental improvements to current software engineering techniques, we need to leverage, extend, and integrate techniques from other disciplines. © 2013 IEEE.
SCOPUS;2013;Requirements and architectures for adaptive systems;Adaptation control mechanisms,, Adaptive systems,, Architectural design,, Requirements;The growing interest in developing adaptive systems has led to numerous proposals for approaches aimed at supporting the development of such systems. Some approaches define adaptation mechanisms in terms of architectural designs, consisting of concepts such as components, connectors and states. Other approaches are requirements-based, thus concerned with goals, tasks, contexts and preferences as concepts in terms of which adaptation is defined. By considering only a partial view of software systems (either the problem space or the solution space), such proposals are limited in specifying the adaptive behavior of a software system. In this paper we present ongoing work towards deriving architectural models in order to support the design and runtime execution of software adaptation both at a requirements and architectural level.
SCOPUS;2013;Business process configuration with NFRs and context-awareness;Business Process Configuration,, Context-Awareness,, Non-Functional Requirements;[Context] Business process models are an important source of information for the development of information systems. Good business processes need to be up-to-date and automated to represent the organizational environment. Representing and configuring business processes variability for a specific organization allows the proper execution of processes. In addition, dynamic environment calls for exible configuration processes that can meet stakeholders' goals. [Question/Problem] Even though current approaches allow the representation of variability of business process models, the selection of business variants in a given context remains a challenging issue. [Main idea] In this proposal, we advocate the use of Non-Functional Requirements (NFR) and contextawareness information to drive the configuration of process models at run-time. In particular, we evaluate the use of NFRs to describe the stakeholders' preferences. [Contribution] We propose a model-driven business process configuration approach that is driven by NFRs and contextual information.
SCOPUS;2013;Satisficing-based approach to resolve feature interactions in control systems;;To handle the complexity of modern control systems there is an urgent need to develop features as independently developed units of extension. However, when independently developed features are later composed they become coupled through the shared environment resources. As a consequence, the system requirements may no longer be entailed when independent features try to control the same shared environment. Malfunctioning behavior as a consequence of feature interference is know in the literature as the feature interaction problem. This paper present an approach that uses designtime specification of independent requirements, in combination with a runtime arbitrator that search for feature interaction free programs which entail the system requirements. In case of conflicting requirements that can't be satisfied simultaneously, the mechanism supports explanation of the interactions as a context sharing problem. We demonstrate our approach in a real-life control system for industrial pot plant cultivation in greenhouses and show that solutions are found for compatible requirements and that conflicts are identified and explained for incompatible requirements. © 2013 Springer Science+Business Media.
SCOPUS;2013;Using requirements models at runtime: Potential and challenges [Usando Modelos de Requisitos em Tempo de Execução: Potencial e Desafios];;[No abstract available]
SCOPUS;2013;Active and adaptive services resource provisioning with personalized customization;ATOM,, personalized customization,, requirements engineering,, services resource provisioning;Software as a service(SaaS), we are moving to the age of service-oriented software engineering(SOSE). But for the goal of services computing, namely on-demand service, it has not been able to achieved by far, especially the active provisioning approach for services resource. In view of these facts of services aggregation, i.e. The relative deficient services resource, single provision structure and passive selection mode for service requesters. Active provisioning of services resource with personalized customization will be focused. In this paper, we have established software architecture with personalized active custom for services resource. Customization of the active provisioning of services resource mainly includes two aspects: Firstly, for unmatched services resource of service composition, sliced or segmentation method should be chosen to acquire the individual needs of services resource according to the overall requirements. Secondly, a huge amount of legacy software will be comprehensively reused, namely servicelization. Through personalized customization of services resource, it will achieve on-demand active provisioning to furnish adequate material for dynamic services aggregation. It will also provide the production framework, process guidance and engineering support of CASE tools for services resource provisioning in runtime based on personalized customization. The feasibility and efficiency of the proposed approach are verified by a series of experiments. © 2013 IEEE.
SCOPUS;2012;Mitigating the obsolescence of quality specifications models in service-based systems;dynamically adaptive systems,, model@runtime,, Quality of Service,, requirements model,, Requirements-awareness,, service-based systems;Requirements-aware systems address the need to reason about uncertainty at runtime to support adaptation decisions, by representing quality of services (QoS) requirements for service-based systems (SBS) with precise values in run-time queryable model specification. However, current approaches do not support updating of the specification to reflect changes in the service market, like newly available services or improved QoS of existing ones. Thus, even if the specification models reflect design-time acceptable requirements they may become obsolete and miss opportunities for system improvement by self-adaptation. This articles proposes to distinguish "abstract" and "concrete" specification models: the former consists of linguistic variables (e.g. "fast") agreed upon at design time, and the latter consists of precise numeric values (e.g. "2ms") that are dynamically calculated at run-time, thus incorporating up-to-date QoS information. If and when freshly calculated concrete specifications are not satisfied anymore by the current service configuration, an adaptation is triggered. The approach was validated using four simulated SBS that use services from a previously published, real-world dataset,, in all cases, the system was able to detect unsatisfied requirements at run-time and trigger suitable adaptations. Ongoing work focuses on policies to determine recalculation of specifications. This approach will allow engineers to build SBS that can be protected against market-caused obsolescence of their requirements specifications. © 2012 IEEE.
SCOPUS;2012;From use cases and their relationships to code;;Use cases are used in many methodologies to drive the software engineering process. Though, their transition to code was usually a mostly manual process. In the context of MDD, use cases gain attention as first-class artifacts with representation notations allowing for automatic transformations to analysis and design models. The paper concentrates on an important problem of constructing transformations that cater for use case relationships. It presents a notation that unifies the ambiguous "include" and "extend", and allows for representing them within textual use case scenarios. This notation, equipped with runtime semantics, is used to construct a direct transformation into working code. The code is placed within method bodies of the Controller/Presenter and View layers within the MVC/MVP framework. Based on this transformation, an agile use-case-driven development process is possible. © 2012 IEEE.
SCOPUS;2012;SecuriTAS: A tool for engineering adaptive security;adaptive software,, dynamic access control,, goals,, security;This paper presents SecuriTAS, a tool to engineer adaptive security. It allows software designers to model security concerns together with the requirements of a system. This model is then used at runtime to analyze changes in security concerns and select the best set of security controls necessary to protect the system. © 2012 Authors.
SCOPUS;2012;A high-rate, low-power, ASIC speech decoder using finite state transducers;ASIC,, finite state transducer,, hardware,, speech recognition;The use of Finite State Transducers in speech recognition has been increasing in recent years. Their application in speech decoding allows for a tradeoff between larger memory requirements and less run-time computation. We believe that this paradigm is especially well suited for a highspeed, energy-efficient hardware solution where customized caching, reduced bit widths, and prefetching can be used to mitigate the effect of the increased model size. We present a virtual silicon prototype for a novel hardware architecture that, using these optimizations and running at 556MHz, is capable of performing recognition on the Wall Street Journal 60K- word speech model with 92.3 percent accuracy a speed 127 times faster than real time, while consuming less than 0.5 watts. © 2012 IEEE.
SCOPUS;2012;Run-time model evaluation for requirements model-driven self-adaptation;model-driven,, run-time requirements,, self-adaptation,, self-adaptive;A self-adaptive system adjusts its configuration to tolerate changes in its operating environment. To date, requirements modeling methodologies for self-adaptive systems have necessitated analysis of all potential system configurations, and the circumstances under which each is to be adopted. We argue that, by explicitly capturing and modelling uncertainty in the operating environment, and by verifying and analysing this model at runtime, it is possible for a system to adapt to tolerate some conditions that were not fully considered at design time. We showcase in this paper our tools and research results. © 2012 IEEE.
SCOPUS;2012;Requirements-driven adaptive security: Protecting variable assets at runtime;Adaptation,, Causal reasoning,, Security requirements;Security is primarily concerned with protecting assets from harm. Identifying and evaluating assets are therefore key activities in any security engineering process from modeling threats and attacks, discovering existing vulnerabilities, to selecting appropriate countermeasures. However, despite their crucial role, assets are often neglected during the development of secure software systems. Indeed, many systems are designed with fixed security boundaries and assumptions, without the possibility to adapt when assets change unexpectedly, new threats arise, or undiscovered vulnerabilities are revealed. To handle such changes, systems must be capable of dynamically enabling different security countermeasures. This paper promotes assets as first-class entities in engineering secure software systems. An asset model is related to requirements, expressed through a goal model, and the objectives of an attacker, expressed through a threat model. These models are then used as input to build a causal network to analyze system security in different situations, and to enable, when necessary, a set of countermeasures to mitigate security threats. The causal network is conceived as a runtime entity that tracks relevant changes that may arise at runtime, and enables a new set of countermeasures. We illustrate and evaluate our proposed approach by applying it to a substantive example concerned with security of mobile phones. © 2012 IEEE.
SCOPUS;2012;Stateful requirements monitoring for self-repairing socio-technical systems;goal models,, requirements monitoring,, self-repair;Socio-technical systems consist of human, hardware and software components that work in tandem to fulfill stakeholder requirements. By their very nature, such systems operate under uncertainty as components fail, humans act in unpredictable ways, and the environment of the system changes. Self-repair refers to the ability of such systems to restore fulfillment of their requirements by relying on monitoring, reasoning, and diagnosing on the current state of individual requirements. Self-repair is complicated by the multi-agent nature of socio-technical systems, which demands that requirements monitoring and self-repair be done in a decentralized fashion. In this paper, we propose a stateful requirements monitoring approach by maintaining an instance of a state machine for each requirement, represented as a goal, with runtime monitoring and compensation capabilities. By managing the interactions between the state machines, our approach supports hierarchical goal reasoning in both upward and downward directions. We have implemented a customizable Java framework that supports experimentation by simulating a socio-technical system. Results from our experiments suggest effective and precise support for a wide range of self-repairing decisions in a socio-technical setting. © 2012 IEEE.
SCOPUS;2012;Privacy arguments: Analysing selective disclosure requirements for mobile applications;mobile applications,, privacy arguments,, privacy requirements;Privacy requirements for mobile applications offer a distinct set of challenges for requirements engineering. First, they are highly dynamic, changing over time and locations, and across the different roles of agents involved and the kinds of information that may be disclosed. Second, although some general privacy requirements can be elicited a priori, users often refine them at runtime as they interact with the system and its environment. Selectively disclosing information to appropriate agents is therefore a key privacy management challenge, requiring carefully formulated privacy requirements amenable to systematic reasoning. In this paper, we introduce privacy arguments as a means of analysing privacy requirements in general and selective disclosure requirements (that are both content- and context-sensitive) in particular. Privacy arguments allow individual users to express personal preferences, which are then used to reason about privacy for each user under different contexts. At runtime, these arguments provide a way to reason about requirements satisfaction and diagnosis. Our proposed approach is demonstrated and evaluated using the privacy requirements of BuddyTracker, a mobile application we developed as part of our overall research programme. © 2012 IEEE.
SCOPUS;2012;Market-aware requirements;Computing with words,, Fuzzy c-means,, Fuzzy sets,, Non-functional requirements;Traditionally, non-functional requirements (NFRs) are specified as measurable entities to permit evaluation satisfaction,, however, NFR specifications quickly become obsolete because (1) NFRs are expressed in numbers, (2) architects specify them using the correct values at design time, and/or (3) providers are constantly improving their offer, in terms of functionality and quality of service (QoS). The computing-with-words approach has already been proposed to replace numerical NFR specifications, where natural language words denote fuzzy quality levels,, unfortunately, current proposals provide only for design-time, stakeholder-defined translation of words as numerical ranges. We propose a mechanism to automatically and dynamically determine current numerical ranges of the fuzzy quality levels from the available data, without human intervention, whenever changes to component QoS specifications. Our main contribution is allowing architects to specify their requirements using words only once (at design time), and whenever providers change components QoS characteristics, automatically update those requirements to the new market view, enabling market-aware requirements. The approach was validated by measuring the number of times that necessarily a requirement had to be rewritten at runtime in order to get new operationalizations which replace the now older ones. We use a set of ten complex requirements, a dataset of 1500 actual Web services with precise measurements for nine QoS aspects, and a simulated offering variability. A Web-based prototype is also made available.
SCOPUS;2012;Requirements-driven qualitative adaptation;adaptive systems,, feedback loops,, goal models,, qualitative reasoning,, requirements;Coping with run-time uncertainty pose an ever-present threat to the fulfillment of requirements for most software systems (embedded, robotic, socio-technical, etc.). This is particularly true for large-scale, cooperative information systems. Adaptation mechanisms constitute a general solution to this problem, consisting of a feedback loop that monitors the environment and compensates for deviating system behavior. In our research, we apply a requirements engineering perspective to the problem of designing adaptive systems, focusing on developing a qualitative software-centric, feedback loop mechanism as the architecture that operationalizes adaptivity. In this paper, we propose a framework that provides qualitative adaptation to target systems based on information from their requirements models. The key characteristc of this framework is extensibility, allowing for it to cope with qualitative information about the impact of control (input) variables on indicators (output variables) in different levels of precision. Our proposal is evaluated with a variant of the London Ambulance System case study. © 2012 Springer-Verlag.
SCOPUS;2012;Proceedings of the 7th Workshop on Models@run.time, MRT 2012 - Being Part of the ACM/IEEE 15th International Conference on Model Driven Engineering Languages and Systems, MODELS 2012;;The proceedings contain 12 papers. The topics discussed include: goal models as run-time entities in context-aware systems,, model-driven development of DSML execution engines,, extracting user requirements for pervasive service compositions,, rapid GUI development on legacy systems: an runtime model-based solution,, a process for continuous validation of self-adapting component based systems,, a process for continuous validation of self-adapting component based systems,, towards supporting multiple execution environments for UML/OCL models at runtime,, a runtime model for fUML,, model execution adaptation?,, a semi-automatic behavioral mediation approach based on models@runtime,, and expressing model relations as basis for structural consistency analysis in models@run.time.
SCOPUS;2012;The research of software requirements analysis of core needs;Circulatory Iteration,, Filed Modeling,, Test First;Software requirements should be divided into functional requirements and non functional requirements, including run-time quality attributes, develop-time quality attributes and constraints in software engineering, the rate of software rework keeps high because of human factors, fail to grasp the key factors.In the practical point of view, the six key steps can be proposed in the needs analysis study to resist development rework, increase success rate of software development. © 2012 SPIE.
SCOPUS;2012;Goal models as run-time entities in context-aware systems;context-aware systems,, goals,, requirements,, rule-based reasoning,, scenarios,, user requirements notation,, workflow;The strength of goal models is their ability to assess candidate solutions against high level criteria for many stakeholders, allowing system-wide trade-offs to be performed. We argue that, in a context-aware system, reasoning based on goal models can complement standard rule-based reasoning engines for decision making without involving explicit interaction with the user. While rule-based systems excel in filtering out unsuitable solutions based on clear criteria, it is difficult to rank suitable solutions based on vague, qualitative criteria of stakeholders with a rule-based approach. The User Requirements Notation (URN) is a goal-based and scenario-based requirements modeling language that has been applied to many different domains, from reactive systems to telecommunication standards to business processes. For context-aware systems, URN's workflow notation can describe the overall behavior of a context-aware system and URN's goal models can further enhance reasoning about contextual situations. While URN already supports some of the interactions between workflow and goal models required for the specification of context-aware systems, it does not yet fully support the modeling, design-time simulation, and run-time execution of a context-aware system based on its URN model. This paper (i) introduces such a modeling, simulation, and execution environment, (ii) discusses three architectural solutions for combined rule-based and goal-oriented reasoning, and (iii) reports on a URN profile that describes a domain-specific language for context-aware reasoning using goal-orientation with the help of an example application from the health care domain. © 2012 ACM.
SCOPUS;2012;Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop;Earned business value,, Feedback control theory,, Goal-oriented reasoning,, Preference,, Self-tuning;Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods. © 2012 Elsevier Inc. All rights reserved.
SCOPUS;2012;Accelerating throughput-aware runtime mapping for heterogeneous MPSoCs;Design-space exploration,, Embedded systems,, Energy consumption,, Multimedia applications,, Multiprocessor systems-on-chip,, Runtime mapping,, Synchronous data-flow graphs,, Throughput;Modern embedded systems need to support multiple time-constrained multimedia applications that often employ multiprocessor-systems-on-chip (MPSoCs). Such systems need to be optimized for resource usage and energy consumption. It is well understood that a design-time approach cannot provide timing guarantees for all the applications due to its inability to cater for dynamism in applications. However, a runtime approach consumes large computation requirements at runtime and hence may not lend well to constrained-aware mapping. In this article, we present a hybrid approach for efficient mapping of applications in such systems. For each application to be supported in the system, the approach performs extensive design-space exploration (DSE) at design time to derive multiple design points representing throughput and energy consumption at different resource combinations. One of these points is selected at runtime efficiently, depending upon the desired throughput while optimizing for energy consumption and resource usage. While most of the existing DSE strategies consider a fixed multiprocessor platform architecture, our DSE considers a generic architecture, making DSE results applicable to any target platform. All the compute-intensive analysis is performed during DSE, which leaves for minimum computation at runtime. The approach is capable of handling dynamism in applications by considering their runtime aspects and providing timing guarantees. The presented approach is used to carry out a DSE case study for models of real-life multimedia applications: H.263 decoder, H.263 encoder, MPEG-4 decoder, JPEG decoder, sample rate converter, and MP3 decoder. At runtime, the design points are used to map the applications on a heterogeneous MPSoC. Experimental results reveal that the proposed approach provides faster DSE, better design points, and efficient runtime mapping when compared to other approaches. In particular, we show that DSE is faster by 83% and runtime mapping is accelerated by 93% for some cases. Further, we study the scalability of the approach by considering applications with large numbers of tasks. © 2012 ACM.
SCOPUS;2012;Automated testing of Web Services system based on OWL-S;automated testing,, mutant,, OWL-S Requirement Model,, specific test case generation,, Web Services system;As Web Services becomes mature and popular, they are always integrated together, forming systems to carry out coherent tasks. The distributed application of Web Services involves many standard protocols and various runtime behaviors and thus makes the systems' automated testing more difficult. In this paper we propose a series of applicable automated testing methods for Web Services system. First, deduce abstract test cases from interaction requirement properties of Web Services system. Second, specify test cases according to SWRL (Semantic Web Rule Language) properties and abstract test cases. Finally, generate mutants under AOP (Aspect-Oriented Programming) technology support, drive them by specific test cases using improved Fit (Framework for Integrated Test), and then kill mutants based on business logic. Experiments have shown that our algorithms meet the applied demands and perform well as an automated testing tool for Web Services system. © 2012 IEEE.
SCOPUS;2012;Proceedings - 2012 8th International Conference on the Quality of Information and Communications Technology, QUATIC 2012;;The proceedings contain 61 papers. The topics discussed include: quality-aware mashup composition: issues, techniques and tools,, using web quality models and questionnaires for web applications understanding and evaluation,, automatic event detection for software product quality monitoring,, a runtime quality measurement framework for cloud database service systems,, investigating the impact of personality and temperament traits on pair programming: a controlled experiment replication,, a metamodel-based approach for customizing and assessing agile methods,, model-driven development for requirements engineering: the case of goal-oriented approaches,, complex events specification for properties validation,, integrating test and risk management,, coordinating exceptions of java systems: implementation and formal verification,, using association rules to identify similarities between software datasets,, and developing a process assessment model for technological and business competencies on software development.
SCOPUS;2012;Towards dynamic evolution of self-Adaptive systems based on dynamic updating of control loops;control loops,, design,, dynamic evolution,, goal-oriented requirements modeling,, programming framework,, self-Adaptation;Self-Adaptive systems, which enable runtime adaptation, are promising ways of dealing with environmental changes, including system intrusions or faults. Such software systems must modify themselves to better fit their environment. One of the main approaches to constructing such systems is to introduce multiple control loops. Software evolution is an essential activity for expanding this adaptation capability, and dynamic evolution has been envisaged as a way of systems adapting themselves at runtime. In this paper, we establish a development process to deal with dynamic evolution. We devise a goal model compiler to generate models for designing dynamic evolutions and a programming framework that supports dynamic deployment of control loops. We experimentally applied our approach to a system and discuss how our compiler and framework support dynamic evolution of self-Adaptive systems. © 2012 IEEE.
SCOPUS;2012;An ontology-based data acquisition infrastructure using ontologies to create domain-independent software systems;Data acquisition,, Meta modelling,, Ontology;We created an ontology-based data acquisition infrastructure which is able to store data of almost arbitrary structure and can be set up for a certain domain of application within hours. An ontology editor helps the domain expert to define and maintain the domain specific ontology. Based on the user-defined ontology, a web-based data acquisition system and an ETL data import interface are automatically created at runtime. Furthermore, rules for semantic data plausibility can be established in the ontology to provide semantic data quality for subsequent processing of the collected data. After a comprehensive requirement analysis we decided to use a special meta model instead of standard OWL ontologies. In this paper, we describe our metamodel and the reason for not using OWL in our case in detail as well as we present the infrastructure and the project it is currently used for.
SCOPUS;2012;Dynamic phase-based tuning for embedded systems using phase distance mapping;Cache tuning,, configurable architectures,, configurable hardware,, dynamic reconfiguration,, energy savings,, phase-based tuning;Phase-based tuning specializes a system's tunable parameters to the varying runtime requirements of an application's different phases of execution to meet optimization goals. Since the design space for tunable systems can be very large, one of the major challenges in phase-based tuning is determining the best configuration for each phase without incurring significant tuning overhead (e.g., energy and/or performance) during design space exploration. In this paper, we propose phase distance mapping, which directly determines the best configuration for a phase, thereby eliminating design space exploration. Phase distance mapping applies the correlation between a known phase's characteristics and best configuration to determine a new phase's best configuration based on the new phase's characteristics. Experimental results verify that our phase distance mapping approach determines configurations within 3% of the optimal configurations on average and yields an energy delay product savings of 26% on average. © 2012 IEEE.
SCOPUS;2012;A bounded error, anytime, parallel algorithm for exact Bayesian network structure learning;;Bayesian network structure learning is NP-hard. Several anytime structure learning algorithms have been proposed which guarantee to learn optimal networks if given enough resources. In this paper, we describe a general purpose, anytime search algorithm with bounded error that also guarantees optimality. We give an efficient, sparse representation of a key data structure for structure learning. Empirical results show our algorithm often finds better networks more quickly than state of the art methods. They also highlight accepting a small, bounded amount of suboptimality can reduce the memory and runtime requirements of structure learning by several orders of magnitude.
SCOPUS;2012;Scenario-driven development of context-aware adaptive web services;Context-awareness,, Requirements Elicitation,, Scenario-driven Development,, Self-adaptation,, Web Services;Context-awareness and adaptability are highly desirable features for web services that operate in dynamic environments. In recent years, a number of approaches have been proposed to support the development of such services. However, the requirements elicitation of this kind of services and the synthesis of their design models from the requirements are still major challenges. In this paper, we propose a novel scenario-driven approach to developing context-aware adaptive web services. Our approach enables the elicitation of a web service's requirements as two sets of scenarios: functional and adaptation. The functional scenarios capture the service's functionality while the adaptation scenarios represent the service's adaptation logic to cope with runtime context changes. We also support the synthesis of the service's design model from its scenarios, and the automatic transformation from the service's design model to the executable service code. To demonstrate the applicability of our approach, we have used it to develop a context-aware travel guide service. © 2012 Springer-Verlag.
SCOPUS;2012;Transparent structural online test for reconfigurable systems;FPGA,, Online Test,, Reconfigurable Architectures;FPGA-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. However, the reliability of modern FPGAs is threatened by latent defects and aging effects. Hence, it is mandatory to ensure the reliable operation of the FPGA's reconfigurable fabric. This can be achieved by periodic or on-demand online testing. In this paper, a system-integrated, transparent structural online test method for runtime reconfigurable systems is proposed. The required tests are scheduled like functional workloads, and thorough optimizations of the test overhead reduce the performance impact. The proposed scheme has been implemented on a reconfigurable system. The results demonstrate that thorough testing of the reconfigurable fabric can be achieved at negligible performance impact on the application. © 2012 IEEE.
SCOPUS;2012;Approaches for mastering change;;Modern software systems are highly configurable and exist in many different variants in order to operate different application contexts. This is called static variability and predominantly considered in software product line engineering [6,14]. Furthermore, software systems have to evolve over time in order to dealwith changing requirements which is referred to by the term temporal evolvability [10,13]. Additionally, modern software systems are designed to dynamically adapt their internal structure and behavior at runtime dependent on their environment in order to efficiently use the available resources, such as energy or computing power [5]. These three dimensions of change, static variability, temporal evolvability and dynamic adaptation, increase the complexity of system development in all phase, from requirements engineering and system design to implementation and quality assurance. In [15], the challenges of static variability and temporal evolution in all phases of the software development process are discussed. In [15], the engineering challenges of self-adaptive systems are described and future research directions are pointed out. © 2012 Springer-Verlag.
SCOPUS;2012;Relaxing claims: Coping with uncertainty while evaluating assumptions at run time;;Self-adaptation enables software systems to respond to changing environmental contexts that may not be fully understood at design time. Designing a dynamically adaptive system (DAS) to cope with this uncertainty is challenging, as it is impractical during requirements analysis and design time to anticipate every environmental condition that the DAS may encounter. Previously, the RELAX language was proposed to make requirements more tolerant to environmental uncertainty, and Claims were applied as markers of uncertainty that document how design assumptions affect goals. This paper integrates these two techniques in order to assess the validity of Claims at run time while tolerating minor and unanticipated environmental conditions that can trigger adaptations. We apply the proposed approach to the dynamic reconfiguration of a remote data mirroring network that must diffuse data while minimizing costs and exposure to data loss. Results show RELAXing Claims enables a DAS to reduce adaptation costs. © 2012 Springer-Verlag.
SCOPUS;2012;An eclipse modelling framework alternative to meet the Models@Runtime requirements;adaptation,, EMF,, Model@Runtime;Models@Runtime aims at taming the complexity of software dynamic adaptation by pushing further the idea of reflection and considering the reflection layer as a first-class modeling space. A natural approach to Models@Runtime is to use MDE techniques, in particular those based on the Eclipse Modeling Framework. EMF provides facilities for building DSLs and tools based on a structured data model, with tight integration with the Eclipse IDE. EMF has rapidly become the defacto standard in the MDE community and has also been adopted for building Models@Runtime platforms. For example, Frascati (implementing the Service Component Architecture standard) uses EMF for the design and runtime tooling of its architecture description language. However, EMF has primarily been thought to support design-time activities. This paper highlights specific Models@Runtime requirements, discusses the benefits and limitations of EMF in this context, and presents an alternative implementation to meet these requirements. © 2012 Springer-Verlag.
SCOPUS;2012;The multi-processor scheduling problem in phylogenetics;phylogenetics,, RAxML-Light,, scheduling;Advances in wet-lab sequencing techniques allow for sequencing between 100 genomes up to 1000 full transcriptomes of species whose evolutionary relationships shall be disentangled by means of phylogenetic analyses. Likelihood-based evolutionary models allow for partitioning such broad phylogenomic datasets, for instance into gene regions, for which likelihood model parameters (except for the tree itself) can be estimated independently. Present day phylogenomic datasets are typically split up into 1000-10,000 distinct partitions. While the likelihood on such datasets needs to be computed in parallel because of the high memory requirements, it has not yet been assessed how to optimally distribute partitions and/or alignment sites to processors, in particular when the number of cores is significantly smaller than the number of partitions. We find that, by distributing partitions (of varying lengths) monolithically to processors, the induced load distribution problem essentially corresponds to the well-known multiprocessor scheduling problem. By implementing the simple Longest Processing Time (LPT) heuristics in the PThreads and MPI version of RAxML-Light, we were able to accelerate run times by up to one order of magnitude. Other heuristics for multi-processor scheduling such as improved MultiFit, improved Zero-One, or the Three Phase approach did not yield notable performance improvements. © 2012 IEEE.
SCOPUS;2012;Uncertainty modeling of self-adaptive software requirement;Business process modeling,, Context snapshot,, Goal modeling,, Requirement evolution;Service oriented computing utilizes services as fundamental elements for developing applications that have the capability to autonomously modify their behavior at run-time in response to the changes in their environment, which is especially suitable for designing and developing self-adaptive software. While uncertainty induced by randomness environment in service oriented self-adaptive software requirement is a well-studied activity, representing and analyzing uncertainty have not enjoyed equal attention. In this paper, we address this problem by amalgamating context snapshot with goal and business process model to support the representation of uncertainty for self-adaptive software requirements. We define a context snapshot model to represent requirement uncertainty with domain knowledge,, context-specific goal-oriented requirement model is constructed for customer requirements and context-specific process-oriented requirement model is constructed for service requirements,, and finally, propose means-c-end analysis to relate the customer requirements and service requirements with context condition. We illustrate and evaluate our approach through a case study about a city intelligent traffic information system.
SCOPUS;2012;DRF4SOA: A dynamic reconfigurable framework for designing autonomic application based on SOA;Autonomic computing,, Dynamic Reconfiguration,, Quality of Service,, Service Component Architecture,, Service Oriented Architecture;Service Oriented Architecture (SOA) allows modeling dynamic interaction between heterogeneous providers. Coupling with Service Component Architecture (SCA) enables governing the development of complex applications. The dynamic reconfiguration of such applications is a key feature, since it gives measures to ensure runtime adaptation in order to guarantee Quality of Service (QoS) and manage the performance. For instance, recovering a QoS degradation requires the identification of its sources and the capacity of reconfiguration planning and execution. This paper presents DRF4SOA, a Dynamic Reconfigurable Framework to design autonomic application based on SOA, that implements the autonomic control loop phases (Monitoring, Analysis, Planning, Execution) with SCA in order to provide flexibility to support evolving of itself and to include new non-functional requirements at runtime. © 2012 IEEE.
SCOPUS;2012;OTERA: Online test strategies for reliable reconfigurable architectures - Invited paper for the AHS-2012 special session "dependability by reconfigurable hardware";;FPGA-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. However, the reliability of FPGAs, which are manufactured in latest technologies, is threatened not only by soft errors, but also by aging effects and latent defects. To ensure reliable reconfiguration, it is mandatory to guarantee the correct operation of the underlying reconfigurable fabric. This can be achieved by periodic or on-demand online testing. The OTERA project develops and evaluates components and strategies for reconfigurable systems that feature reliable reconfiguration. The research focus ranges from structural online tests for the FPGA infrastructure and functional online tests for the configured functionality up to the resource management and test scheduling. This paper gives an overview of the project tasks and presents first results. © 2012 IEEE.
SCOPUS;2012;Identifying test requirements by analyzing SLA guarantee terms;Service Level Agreements,, Service Monitoring,, Software Testing,, Test Requirements;Service Level Agreements (SLAs) are used to specify the negotiated conditions between the provider and the consumer of services. In this paper we present a stepwise method to identify and categorize a set of test requirements that represent the potential situations that can be exercised regarding the specification of each isolated guarantee term of an SLA. This identification is addressed by means of devising a set of coverage levels that allow grading the thoroughness of the tests. The utilization of these test requirements would focus on twofold objectives: (1) the generation of a test suite that allows exercising the situations described in the test requirements and (2) the support for the derivation of a monitoring plan that checks the compliance of these requirements at runtime. The approach is illustrated over an eHealth case study. © 2012 IEEE.
SCOPUS;2012;(Requirement) evolution requirements for adaptive systems;adaptive systems,, evolution,, modeling,, requirements,, Requirements engineering;It is often the case that stakeholders want to strengthen/weaken or otherwise change their requirements for a system-to-be when certain conditions apply at runtime. For example, stakeholders may decide that if requirement R is violated more than N times in a week, it should be relaxed to a less demanding one R-. Such evolution requirements play an important role in the lifetime of a software system in that they define possible changes to requirements, along with the conditions under which these changes apply. In this paper we focus on this family of requirements, how to model them and how to operationalize them at runtime. In addition, we evaluate our proposal with a case study adopted from the literature. © 2012 IEEE.
SCOPUS;2012;A taxonomy of uncertainty for dynamically adaptive systems;design,, Dynamically adaptive systems,, requirements engineering,, runtime,, taxonomy,, uncertainty;Self-reconfiguration enables a dynamically adaptive system (DAS) to satisfy requirements even as detrimental system and environmental conditions arise. A DAS, especially one intertwined with physical elements, must increasingly reason about and cope with unpredictable events in its execution environment. Unfortunately, it is often infeasible for a human to exhaustively explore, anticipate, or resolve all possible system and environmental conditions that a DAS will encounter as it executes. While uncertainty can be difficult to define, its effects can hinder the adaptation capabilities of a DAS. The concept of uncertainty has been extensively explored by other scientific disciplines, such as economics, physics, and psychology. As such, the software engineering DAS community can benefit from leveraging, reusing, and refining such knowledge for developing a DAS. By synthesizing uncertainty concepts from other disciplines, this paper revisits the concept of uncertainty from the perspective of a DAS, proposes a taxonomy of potential sources of uncertainty at the requirements, design, and execution phases, and identifies existing techniques for mitigating specific types of uncertainty. This paper also introduces a template for describing different types of uncertainty, including fields such as source, occurrence, impact, and mitigating strategies. We use this template to describe each type of uncertainty and illustrate the uncertainty source in terms of an example DAS application from the intelligent vehicle systems (IVS) domain. © 2012 IEEE.
SCOPUS;2012;A reinforcement learning based hybrid evolutionary algorithm for ship stability design;;Over the past decades, various search and optimisation methods have been used for ship design - a dynamic and complicated process. While several advantages of using these methods have been demonstrated, one of the main limiting factors of optimisation applications in ship design is the high runtime requirement of the involved simulations. This severely restricts the number of real applications in this area. This chapter presents a hybrid evolutionary algorithm that uses reinforcement learning to guide the search. Through giving and correcting the search direction, the runtime of optimisation can be effectively reduced. The NSGA-II, a well known multi-objective evolutionary algorithm, is utilised together with reinforcement learning to form the hybrid approach. As an important optimisation application field, the ship stability design problem has been selected for evaluating the performance of this new method. A Ropax (roll on/roll off passenger ship) damage stability problem is selected as a case study to demonstrate the effectiveness of the proposed approach. © 2012 Springer-Verlag Berlin Heidelberg. All rights are reserved.
SCOPUS;2012;A requirements-based approach for the design of adaptive systems;;Complexity is now one of the major challenges for the IT industry [1]. Systems might become too complex to be managed by humans and, thus, will have to be self-managed: Self-configure themselves for operation, self-protect from attacks, self-heal from errors and self-tune for optimal performance [2]. (Self-)Adaptive systems evaluate their own behavior and change it when the evaluation indicates that it is not accomplishing the software's purpose or when better functionality and performance are possible [3]. To that end, we need to monitor the behavior of the running system and compare it to an explicit formulation of requirements and domain assumptions [4]. Feedback loops (e.g., the MAPE loop [2]) constitute an architectural solution for this and, as proposed by past research [5], should be a first class citizen in the design of such systems. We advocate that adaptive systems should be designed this way from as early as Requirements Engineering and that reasoning over requirements is fundamental for run-time adaptation. We therefore propose an approach for the design of adaptive systems based on requirements and inspired in control theory [6]. Our proposal is goal-oriented and targets softwareintensive socio-technical systems [7], in an attempt to integrate control-loop approaches with decentralized agents inspired approaches [8]. Our final objective is a set of extensions to state-of-the-art goal-oriented modeling languages that allow practitioners to clearly specify the requirements of adaptive systems and a run-time framework that helps developers implement such requirements. In this 2-page abstract paper, we summarize this approach. © 2012 IEEE.
SCOPUS;2012;Towards smart user interface design;automated data visualization,, code inspection,, model-driven development,, User interface development;User interface development and maintenance presents a burden for many developers. UI development approaches often restate information already captured in the application model such as entity attributes, validation, security, etc. Changes in application model often require many subsequent changes to the UI. Such duplication creates additional maintenance requirements for synchronization (at a minimum) and often is a source for errors (i.e., when model and UI disagree). Adding to the difficulties of creation and maintenance, typical UI implementations often tangle multiple concerns together such as presentation, validation, layout, security, etc. In this paper, we provide an approach that reduces information duplication and untangles mixed concerns. The capability of runtime UI generation can render user-specific UI, reduce conditional evaluation, and integrate third party security frameworks. To evaluate our approach, we provide a case study that demonstrates reduction of maintenance efforts, separation of concerns and performance of runtime UI generation. © 2012 IEEE.
SCOPUS;2012;Intention-oriented modelling support for socio-technical driven elastic cloud applications;Cloud Applications,, GORE,, Intention,, Neptune;Businesses have already started to exploit potential uses of cloud computing as a new paradigm for promoting their services. Although the general concepts they practically focus on are: viability, survivability, adaptability, etc., however, there is still a lack of forming mechanisms to sustain viability with adaptation of new requirements in cloud-based applications. This has inspired a pressing need to adopt new methodologies and abstract models which support system acquisition for self-adaptation, thus guaranteeing autonomic cloud application behaviour. In this paper we use state-of-the-art Neptune framework as runtime adaptive software development environment developed mainly for distributed computing supported with intention-oriented modelling language. The representation and adaptation of goal based model artifacts and their intrinsic properties requirements will in turn support distributed service based applications running in the cloud. This harnesses obedient system behaviour with respect to its functional and non-functional characteristics. © 2012 IEEE.
SCOPUS;2012;Middleware for differentiated quality in spontaneous networks;middleware,, mobile applications,, mobile computing,, support services;Spontaneous-network management requires application-driven middleware to address differentiated application-specific requirements at runtime. Real Ad hoc Multihop Peer-to-peer (RAMP) middleware easily deploys over existing and heterogeneous wireless networks, supporting adaptive and per-application strategies even in challenging scenarios, such as multimedia streaming. © 2012 IEEE.
SCOPUS;2012;A framework for service-based business process collaboration;Agent-based system,, Behavioral compatibility,, Business process collaboration,, Software reliability;Nowadays, Web services-based systems have gained the attention of researchers, and it enables business process system to automatically discover and invocate suitable Web services at run time. Further with portable terminals, it has become more and more popular for mobile users to do their business management in modern business process system. This demands a new framework for system requirements elicitation and design in order to support this new application. In this paper, we introduce a framework for the elicitation of function and performance aware adaptation requirements. In this framework, we present rule for verifying and evaluating the behavior of the collaboration system, and the overall reliability of the system from its architecture. We also propose a process mining method to help reconstruct the architecture of business process from execution logs. An agent-based mechanism to support mobile users completing their remote tasks is also discussed. © 2011 by Binary Information Press.
SCOPUS;2012;Engineering Secure Software and Systems - 4th International Symposium, ESSoS 2012, Proceedings;;The proceedings contain 14 papers. The topics discussed include: application-replay attack on Java cards: when the garbage collector gets confused,, supporting the development and documentation of ISO 27001 information security management systems through security requirements engineering approaches,, typed assembler for a RISC crypto-processor,, transversal policy conflict detection,, challenges in implementing an end-to-end secure protocol for Java ME-based mobile data collection in low-budget settings,, runtime enforcement of information flow security in tree manipulating processes,, formalisation and implementation of the XACML access control mechanism,, a task ordering approach for automatic trust establishment,, an idea of an independent validation of vulnerability discovery models,, a sound decision procedure for the compositionality of secrecy,, and plagiarizing smartphone applications: attack strategies and defense techniques.
SCOPUS;2012;Towards a goal-driven approach to action selection in self-adaptive software;goal-driven model,, run-time action selection,, self-adaptive software;Self-adaptive software is a closed-loop system, since it continuously monitors its context (i.e. environment) and/or self (i.e. software entities) in order to adapt itself properly to changes. We believe that representing adaptation goals explicitly and tracing them at run-time are helpful in decision making for adaptation. While goal-driven models are used in requirements engineering, they have not been utilized systematically yet for run-time adaptation. To address this research gap, this article focuses on the deciding process in self-adaptive software, and proposes the Goal-Action-Attribute Model (GAAM). An action selection mechanism, based on cooperative decision making, is also proposed that uses GAAM to select the appropriate adaptation action(s). The emphasis is on building a light-weight and scalable run-time model which needs less design and tuning effort comparing with a typical rule-based approach. The GAAM and action selection mechanism are evaluated using a set of experiments on a simulated multi-tier enterprise application, and two sample ordinal and cardinal action preference lists. The evaluation is accomplished based on a systematic design of experiment and a detailed statistical analysis in order to investigate several research questions. The findings are promising, considering the obtained results, and other impacts of the approach on engineering self-adaptive software. Although, one case study is not enough to generalize the findings, and the proposed mechanism does not always outperform a typical rule-based approach, less effort, scalability, and flexibility of GAAM are remarkable. Copyright © 2011 John Wiley & Sons, Ltd.
SCOPUS;2012;Requirement uncertainty modeling for service oriented self-adaptive software;Context,, Requirement,, Self-adaptation,, Service oriented computing;Service-oriented software utilizes services as fundamental elements for developing applications that have the capability to autonomously modify their behavior at run-time in response to changes in their environment. While a few techniques have been developed to support the modeling and analysis of requirements for self-adaptive systems, limited attention has been paid to the description of service requirements and uncertainty in requirements of service-oriented software. In this paper, we propose a task solving strategy for requirement analysis and modeling framework as a fundamental of self-adaptation evolution. We introduce task solving strategy method for requirement analysis process,, a context snapshot model to represent uncertainty in requirement with domain knowledge,, goal-oriented context requirement to model user requirements and process-oriented context requirement to model service requirements,, and finally, propose means-c-end analysis to relate user and service requirement with context condition. © (2012) Trans Tech Publications, Switzerland.
SCOPUS;2012;Using RELAX, SysML and KAOS for ambient systems requirements modeling;Ambient Assisted Living,, Ambient Systems,, Domain Specific Language,, Dynamic Adaptive Systems,, Non Functional Requirements,, RELAX,, Requirements Engineering;Ambient Systems are highly adaptive. They modify their behavior at run-time in response to changing environmental conditions. For these systems, Non Functional Requirements (NFR's) play an important role, and one has to identify as early as possible the requirements that are adaptable. Because of the inherent uncertainty in these systems, goal based approaches can help in the development of their requirements. RELAX, which is a Requirement Engineering (RE) language for adaptive systems, can introduce flexibility in NFR's to adapt to any changing environmental conditions. We illustrate our proposal through a case study of an Ambient Assisted Living (AAL) system. We use an existing goal oriented approach, based on KAOS, which extends the SYSML1 meta-model and our proposed Domain Specific Language (DSL) for RELAX,, that enables to derive requirements in graphical format from textual requirements in the form of SYSML requirements diagrams. In this paper we show how we have integrated these two approaches for a better modeling of these systems. © 2012 Published by Elsevier Ltd.
SCOPUS;2012;Optimizing monitoring requirements in self-adaptive systems;Adaptation,, Mobility,, Monitoring Optimization,, Requirements;Monitoring the system environment is a key functionality of a self-adaptive system. Monitoring requirements denote the information a self-adaptive system has to capture at runtime to decide upon whether an adaptation action has to be taken. The identification of monitoring requirements is a complex task which can easily lead to redundancy and uselessness in the set of information to monitor and this, consequently, means unjustified instalment of monitoring infrastructure and extra processing time. In this paper, we study the optimization of monitoring requirements. We discuss the case of contextual goal model, which is a requirements model that weaves between variability of goals (functional and non-functional requirements) and variability of context (monitoring requirements) and is meant to be used for modelling mobile and self-adaptive systems requirements. We provide automated analysis -based on a SAT-solver- to process a contextual goal model and find a reduced set of contextual information to monitor guaranteeing that this reduction does not sacrifice the system ability of taking correct adaptation decisions when fulfilling its requirements. © 2012 Springer-Verlag Berlin Heidelberg.
SCOPUS;2012;Efficient resource management based on non- Functional requirements for sensor/actuator networks;Middleware,, Non-functional requirements,, Sensor/actuator network;In this paper, a novel resource management approach is presented for publish-subscribe middleware for sensor/actuator networks. The resource management was designed with the possibility to add non-functional requirements at runtime to subscription messages. This approach allows utilizing service level agreements that can then be employed in order to guarantee a certain quality of service or to reduce the energy consumption of a sensor node in a sensor/actuator network. As an example, a sensor/actuator network for facility logistics system (a conveyor belt system) is evaluated with respect to energy consumption. This sensor/actuator network is mostly controlled by image processing based sensor nodes. It is shown that an adaptive processing interval for these sensor nodes can reduce the energy consumption of the entire network. The utilization of non-functional requirements allows the system to adapt - After software development - To context changes such as the extension of the conveyor belt systems topology.
SCOPUS;2012;Service-driven migrating of enterprise information systems: A case study;information system,, requirements engineering,, service-oriented;There are many business information systems under operation supporting the essential business processes of large scale multi-national enterprises, which forms the essential IT assets of these organizations. However, the isolation of these systems, improper process configurations and unbalanced resource allocations often leads to the deterioration of these enterprises, or even major management crisis. Service-oriented computing is considered a possible remedy for such issues with relatively low migration cost that has attracted much industrial attention. In this paper, through a survey to an industrial consulting case from a Southeastern Asia Garment Manufacture, we discuss how service requirements are elicited, how to evolve conventional management information systems into composite service systems that is reconfigurable at runtime, so as to realize resources optimization and efficiency improvement. The purpose of this paper is to analyze the know how of migrating conventional management information systems into service-oriented architecture, from a requirements engineering perspective. © 2012 IEEE.
SCOPUS;2012;How to design and deliver process context sensitive information: Concept and prototype;Business Process Management,, Information Delivery,, Process Context Sensitivity;Providing employees with relevant, context-specific information is crucial to achieve productivity and efficiency while executing business processes. Today, tools exist to model various aspects of organizations such as processes, organization structures, services, and their descriptions. However, there still exists a gap between information modeling on a conceptual level and information provision on a runtime level that hinders information dissemination and retrieval while employees execute processes. In daily business life, information workers demand for unstructured content to fulfill well-defined process steps. In this paper, we adapt an existing conceptual approach of process-driven information requirements engineering and present its prototypical implementation based on an industry-developed BPM product. Our solution therefore introduces "information objects" and integrates these with process activities to model the users' information requirements at process runtime. In doing so, users are empowered to leverage context information such as documents, reports, or emails, while executing human steps in a process. © 2012 Springer-Verlag.
SCOPUS;2012;Value models for engineering of complex Sustainable Systems;Requirements engineering,, Sustainable Systems,, Value Network Models;Sustainable Systems are typically complex systems-of-systems spanning across several technical domains and organizations. They could include different kinds of sub-systems such as energy systems, information technology systems, transportation systems, buildings, etc. The organizations (e.g., companies and infrastructure providers) are involved in complex exchanges of goods, services and value. To make such systems sustainable but also economically feasible, it is important to analyze and understand these exchanges. We propose to integrate Value Network Models (VNMs) into analysis and design of complex sustainable systems. VNMs include, e.g., actors (typically organizations but also customers), value objects (goods, services, money), activities that generate value, and their dependencies. They should also include the information about the environmental impact and could be used in the context requirements engineering and high-level systems architecting. In addition, we propose to make VNMs available not only for design but also during system's operations using information and communication technology systems. Thus, exchange of value represented within VNMs could be monitored and evaluated during runtime. We show our approach on the example of Masdar City, a project of a planned city designed to be a sustainable, minimal emissions, and minimal waste urban environment. © 2012 Published by Elsevier Ltd.
SCOPUS;2011;A divide-and-conquer tabu search approach for online test paper generation;intelligent tutoring system,, multi-objective optimization,, Online test paper generation,, web-based testing;Online Test Paper Generation (Online-TPG) is a promising approach for Web-based testing and intelligent tutoring. It generates a test paper automatically online according to user specification based on multiple assessment criteria, and the generated test paper can then be attempted over the Web by user for self-assessment. Online-TPG is challenging as it is a multi-objective optimization problem on constraint satisfaction that is NP-hard, and it is also required to satisfy the online runtime requirement. The current techniques such as dynamic programming, tabu search, swarm intelligence and biologically inspired algorithms are ineffective for Online-TPG as these techniques generally require long runtime for generating good quality test papers. In this paper, we propose an efficient approach, called DAC-TS, which is based on the principle of constraint-based divide-and-conquer (DAC) and tabu search (TS) for constraint decomposition and multi-objective optimization for Online-TPG. Our empirical performance results have shown that the proposed DAC-TS approach has outperformed other techniques in terms of runtime and paper quality. © 2011 Springer-Verlag.
SCOPUS;2011;Run-time resolution of uncertainty;goals,, requirements models,, self adaptive systems;Requirements awareness should help optimize requirements satisfaction when factors that were uncertain at design time are resolved at runtime. We use the notion of claims to model assumptions that cannot be verified with confidence at design time. By monitoring claims at runtime, their veracity can be tested. If falsified, the effect of claim negation can be propagated to the system's goal model and an alternative means of goal realization selected automatically, allowing the dynamic adaptation of the system to the prevailing environmental context. © 2011 IEEE.
SCOPUS;2011;Are your sites down? Requirements-driven self-tuning for the survivability of web systems;earned business value,, goal-oriented reasoning,, self-tuning,, survivability assurance;Running in a highly uncertain and greatly complex environment, Web systems cannot always provide full set of services with optimal quality, especially when work loads are high or subsystem failures are frequent. Hence, it is significant to continuously maintain a high satisfaction level of survivability, hereafter survivability assurance, while relaxing or sacrificing certain quality or functional requirements that are not crucial to the survival of the entire system. After giving a value-based interpretation to survivability assurance to facilitate a quantitative analysis, we propose a requirements-driven self-tuning method for the survivability assurance of Web systems. Maintaining an enriched and live goal model, our method adapts to runtime tradeoff decisions made by our PID (proportional-integral-derivative) controller and goal-oriented reasoner for both quality and functional requirements. The goal-based configuration plans produced by the reasoner is carried out on the live goal model, and then mapped into system architectural configurations. Experiments on an online shopping system are conducted to validate the effectiveness of the proposed method. © 2011 IEEE.
SCOPUS;2011;Modelling adaptability and variability in requirements;adaptive systems,, feature models,, modelling,, natural language processing;The requirements and design level identification and representation of dynamic variability for adaptive systems is a challenging task. This requires time and effort to identify and model the relevant elements as well as the need to consider the large number of potentially possible system configurations. Typically, each individual variability dimension needs to identified and modelled by enumerating each possible alternative. The full set of requirements needs to be reviewed to extract all potential variability dimensions. Moreover, each possible configuration of an adaptive system needs to be validated before use. In this demonstration, we present a tool suite that is able to manage dynamic variability in adaptive systems and tame such system complexity. This tool suite is able to automatically identify dynamic variability attributes such as variability dimensions, context, adaptation rules, and soft/hard goals from requirements documents. It also supports modelling of these artefacts as well as their run-time verification and validation. © 2011 IEEE.
SCOPUS;2011;Towards adaptive systems through requirements@runtime?;;Software systems must adapt their behavior in response to changes in the environment or in the requirements they are supposed to meet. Despite adaptation capabilities could be modeled with great detail at design time, anticipating all possible adaptations is not always feasible. To address this problem the requirements model of the system, which also includes the adaptation capabilities, is conceived as a run- time entity. This way, it is possible to trace requirements/adaptation changes and propagate them onto the application instances. This paper leverages the FLAGS [1] methodology, which provides a goal model to represent adaptations and a runtime infrastructure to manage require- ments@runtime. First, this paper explains how the FLAGS infrastructure can support requirements@runtime, by managing the interplay between the requirements and the executing applications. Finally, it describes how this infrastructure can be used to adapt the system, and, consequently, support the evolution of requirements.
SCOPUS;2011;Requirement analysis in agile software development of distributed systems;Agile development,, Distributed systems,, Emergent behavior,, Message sequence chart;Agile methodologies (DSDM, XP, Crystal, SCRUM, etc.) have been adopted as popular approaches to software development mainly because of their capability to deliver the end product faster and incorporate changes to the requirements. Agile follows iterative development, i.e. the implementation of software systems in successive releases. A side effect of multiple releases is introducing inconsistencies to the requirements that may lead to unpredictability of the system behavior at the run-time. Unpredictable systems are hard to debug and harder to manage. It is believed that agile methods can benefit from using more quantified and yet light-weight approaches across the entire development life cycle and particularly, the analysis of requirements for correctness. Checking for consistency of requirements prior to their implementation can remove the predictability problem and lead to significant savings in time and maintenance cost. This research proposes the utilization of a software tool which employs methodologies to automatically analyze system requirements in agile development and detect inconsistencies. The portrayal of requirements via user stories in agile development is used for this purpose. The advantages of adding requirement analysis methodologies to agile development are illustrated using a case study of a real-time drilling simulator.
SCOPUS;2011;Intentional models based on measurement theory;Business service,, Goal-oriented requirements engineering,, Measurement framework,, Measurement theory,, Service level management;Metrics and measures have always been the subject of quite a lot of research works in the Requirements Engineering (RE) community, including about intentional models of Goal-Oriented RE (GORE) such as those of i,,. However, using recent developments of the Measurement Theory, in this paper we show that the concept of Measurement Framework (MF) for soft-systems is useful for the analysis of business service systems that need long-term service agreements based on consistent measurements at all stages of their life-cycle (from inception to operation). We show that with two kinds of goals and softgoals based on MF, it is possible to improve (a) the elicitation of functional and non- functional requirements, (b) the structure of the i,, models, and (c) the consistency between run-time measurements and the model-based assessments of business services at early stages of RE.
SCOPUS;2011;Empirical evaluation of tropos4AS modelling;Agent-oriented software engineering,, Empirical studies,, Self- adaptive systems;Our work addresses the challenges arising in the development of self-adaptive software, which has to work autonomously in an unpredictable environment, fulfilling the objectives of its stakeholders, while avoiding failure. In this context we developed the Tropos4AS framework, which extends the AOSE methodology Tropos to capture and detail at design time the specific decision criteria needed for a system to guide selfadaptation at run-time, and to preserve the concepts of agent and goal model explicitly along the whole development process until run-time. In this paper, we present the design of an empirical study for the evaluation of Tropos4AS, with the aim of assessing the modeling effort, expressiveness and comprehensibility of Tropos4AS models. This experiment design can be reused for the evaluation of other modeling languages extensions.
SCOPUS;2011;A fast real-time rendering method of 3D terrain using out-of-core visualization;Out-of-core,, Real-time rendering,, Terrain;In this paper, we propose an improved algorithm for real-time rendering of large 3D scene, in which we combine quad-tree hierarchy, level of detail (LOD) and out-of-core algorithm. In order to get an efficient rendering method, we construct a scene hierarchy to maintain the rendering scene tree, use quad-tree to simplify terrain meshes, and import the out-of-core algorithm to reduce memory requirements. There are two key steps in his algorithm: Firstly, we will simplify the scene data using quad-tree algorithm. Then we could get a set of simplified meshes for rendering. Secondly, View-dependent out-of-core method is imported to determine which part of meshes should be rendering or delete from the memory. Then we could reduce the memory requirements at runtime. In the last part of the paper, we use terrain data to test out algorithm. Comparing to traditional quad-tree algorithm our method run faster and need less memory requirements.
SCOPUS;2011;Toward a data logging data interchange format: Use cases and requirements;After action review,, Data logging,, DIS,, HLA,, Interoperability,, Simulation,, Training;Many use cases for distributed simulation depend on the effective analysis of simulation data after the simulation has completed, sometimes even years later. While many proprietary data loggers exist, logs are stored in proprietary formats often tailored for the specific use case for which each tool was designed and the specific data model used in a given simulation. This paper suggests that it is both possible and desirable to exchange, archive, and reuse simulation event log data using a standardized format for data interchange. The authors propose that such a format should be developed. There are several differences in the requirements between runtime formats and interchange log formats, with long-term reusability trumping runtime requirements for performance and space efficiency. Finally some solutions are suggested for several of the identified technical challenges. These include support for arbitrary data models, simple yet expressive metadata, log size, and complexity. Some use cases for which the suggested format would be most useful are also given.
SCOPUS;2011;From awareness requirements to adaptive systems: A control-theoretic approach;adaptive systems,, awareness,, control theory,, feedback loop,, requirements;Several proposals for the design of adaptive systems rely on some kind of feedback loop that monitors the system output and adapts in case of failure. Roadmap papers in the area advocate the need to make such feedback loops first class entities in adaptive systems design. We go further by adopting a Requirements Engineering perspective that is not only based on feedback loops but also applies other concepts from Control Theory to the design of adaptive systems. Our plans include a framework that reasons over requirements at runtime to provide adaptivity to a system proper. In this position paper, we argue for a control-theoretic view for adaptive systems and outline our long-term research agenda, briefly presenting work that we have already accomplished and discussing our plans for the future. © 2011 IEEE.
SCOPUS;2011;Compliance management with measurement frameworks;;New regulatory regimes advocate the use of "goal-oriented" regulations that are more flexible during regulatory conversations occurring between the regulators and the regulatees when new regulations are introduced. In that context, long-term "compliance agreements" between regulators and regulatees are needed. Using recent developments of the Measurement Theory, this paper shows that the concept of Measurement Framework (MF) for soft-systems is of particular importance for providing those compliance agreements. We show that with two kinds of goals and softgoals based on MF, one can improve (a) the elicitation of compliance requirements, (b) the structure of the compliance arguments for compliant requirements, and (c) the consistency between actual compliance at run-time and the intentional compliance at early stages of Requirements Engineering. © 2011 IEEE.
SCOPUS;2011;Self-adaptability in secure embedded systems: An energy-performance trade-off;Encryption algorithms,, Power-aware system,, Self-adaptable security,, Weighted product model;Securing embedded systems is a challenging and important research topic due to limited computational and memory resources. Moreover, battery powered embedded systems introduce power constraints that make the problem of deploying security more difficult. This problem may be addressed by improving the trade-off between minimizing energy consumption and maintaining a proper security level. This paper proposes an energy-aware method to determine the security resources consistent with the requirements of the system. The proposed solution is based on a multi-criteria decision mechanism, the Weighted Product Model (WPM), used to evaluate the relations between different security solutions and to select the appropriate one based on variable runtime requirements.
SCOPUS;2011;Reasoning about adaptive requirements for self-adaptive systems at runtime;Feedback,, Planning,, Requirements Engineering,, Self-Adaptive Systems;Increasing proliferation of mobile applications challenge the role of requirements engineering (RE) in developing customizable and adaptive software applications for the end-users. Such adaptive applications need to alter their behavior while monitoring and evaluating the changes in the environment at runtime by being aware of their end-user's needs, context and resources. More specifically, these applications should be able to: (i) reason about their own requirements and refine and validate them at run-time by involving end-users, if necessary,, (ii) provide solutions for the refined or changed requirements at runtime, for instance by exploiting available services. In this position paper we focus on the first issue. We propose to extend our previous work on adaptive requirements with preference-based reasoning and automated planning to enable a continuous adaptive reasoning of requirements at runtime. We describe this vision using a navigation system example and highlight challenges. © 2011 IEEE.
SCOPUS;2011;Enforcing safety requirements for industrial automation systems at runtime position paper;industrial automation systems-safety requirements,, requirements at runtime,, requirements elicitation;Current industrial automation systems are becoming more and more complex, and typically involve different phases of engineering, such as design time and runtime. System requirements, which are usually elicited during design time by engineers, currently are not sufficiently represented at runtime, like the runtime enforcement of safety requirements for industrial automation systems. Such kind of enforcement usually is very hard to model and predict at design time. Hence, the need exists to capture and manage safety requirements at design time and runtime, since safety requirements of industrial automation systems may lead to high risks if not addressed properly. In this position paper, we introduce a safety requirements enforcement framework and the using of Boilerplates for requirements elicitation and by explicitly modeling the runtime requirements knowledge for further application. We illustrate and evaluate the approach with data from a real-world case study in the area of industrial process systems. Major result was that the Boilerplates and explicit engineering knowledge are well suited to capture and enforce runtime safety requirements of industrial automation systems. © 2011 IEEE.
SCOPUS;2011;Requirements models at run-time to support consistent system evolutions;Adaptive systems,, consistent evolution,, requirements engineering;Self-adaptive systems call for run-time management because of the environment uncertainty. In addition users may put forward new needs while the system is in execution possibly in response to environment variations. This means that a self-adaptive system needs to evolve at runtime accordingly to the user and context variations. We propose a context-aware framework that is inspired by the feature engineering perspective, and brings requirements specifications at run time by emphasizing the requirements that are context-dependent. We support system evolution by proposing a notion of correctness which is based on our context requirements. Our framework is generic and it is amenable to augment the system with new requirements arising at run-time. Since new requirements may interact with deployed requirements we provide the support to keep those entities at run-time and check their correctness jointly. Furthermore we identify the characteristics that a requirement language should have, to manage and check requirements at run-time in our framework. © 2011 IEEE.
SCOPUS;2011;Foreword: 2nd workshop requirements@run.time;reflection,, Requirements,, run-time,, self-adaptation;The 2nd edition of the Workshop requirements@run.time was held at the 19th International Conference on Requirements Engineering (RE 2011) in the city of Trento, Italy on the 30th of August 2011. It was organized by Nelly Bencomo, Emmanuel Letier, Jon Whittle, Anthony Finkelstein, and Kris Welsh. This foreword presents a digest of the discussions and presentations that took place during the workshop. © 2011 IEEE.
SCOPUS;2011;Proceedings of the 2011 2nd International Workshop on Requirements@Run. Time, RE@RunTime 2011;;The proceedings contain 8 papers. The topics discussed include: using NFR and context to deal with adaptability in business process models,, requirements models at run-time to support consistent system evolutions,, from awareness requirements to adaptive systems: a control-theoretic approach,, reasoning about adaptive requirements for self-adaptive systems at runtime,, dealing with softgoals at runtime: a fuzzy logic approach,, requirements-driven adaptation: compliance, context, uncertainty, and systems,, and enforcing safety requirements for industrial automation systems at runtime.
SCOPUS;2011;Dealing with softgoals at runtime: A fuzzy logic approach;decisions at runtime,, fuzzy logic,, non-functional requirements analysis,, propagation rules,, reasoning engine;One of the first frameworks to deal with NonFunctional Requirements, or softgoals, is the NFR Framework. This framework allows among other contributions softgoals analysis by applying propagation rules. This analysis is commonly performed during design activities. Instead of working with softgoals at design time, the proposal described in this paper combines propagation rules, fuzzy logic and Multi-Agent Systems in order to provide support for dealing with softgoals at runtime. Observing, for example, how the Requirements Engineering community deals with softgoals analysis by using propagation rules, we developed a propagation simulator centered on a specific algorithm. This simulator tries to replicate the requirements engineers' practices when using propagation rules to make decisions at design time. Based on this propagation simulator, we propose an intentional-MAS-driven reasoning engine capable of analyzing softgoals at runtime by selecting an adequate strategy (i.e. an adequate plan) that will be performed by the intentional agent to achieve the desired goal. © 2011 IEEE.
SCOPUS;2011;Social software product lines;Models at Runtime,, Social Software Engineering,, Software Product Lines,, Users feedback;Software product lines are an engineering paradigm to systematically configure software products from reusable assets so that development effort and time are minimized. Configuring a high quality product is challenging, because quality is a dynamic property and can be difficult to determine at design time. In this paper, we propose Social Software Product Lines paradigm (SSPL) which exploits users' perception in judging products quality and guiding the configuration process at runtime. The SSPL paradigm advocates two principles. First, quality has to be evaluated iteratively during the product operation so that quality evaluation is kept up-to-date. Second, users are the primary evaluators of quality and their feedback is a primitive driver of configuration. At runtime, SSPL obtains users' quality feedback and reconfigures repeatedly in order to deliver the product found to be most adequate by the users' community. We discuss motivation and foundations of SSPL, and outline a set of research challenges. © 2011 IEEE.
SCOPUS;2011;Monitoring fuzzy temporal requirements for service compositions: Motivations, challenges and experimental results;Fuzzy Goals,, Requirements Monitoring,, Self-Adaptive Systems,, Service-Compositions;Service compositions are an important family of self-adaptive systems, which need to cope with the variability of the environment (e.g., heterogeneous devices, changing context), and react to unexpected events (e.g., changing components) that may take place at runtime. To this aim, it is fundamental to continuously assess requirements while the system is executing and detect partial mismatches or handle uncertainty. Detecting the entity of a violation is very helpful, since it can guide the way applications adapt at runtime. This paper is based on the FLAGS language we already proposed in our previous work to represent requirements as fuzzy temporal formulas and identify partial violations at the temporal level. The paper illustrates the advantages of using the FLAGS language to express the requirements of service compositions, and proposes a technique to monitor them at runtime. The experimental evaluation demonstrates that the monitoring technique is feasible and the overhead introduced in the running system is negligible. © 2011 IEEE.
SCOPUS;2011;Goal-driven adaptation of service-based systems from runtime monitoring data;Goal-oriented requirements engineering,, IStar,, Runtime adaptation,, Service-oriented system,, Variability modelling;Service-based systems need to provide flexibility to adapt both to evolving requirements from multiple, often conflicting, ephemeral and unknown stakeholders, as well as to changes in the runtime behavior of their component services. Goal-oriented models allow representing the requirements of the system whilst keeping information about alternatives. We present the MAESoS approach which uses i* diagrams to identify quality of service requirements over services. The alternatives are extracted and kept in a variability model. A monitoring infrastructure identifies changes in runtime behavior that can propagate up to the level of stakeholder goals and trigger the required adaptations. We illustrate the approach with a scenario of use. © 2011 IEEE.
SCOPUS;2011;DSX: A knowledge-based scoring function for the assessment of protein-ligand complexes;;We introduce the new knowledge-based scoring function DSX that consists of distance-dependent pair potentials, novel torsion angle potentials, and newly defined solvent accessible surface-dependent potentials. DSX pair potentials are based on the statistical formalism of DrugScore, extended by a much more specialized set of atom types. The original DrugScore-like reference state is rather unstable with respect to modifications in the used atom types. Therefore, an important method to overcome this problem and to allow for robust results when deriving pair potentials for arbitrary sets of atom types is presented. A validation based on a carefully prepared test set is shown, enabling direct comparison to the majority of other popular scoring functions. Here, DSX features superior performance with respect to docking- and ranking power and runtime requirements. Furthermore, the beneficial combination with torsion angle-dependent and desolvation-dependent potentials is demonstrated. DSX is robust, flexible, and capable of working together with special features of popular docking engines, e.g., flexible protein residues in AutoDock or GOLD. The program is freely available to the scientific community and can be downloaded from our Web site www.agklebe.de. © 2011 American Chemical Society.
SCOPUS;2011;Automatic derivation of utility functions for monitoring software requirements;;Utility functions can be used to monitor requirements of a dynamically adaptive system (DAS). More specifically, a utility function maps monitoring information to a scalar value proportional to how well a requirement is satisfied. Utility functions may be manually elicited by requirements engineers, or indirectly inferred through statistical regression techniques. This paper presents a goal-based requirements model-driven approach for automatically deriving state-, metric-, and fuzzy logic-based utility functions for RELAXed goal models. State- and fuzzy logic-based utility functions are responsible for detecting requirements violations, and metric-based utility functions are used to detect conditions conducive to a requirements violation. We demonstrate the proposed approach by applying it to the goal model of an intelligent vehicle system (IVS) and use the derived utility functions to monitor the IVS under different environmental conditions at run time. © 2011 Springer-Verlag.
SCOPUS;2011;Adaptable Decentralized Service Oriented Architecture;BPEL,, Distributed orchestration engine,, Dynamic software,, Process mining,, Self-,, Self-adaptive systems,, Service Oriented Architecture,, systems,, Workflow Decentralization;In the Service Oriented Architecture (SOA), BPEL specified business processes are executed by non-scalable centralized orchestration engines. In order to address the scalability issue, decentralized orchestration engines are applied, which decentralize BPEL processes into static fragments at design time without considering runtime requirements. The fragments are then encapsulated into runtime components such as agents. There are a variety of attitudes towards workflow decentralization,, however, only a few of them produce adaptable fragments with runtime environment. In this paper, producing runtime adaptable fragments is presented in two aspects. The first one is frequent-path adaptability that is equal to finding closely interrelated activities and encapsulating them in the same fragment to omit the communication cost of the activities. Another aspect is proportional-fragment adaptability, which is analogous to the proportionality of produced fragments with number of workflow engine machines. It extenuates the internal communication among the fragments on the same machine. An ever-changing runtime environment along with the mentioned adaptability aspects may result in producing a variety of process versions at runtime. Thus, an Adaptable and Decentralized Workflow Execution Framework (ADWEF) is introduced that proposes an abstraction of adaptable decentralization in the SOA orchestration layer. Furthermore, ADWEF architectures Type-1 and Type-2 are presented to support the execution of fragments created by two decentralization methods, which produce customized fragments known as Hierarchical Process Decentralization (HPD) and Hierarchical Intelligent Process Decentralization (HIPD). However, mapping the current system conditions to a suitable decentralization method is considered as future work. Evaluations of the ADWEF decentralization methods substantiate both adaptability aspects and demonstrate a range of improvements in response-time, throughput, and bandwidth-usage compared to previous methods. © 2011 Elsevier Inc.
SCOPUS;2011;Social sensing: When users become monitors;Adaptive software engineering,, Models at runtime,, Requirements engineering,, Social software engineering;Adaptation requires a system to monitor its operational context to ensure that when changes occur, a suitable adaptation action is planned and taken at runtime. The ultimate goal of adaptation is that users get their dynamic requirements met efficiently and correctly. Context changes and users'judgment of the role of the system in meeting their requirements are drivers for adaptation. In many cases, these drivers are hard to identify by designers at design time and hard to monitor by the use of exclusively technological means by the system at runtime. In this paper, we propose Social Sensing as the activity performed by users who act as monitors and provide information needed for adaptation at runtime. Such information helps the system cope with technology limitations and designers'uncertainty. We discuss the motivation and foundations of Social Sensing and outline a set of research challenges to address in future work. © 2011 ACM.
SCOPUS;2011;Adaptive service composition based on runtime requirements monitoring;Goal refinement,, Requirement monitoring,, Service composition;In today's service computing environments, user needs and expectations are constantly changing. New services emerge while old ones become obsolete and need to be replaced. In such settings, composite services need to be adaptive to changes in user requirements and the environment. This paper proposes a conceptual framework for modeling compositional adaptation for services founded on a requirements monitoring facility. This facility helps maintain adherence between user requirements changes and the dynamics of service composition structure and quality attributes. Specifically, user requirements are represented as goals and softgoals, service composition structure is represented with a CSP-like grammar, and the adaptation mechanism is based on AI planning. The proposed approach is evaluated in a service simulation environment of real-world supply-chain adaptation scenarios. © 2011 IEEE.
SCOPUS;2011;Method for developing requirements of equipment system based on synthetic microanalytic approach;Equipment system,, Requirements development,, Requirements measure,, Requirements model,, Synthetic microanalytic approach(SMA);In order to systemically develop requirements of equipment system, a new method is presented, which is based on SMA (synthetic microanalytic approach) and considers the combination of command and technology. Top level requirements and lower level requirements in requirement reflection are viewed as two conceptual planes, which are named as macro-plane and micro-plane. Top level requirement models, as the macroscopic constraints, are used to guide the development of lower level requirements, while lower level requirement models are used to solve macroscopic problems from the view of microcosmic mechanism. Therefore, requirements of equipment system can be developed by a two-way integrating method, that is, the integration method of 'top-down' and 'bottom-up'. An example is presented to illustrate the method and analysis results show that it is efficient for systemically developing requirements of equipment system.
SCOPUS;2011;Assessment of ABET student outcomes during industrial internships;;The Paper Science and Engineering (PSEN) program at UW-Stevens Point has had a three-credit industrial internship requirement since 1973. We assessed this requirement through comprehensive student papers covering the technology of the pulp and paper industry and the processes and products of the mills in which students worked. This assessment worked well until roughly ten years ago, when mills began retaining the reports, saying that they contained proprietary information. At the time, faculty decided to share the rubric used to evaluate student papers with mill supervisors so that they would have a standard by which they could rate papers, as well as an evaluation form to provide feedback on student work in the mill. In 2010, we developed a new approach to assessing these internships. Taking advantage of the capabilities of the online course management system Desire2Learn®, students now respond to 16 questions about their internship work while they are in the mills. These responses help students to remember activities performed during the entire internship. When they return to campus, students provide two pieces of work to satisfy academic requirements: reflection papers and electronic portfolios that document their internship work, specifically addressing how their internships helped them develop skills in several ABET Student Outcomes for the PSEN program. The portfolios provide evidence that the faculty can use to assess the achievement level for outcomes associated with these internships. In this paper, we describe the assessment method in more detail, and the conference presentation will include a demonstration of the technology. We also discuss the need for students to be able to more clearly identify and articulate achievement of learning outcomes. A critical finding of our initial study is that students often met learning outcomes without realizing they did, without understanding the importance of communicating that they did, or simply by not being able to effectively communicate that they did. © 2011 American Society for Engineering Education.
SCOPUS;2011;A practical formal approach for requirements validation and verification of dependable systems;JUnits tests,, Requirements,, Runtime execution monitoring,, Statechart assertions,, V&V;Classical requirements validation methods usually work with static behavioral models, and under the assumption that there are no dependencies and interactions between the requirements. Requirements verification is mostly done by statically analyzing the design artifacts and by running tests. This work presents a practical formal approach for requirements validation and verification (V&V) of dependable systems, under two different perspectives: development and acquisition. The approach considers the system's dynamic behavior that is formally represented as statechart assertions and validated using JUnit test scenarios. Runtime execution monitoring (REM) data is used to create JUnit tests to verify the system's behavior against the assertions. The V&V activities are supported by the StateRover tool. Two space systems case studies are briefly presented. As dependability often manifests as decidable system sequencing behaviors, the main contribution of this work is centered on the validation and verification of such behaviors. © 2011 IEEE.
SCOPUS;2011;Requirements engineering for self-adaptive systems: Core ontology and problem statement;Adaptation Problem,, Requirements Engineering,, Runtime,, Self-Adaptive Systems;The vision for self-adaptive systems (SAS) is that they should continuously adapt their behavior at runtime in response to changing user's requirements, operating contexts, and resource availability. Realizing this vision requires that we understand precisely how the various steps in the engineering of SAS depart from the established body of knowledge in information systems engineering. We focus in this paper on the requirements engineering for SAS. We argue that SAS need to have an internal representation of the requirements problem that they are solving for their users. We formally define a minimal set of concepts and relations needed to formulate the requirements problem, its solutions, the changes in its formulation that arise from changes in the operating context, requirements, and resource availability. We thereby precisely define the runtime requirements adaptation problem that a SAS should be engineered to solve. © 2011 Springer-Verlag.
SCOPUS;2011;CleGo: A package for automated computation of Clebsch-Gordan coefficients in tensor product representations for Lie algebras A-G;Clebsch-Gordan coefficients,, GUT,, Lie algebra,, Model building,, Multiple tensor product,, Symmetry breaking,, Tensor product,, Tensor product decomposition;We present a program that allows for the computation of tensor products of irreducible representations of Lie algebras A-G based on the explicit construction of weight states. This straightforward approach (which is slower and more memory-consumptive than the standard methods to just calculate dimensions of the tensor product decomposition) produces Clebsch-Gordan coefficients that are of interest for instance in discussing symmetry breaking in model building for grand unified theories. For that purpose, multiple tensor products have been implemented as well as means for analyzing the resulting effective operators in particle physics. Program summary: Program title: CleGo Catalogue identifier: AEIQ-v1-0 Program summary URL: http://cpc.cs.qub.ac.uk/ summaries/AEIQ-v1-0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 3641 No. of bytes in distributed program, including test data, etc.: 34 536 Distribution format: tar.gz Programming language: O'Caml Computer: i386-i686, x86-64 Operating system: Cross-platform, for definiteness though we assume some UNIX system. RAM: ≥4 GB commendable, though in general memory requirements depend on the size of the Lie algebras and the representations involved. Classification: 4.2, 11.1 Nature of problem: Clebsch-Gordan coefficients are widely used in physics. This program has been written as a means to analyze symmetry breaking in the context of grand unified theories in particle physics. As an example, we computed the singlets appearing in higher-dimensional operators 27⊗ 27⊗ 27⊗ 78 and 27⊗ 27⊗ 27⊗ 650 in an E6-symmetric GUT. Solution method: In contrast to very efficient algorithms that also produce tensor product decompositions (as far as outer multiplicities/Littlewood-Richardson coefficients are concerned) we proceed straightforwardly by constructing all the weight states, i.e. the Clebsch-Gordan coefficients. This obviously comes at the expense of high memory and CPU-time demands. Applying Dynkin arithmetic in weight space, the algorithm is an extension of the one for the addition of angular momenta in su(2) ≈ A1, for reference see [1]. Note that, in general, Clebsch-Gordan coefficients are basis-dependent and therefore need to be understood with respect to the chosen basis. However, singlets appearing in (multiple) tensor products are less basis-dependent. Restrictions: Generically, only tensor products of non-degenerate or adjoint representations can be computed. However, the irreps appearing therein can subsequently be used as new input irreps for further tensor product decomposition so in principle there is no restriction on the irreps the tensor product is taken of. In practice, though, it is by the very nature of the explicit algorithm that input is restricted by memory and CPU runtime requirements. Unusual features: Analytic computation instead of float numerics. Additional comments: The program can be used in "notebook style" using a suitable O'Caml toplevel. Alternatively, an O'Caml input file can be compiled which results in processing that is approximately a factor of five faster. The latter mode is commendable when large irreps need to be constructed. Running time: Varies depending on the input from parts of seconds to weeks for very large representations (because of memory exhaustion). © 2011 Elsevier B.V. All rights reserved.
SCOPUS;2011;Runtime models for automatic reorganization of multi-robot systems;Agent-oriented software engineering,, Cooperative Robotics,, Runtime models,, Self-adaptive systems;This paper presents a reusable framework for developing adaptive multi-robotic systems for heterogeneous robot teams using an organization-based approach. The framework is based on the Organizational Model for Adaptive Computational Systems (OMACS) and the Goal Model for Dynamic Systems (GMoDS). GMoDS is used to capture system-level goals that drive the system. OMACS is an abstract model used to capture the system configuration and allows the team to organize and reorganize without the need for explicit runtime reorganization rules. While OMACS provides an implicit reorganization capability, it also supports policies that can either guide or restrict the resulting organizations thus limiting unexpected or harmful adaptation. We demonstrate our framework by presenting the design and implementation of a multi-robot system for detecting improvised explosive devices. We then highlight the adaptability of the resulting system. © 2011 ACM.
SCOPUS;2011;Towards self-managed systems aware of economic value;Autonomic computing,, Requirements engineering,, Self-managed systems,, Value-based software engineering;Current self-managed systems do not take into account the economic value they exchange with external systems and services. While value-based software and requirements engineering approaches exist, they do not specifically deal with self-managed systems and their operations. These operations are typically associated with changing requirements, dynamic adaptations, and runtime reconfigurations, as well as changes in the environment. We argue that self-managed systems should be "aware" of their requirements and the economic value created (or loosed) by their fulfillment (or not fulfillment) as well as the economic value exchanged with other systems. They should be able to reason and take actions based on this "awareness", especially when their environmental conditions are changing (both technical and economic ones). More generally, self-managed systems should be able to reason about the systems' business goals and relate them with the Quality of Service they offer. This paper discusses these issues and proposes a generic architecture for addressing them. © 2010 IEEE.
SCOPUS;2011;Satisfying user needs at the right time and in the right place: A research preview;End-User Involvement,, Requirements Engineering,, Self Adaptive Systems;[Context and motivation] Most requirements engineering (RE) approaches involve analysts in gathering end-user needs. However, we promote the idea that future service-based applications should support end-users in expressing their needs themselves, while the system should be able to respond to these requests by combining existing services in a seamless way. [Question/problem] Research tackling this idea is limited. In this research preview paper we sketch a plan to investigate the following research questions: How can end-users be facilitated by a system to express new needs (e.g. goals, preferences)? How can the continuous analysis of end-user needs result in an appropriate solution? [Principal ideas/results] In our recent research, we have started to explore the idea of involving end-users in RE. Furthermore, we have proposed an architecture that allows performing RE at run-time. The purpose of the planned research is to combine and extend our recent work and to come up with a tool-based solution, which involves end-users in realizing self-adaptive services. Our research objectives include to continuously capture, communicate and analyze end-user needs and feedback in order to provide a tailored solution. [Contribution] In this paper we give a preview on the planned work. After reporting on our recent work we present our research idea and the research objectives in more detail. © 2011 Springer-Verlag Berlin Heidelberg.
SCOPUS;2011;Study of goal-oriented migrating workflow system based on mobile computing paradigm;Goal-oriented,, Migrating workflow,, Mobile agent;Workflow systems provide the automation of business processes where a collection of tasks is organized between participants according to a set of defined rules to accomplish some business goals. However, due to the lack of flexible mechanisms, such as fast changing customer requirements and enterprise goal shifts, a static workflow definition designed at build time is inflexible to meet the complex, dynamic situations that happen at run time. This paper presents a goaland agent-based migrating workflow model. Structure of goal model in goal-oriented migrating instance and a way how GoMI pursues its goals is presented. A case study on goal-oriented migrating workflow system using the proposed methodology is also illustrated in this paper, The major contribution of this paper includes: (1) a definition of GoMWf model adopted the mobile computing paradigm and a framework of GoMWf,, (2) a definition of goal-oriented migrating instance (GoMI) and its architecture based on BDI model,, (3) evaluation of goal-based workflow system and process-based workflow system.
SCOPUS;2011;Requirements evolution: From assumptions to reality;Contextual Requirements,, Requirements at Runtime,, Requirements Engineering,, Requirements Evolution;Requirements evolution is a main driver for systems evolution. Traditionally, requirements evolution is associated to changes in the users' needs and environments. In this paper, we explore another cause for requirements evolution: assumptions. Requirements engineers often make assumptions stating, for example, that satisfying certain sub-requirements and/or correctly executing certain system functionalities would lead to reach a certain requirement. However, assumptions might be, or eventually become, invalid. We outline an approach to monitor, at runtime, the assumptions in a requirements model and to evolve the model to reflect the validity level of such assumptions. We introduce two types of requirements evolution: autonomic (which evolves the priorities of system alternatives based on their success/failure in meeting requirements) and designer-supported (which detects loci in the requirements model containing invalid assumptions and recommends designers to take evolutionary actions). © 2011 Springer-Verlag.
SCOPUS;2011;A Runtime evaluation methodology and framework for autonomic systems;Adaptive systems,, Autonomic computing,, Embedded system,, Goal model;An autonomic system provides self-adaptive ability that enables system to dynamically adjust its behavior on environmental changes or system failure. Fundamental process of adaptive behavior in an autonomic system is consist of monitoring system or/and environment information, analyzing monitored information, planning adaptation policy and executing selected policy. Evaluating system utility is one of a significant part among them. We propose a novel approach on evaluating autonomic system at runtime. Our proposed method takes advantage of a goal model that has been widely used at requirement elicitation phase to capture system requirements. We suggest the state-based goal model that is dynamically activated as the system state changes. In addition, we defined type of constraints that can be used to evaluate goal satisfaction level. We implemented a prototype of autonomic computing software engine to verity our proposed method. We simulated the behavior of the autonomic computing engine with the home surveillance robot scenario and observed the validity of our proposed method.
SCOPUS;2010;A method to acquire compliance monitors from regulations;;Developing software systems in heavily regulated industries requires methods to ensure systems comply with regulations and law. A method to acquire finite state machines (FSM) from stakeholder rights and obligations for compliance monitoring is proposed. Rights and obligations define what people are permitted or required to do,, these rights and obligations affect software requirements and design. The FSM allows stakeholders, software developers and compliance officers to trace events through the invocation of rights and obligations as pre- and post-conditions. Compliance is monitored by instrumenting runtime systems to report these events and detect violations. Requirements and software engineers specify the rights and obligations, and apply the method using three supporting tasks: 1) identify under-specifications, 2) balance rights with obligations, and 3) generate finite state machines. Preliminary validation of the method includes FSMs generated from U.S. healthcare regulations and tool support to parse these specifications and generate the FSMs. ©2010 IEEE.
SCOPUS;2010;Adaptive software needs continuous verification;;Modern software applications increasingly live in an open world [2], characterized by continuous change in the environment in which they are situated and in the requirements they have to meet. Continuous changes occur autonomously and unpredictably, in a way that can hardly be predicted (and taken care of) by software engineers, as the application is designed. As a consequence, changes are out of control of the running application, which cannot handle them. On the other hand, there is an increasing demand for software solutions that can easily evolve and dynamically adapt their behavior to provide continuous service as changes occur. This is especially needed when systems must be perpetually running and cannot be changed off-line. Hereafter I focus on environment changes that may affect an application. This may include changes in the way people interact with the system or changes in the external components, which offer services upon which the currently developed application relies. Moreover, I will mostly focus on quantitatively stated requirements that express nonfunctional properties of an application, such as performance or reliability. Because of the uncertainty that characterizes open-world settings, requirements should be expressed in probabilistic terms. I will argue that models at run-time are needed to support continuous verification. Furthermore, I will discuss why continuous verification is needed to support an on-line update of the application's model, which-in turn-may support formal approaches to software evolution. Traditionally, modeling is considered as a conceptual tool in the requirements and design stages of software development. For example, requirements models are built to better understand and reason about the qualities a system should exhibit in order to fulfill its goals. They are also used to support systematic interaction with the stakeholders in requirements elicitation. Modeling also helps crystallize design decisions and evaluate the trade-offs among them. Model-driven development has also been proposed, and several approaches are being investigated, with the goal of automating the generation of implementations through chains of (possibly automated) model transformation steps. Models, however, are not only the cornerstone of rigorous software development approaches, but must extend their lifetime to run-time to support continuous verification. © 2010 IEEE.
SCOPUS;2010;Continuous adaptive requirements engineering: An architecture for self-adaptive service-based applications;Requirements engineering,, Self-adaptive systems,, Service based applications;Engineering self-adaptive service-based applications significantly challenges the role of requirements engineering (RE). Such systems need to cope with the evolving requirements at runtime by monitoring the changes in users' preferences and in the environment, evaluating the changes and enacting a suitable behavior ensuring an acceptable level of quality for their users. This calls for continuous reappraisal of their requirements specification enabling them to reason for them at run-time. Recently, we proposed a novel framework for Continuous Adaptive RE (CARE) supporting self-adaptive service-based applications and on conceptual tools needed to realize RE at run-time. In this position paper, we focus on a conceptual architecture for the CARE framework, and illustrate how it can work using scenarios from travel domain. Potential impact of our work and useful integration with recent studies are discussed, highlighting open points for future research. © 2010 IEEE.
SCOPUS;2010;Requirements-aware systems: A research agenda for RE for self-adaptive systems;Reflection,, Requirements,, Run-time,, Self-adaptive systems;Requirements are sensitive to the context in which the system-to-be must operate. Where such context is well-understood and is static or evolves slowly, existing RE techniques can be made to work well. Increasingly, however, development projects are being challenged to build systems to operate in contexts that are volatile over short periods in ways that are imperfectly understood. Such systems need to be able to adapt to new environmental contexts dynamically, but the contextual uncertainty that demands this self-adaptive ability makes it hard to formulate, validate and manage their requirements. Different contexts may demand different requirements trade-offs. Unanticipated contexts may even lead to entirely new requirements. To help counter this uncertainty, we argue that requirements for self-adaptive systems should be run-time entities that can be reasoned over in order to understand the extent to which they are being satisfied and to support adaptation decisions that can take advantage of the systems' self-adaptive machinery. We take our inspiration from the fact that explicit, abstract representations of software architectures used to be considered design-time-only entities but computational reflection showed that architectural concerns could be represented at run-time too, helping systems to dynamically reconfigure themselves according to changing context. We propose to use analogous mechanisms to achieve requirements reflection. In this paper we discuss the ideas that support requirements reflection as a means to articulate some of the outstanding research challenges. © 2010 IEEE.
SCOPUS;2010;2010 1st International Workshop on Requirements@Run.Time, RE@RunTime 2010;;The proceedings contain 7 papers. The topics discussed include: goal-oriented requirements modeling for running systems,, towards a continuous requirements engineering framework for self-adaptive systems,, continuous adaptive requirements engineering: an architecture for self-adaptive service-based applications,, using requirements traceability links at runtime - a position paper,, run-time monitoring of system performance: a goal-oriented and system architecture simulation approach,, and adaptive monitoring of software requirements.
SCOPUS;2010;Foreword: First workshop requirements@run.time;Reflection,, Requirements,, Run-time,, Self-adaptive systems;The first edition of the Workshop requirements@run.time was held at the Eighteenth International Conference on Requirements Engineering (RE 2010) in the city of Sydney, NSW, Australia on the 28th of September 2010. It was organized by Pete Sawyer, Jon Whittle, Nelly Bencomo, Daniel Berry, and Anthony Finkelstein. This foreword presents a digest of the presentations and discussions that took place during the workshop. © 2010 IEEE.
SCOPUS;2010;Requirements engineering for adaptive Service Based Applications;Requirements engineering,, Self-adaptive systems,, Service Based Applications;Service-Based Applications (SBA) are inherently open and distributed, as they rely on third-party services that are available over the Internet, and have to cope with the dynamism of such operating environment. This motivates the need for SBA to be self-adaptive to accommodate changes in service availability and performance, in consumers' needs and preferences, and more generally in the operational environment, which may occur at run-time. Engineering such applications significantly challenges the role of requirements engineering (RE). Usually, RE is carried out at the outset of the whole development process, but in the context of SBA, RE activities are also needed at run-time thus enabling a seamless SBA evolution. In this paper, we investigate RE for SBA at run-time proposing a method that supports the continuous refinement of requirements artifacts at run-time, which involves consumers and the SBA itself as primary stakeholders. © 2010 IEEE.
SCOPUS;2010;Fuzzy goals for requirements-driven adaptation;Fuzzy goals,, Goals,, KAOS,, Self-adaptation;Self-adaptation is imposing as a key characteristic of many modern software systems to tackle their complexity and cope with the many environments in which they can operate. Self-adaptation is a requirement per-se, but it also impacts the other (conventional) requirements of the system,, all these new and old requirements must be elicited and represented in a coherent and homogenous way. This paper presents FLAGS, an innovative goal model that generalizes the KAOS model, adds adaptive goals to embed adaptation countermeasures, and fosters self-adaptation by considering requirements as live, runtime entities. FLAGS also distinguishes between crisp goals, whose satisfaction is boolean, and fuzzy goals, whose satisfaction is represented through fuzzy constraints. Adaptation countermeasures are triggered by violated goals and the goal model is modified accordingly to maintain a coherent view of the system and enforce adaptation directives on the running system. The main elements of the approach are demonstrated through an example application. © 2010 IEEE.
SCOPUS;2010;Using requirements traceability links at runtime - A position paper;Development time,, Requirements,, Runtime,, Traceability;During software development a large amount of varied information is created. It comprises the requirements specification and depending artifacts such as design, code or test cases, as well as supporting information such as traceability links. This information is intended to be used during development time. The research in requirements at runtime has so far focused on using the requirements specification at runtime. This paper explores how to use the existing traceability links between requirements and other artifacts at runtime. © 2010 IEEE.
SCOPUS;2010;Self-tuning of software systems through goal-based feedback loop control;Control theory,, Goal reasoning,, Self-tuning;Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators rather than high-level requirements. By linking the overall satisfaction to a business value indicator as feedback, we propose a control-theoretic self-tuning method that can dynamically adjust the tradeoff decisions among different quality requirements. A preference-based reasoning algorithm is involved to configure hard goals accordingly to guide the following architecture reconfiguration. © 2010 IEEE.
SCOPUS;2010;A metamodel for aspect-oriented analysis approach;Aspect-oriented requirements analysis,, Metamodel;The Aspect-Oriented Requirements Analysis (AORA) approach concentrates on the identification, modularization, representation and composition of crosscutting concerns during Requirements Engineering. Crosscutting concerns are encapsulated in separate modules, known as aspects, and composition mechanisms are later used to compose them back with other core modules, at loading time, compilation time, or run-time. AORA is an integrated framework with a process model, a traceability schema, a conflict resolution technique and tool supporting. But for an approach to be more precisely defined a metamodel specification is desired. This paper presents a metamodel for AORA as a means to define its concepts, relationships and composition rules rigorously. © 2010 by CIbSE 2010.
SCOPUS;2010;Runtime adaptability through automated model evolution;Dynamic adaptability,, Model at runtime,, Model evolution,, Quality requirements;Dynamically adaptive systems propose adaptation by means of variants that are specified in the system model at design time and allow for a fixed set of different runtime configurations. However, in a dynamic environment, unanticipated changes may result in the inability of the system to meet its quality requirements. To allow the system to react to these changes we propose a solution for automatically evolving the system model by integrating new variants and periodically validating existing ones based on updated quality parameters. To illustrate our approach we present a BPEL based framework using a service composition model to represent the system functional requirements. Our framework estimates Quality of Service (QoS) values based on information provided by our monitoring mechanism, ensuring that changes in QoS are reflected in the system model. We show how the evolved model can be used at runtime to increase the system's autonomic capabilities and delivered QoS. © 2010 IEEE.
SCOPUS;2010;SIGAda 2010 - Proceedings of the 2010 ACM International Conference on Ada and Related Technologies;;The proceedings contain 9 papers. The topics discussed include: effective requirements engineering,, use of object oriented technologies in high reliability system,, designing real-time, concurrent, and embedded software systems using UML and Ada,, Ada for parallel, embedded, and real-time applications,, unmanned systems with Ada and RTEMS,, systems software integrity assurance,, a deterministic run-time environment for Ada-05 on the ATmega 16 microcontroller,, a methodology for avoiding known compiler problems using static analysis,, wouldn't it be nice to have software labels?,, experience report: Ada & Java integration in the FAA's ERAM SWIM program,, real-time system development in Ada using Lego® Mindstorms® NXT,, parallelism generics for Ada 2005 and beyond,, extending Ada to support multi-core based monitoring and fault tolerance,, towards Ada 2012: an interim report,, and the rise, fall and persistence of Ada.
SCOPUS;2010;From ODRL-S to low-level DSL: A case study based on license compliance in service oriented systems;;In this paper, we present a case study in the framework of COMPAS, a research project focused on supporting compliance monitoring and verification in service based systems. In this paper, we also illustrate how we translate high-level service licenses (specified in Open Digital Rights Language for Services (ODRL-S)) to low-level rules for verifying the compliance requirements at runtime. We validate our approach by architecting a compliance driven service oriented system, where at runtime business processes are monitored for compliance.
SCOPUS;2010;Utilizing parallelism of TMR to enhance power efficiency of reliable ASIC designs;Circuit design,, Power consumption,, Power-Aware design,, Reliability,, Triple modular redundancy;Due to aggressive scaling, reliability issues influence the design process of integrated circuits more and more. A well known technique to tackle these issues represents Triple Modular Redundancy (TMR). It strongly improves reliability of a design at the expense of at least tripled area and power consumption. In this contribution, we propose an enhanced TMR approach that significantly decreases the power overhead of conventional TMR designs. Therefore, the control logic was modified so as to switch between a TMR mode and a parallel mode. This parallel mode allows the circuit to operate with decreased frequency without losing performance by taking advantage of the parallelism offered by the tripled design. Achieved results of investigations on the ISCAS benchmark circuits show power savings of up to 50 % with a small reliability penalty compared to a conventional TMR approach for permanent failures. We also propose strategies how to utilize both operating modes in order to balance the design concerning reliability and power consumption requirements at runtime. ©2010 IEEE.
SCOPUS;2010;Efficient object detection using orthogonal NMF descriptor hierarchies;;Recently descriptors based on Histograms of Oriented Gradients (HOG) and Local Binary Patterns (LBP) have shown excellent results in object detection considering the precision as well as the recall. However, since these descriptors are based on high dimensional representations such approaches suffer from enormous memory and runtime requirements. The goal of this paper is to overcome these problems by introducing hierarchies of orthogonal Non-negative Matrix Factorizations (NMF). In fact, in this way a lower dimensional feature representation can be obtained without loosing the discriminative power of the original features. Moreover, the hierarchical structure allows to represent parts of patches on different scales allowing for a more robust classification. We show the effectiveness of our approach for two publicly available datasets and compare it to existing state-of-the-art methods. In addition, we demonstrate it in context of aerial imagery, where high dimensional images have to be processed requiring efficient methods. © 2010 Springer-Verlag.
SCOPUS;2010;Model-based platform-specific co-design methodology for dynamically partially reconfigurable systems with hardware virtualization and preemption;Dynamically partially reconfigurable system,, Hardware/software co-design,, UML;To facilitate the development of the dynamically partially reconfigurable system (DPRS), we propose a model-based platform-specific co-design (MPC) methodology for DPRS with hardware virtualization and preemption. For DPRS analysis and validation, a model-based verification and estimation framework is proposed to make model-driven architecture (MDA) more realistic and applicable to the DPRS design. Considering inherent characteristics of DPRS and real-time system requirements, a semi-automatic model translator converts the UML models of DPRS into timed automata models with transition urgency semantics for model checking. Furthermore, a UML-based hardware/software co-design platform (UCoP) can support the direct interaction between the UML models and the real hardware architecture. Compared to the existing estimation methods, UCoP can provide accurate and efficient platform-specific verification and estimation. We also propose a hierarchical design that consists of a hardware virtualization mechanism for dynamically linking the device nodes, kernel modules, and on-demand reconfigurable hardware functions and a hardware preemption mechanism for further increasing the utilization of hardware resources per unit time. Further, we realize a dynamically partially reconfigurable network security system (DPRNSS) to show the applicability and practicability of the MPC methodology. The DPRNSS cannot only dynamically adapt some of its hardware functions at run-time to meet different system requirements, but also determine which mechanism will be used. Our experiments also demonstrate that the hardware virtualization mechanism can save the overall system execution time up to 12.8% and the hardware preemption mechanism can reduce up to 41.3% of the time required by reconfiguration-based methods. © 2010 Elsevier B.V. All rights reserved.
SCOPUS;2010;Adaptive goals for self-adaptive service compositions;;Service compositions need to continuously selfadapt to cope with unexpected failures. In this context adaptation becomes a fundamental requirement that must be elicited along with the other functional and non functional requirements. Beside modelling, effective adaptation also demands means to trigger it at runtime as soon as the actual behavior of the composition deviates from stated requirements. This paper extends traditional goal models with adaptive goals to support continuous adaptation. Goals become live, runtime entities whose satisfaction level is dynamically updated. Furthermore, boundary infringement triggers adaptation capabilities. The paper also provides a methodology to trace goals onto the underlying composition, assess goals satisfaction at runtime, and activate adaptation consequently. All the key elements are demonstrated on the definition of the process to control an advanced washing machine. © 2010 IEEE.
SCOPUS;2010;Two-staged approach for semantically annotating and brokering TV-related services;Digital TV,, Linked data,, Linked services,, Semantic web,, Semantic web services;Nowadays, more and more distributed digital TV and TV-related resources are published on the Web, such as Electronic Personal TV Guide (EPG) data. To enable applications to access these resources easily, the TV resource data is commonly provided by Web service technologies. The huge variety of data related to the TV domain and the wide range of services that provide it, raises the need to have a broker to discover, select and orchestrate services to satisfy the runtime requirements of applications that invoke these services. The variety of data and heterogeneous nature of the service capabilities makes it a challenging domain for automated web-service discovery and composition. To overcome these issues, we propose a two-stage service annotation approach, which is resolved by integrating Linked Services and IRS-III semantic web services framework, to complete the lifecycle of service annotating, publishing, deploying, discovering, orchestration and dynamic invocation. This approach satisfies both developer's and application's requirements to use Semantic Web Services (SWS) technologies manually and automatically. © 2010 IEEE.
SCOPUS;2010;First step towards a domain specific language for self-adaptive systems;Domain Specific Language (DSL),, Dynamically Adaptive Systems (DASs),, Eclipse Modeling Framework (EMF);Self-adaptive systems are capable of autonomously modifying their behavior at run-time in response to changing environmental conditions. In order to modify the behavior, requirements play an important role, as they tend to change for these systems. For this we need to identify those requirements that are concerned with the adaptability features of the self-adaptive systems. In order to cope with the uncertainty inherent in self-adaptive systems, requirements engineering languages for these systems should include explicit constructs. RELAX is a requirement engineering language for self-adaptive systems that incorporates uncertainty into the specification of these systems. To go one step further, we aim at developing a domain specific language that would bridge the gap between requirements and the overall system model. The first step that is illustrated in this paper is to build a textual editor for RELAX. ©2010 IEEE.
SCOPUS;2010;Template based SOA framework for dynamic and adaptive composition of Web Services;Context,, Dynamic composition,, SOA,, Web service;Automated Web service composition is currently one of the major research problems in the area of service oriented computing. Web services facilitate seamless business-to-business integration. Whenever it becomes difficult to find a single service for a particular task, a composition of services that can together perform the given task, is required. In general, this is accomplished using Service Oriented Architecture (SOA). However, the requirements of the users are frozen before the system locates, composes and executes the required services. The response is not personalized to the user environment. Further, conventional web services cannot handle the context and the context aware web services need to contain the context processing logic. Hence, we propose a framework for dynamic composition of Web services using templates in SOA. This framework allows maximum flexibility for the users to change their requirements at runtime and provides adaptive composition irrespective of whether a web service is context enabled or not. © 2010 IEEE.
SCOPUS;2010;A Map-reduce system with an Alternate API for multi-core environments;Data-intensive computing,, Map-reduce,, Middleware,, Multi-core architectures;Map-reduce framework has received a significant attention and is being used for programming both large-scale clusters and multi-core systems. While the high productivity aspect of map-reduce has been well accepted, it is not clear if the API results in efficient implementations for different subclasses of data-intensive applications. In this paper, we present a system MATE (Map-reduce with an AlternaTE API), that provides a high-level, but distinct API. Particularly, our API includes a programmer-managed reduction object, which results in lower memory requirements at runtime for many data-intensive applications. MATE implements this API on top of the Phoenix system, a multi-core map-reduce implementation from Stanford. We evaluate our system using three data mining applications and compare its performance to that of both Phoenix and Hadoop. Our results show that for all the three applications, MATE outperforms Phoenix and Hadoop. Despite achieving good scalability, MATE also maintains the easy-to-use API of map-reduce. Overall, we argue that, our approach, which is based on the generalized reduction structure, provides an alternate highlevel API, leading to more efficient and scalable implementations. © 2010 IEEE.
SCOPUS;2010;An agent-based system to support assurance of security requirements;Multi-agents systems,, Secure Tropos,, Security assurance,, Security requirements,, Security verification;Current approaches to evaluating security assurance either focus on the software development stage or at the end product software. However, most often, it is after the deployment or implementation phase that specified security requirements may be violated. This may be due to improper deployment of the security measures, environmental hazards or to the fact that the assumptions under which the security requirements have been specified have become invalid. As such, this paper proposes an approach (supported by a system) which will complement security requirements engineering methodologies by gathering continuous evidence to inform on whether the security requirements elucidated during system development stage have been correctly implemented and as such, they can be relied upon to effectively protect system assets at runtime. We use Secure Tropos methodology to highlight the security assurance case and elicit the features of our security assurance evaluation system. We further depict the security assurance evaluation through an example based on firewalls configurations. © 2010 IEEE.
SCOPUS;2010;A goal-based framework for contextual requirements modeling and analysis;Context analysis,, Contextual requirements,, Goal modeling,, Requirements analysis;Requirements engineering (RE) research often ignores or presumes a uniform nature of the context in which the system operates. This assumption is no longer valid in emerging computing paradigms, such as ambient, pervasive and ubiquitous computing, where it is essential to monitor and adapt to an inherently varying context. Besides influencing the software, context may influence stakeholders' goals and their choices to meet them. In this paper, we propose a goal-oriented RE modeling and reasoning framework for systems operating in varying contexts. We introduce contextual goal models to relate goals and contexts,, context analysis to refine contexts and identify ways to verify them,, reasoning techniques to derive requirements reflecting the context and users priorities at runtime,, and finally, design time reasoning techniques to derive requirements for a system to be developed at minimum cost and valid in all considered contexts. We illustrate and evaluate our approach through a case study about a museum-guide mobile information system. © 2010 Springer-Verlag London Limited.
SCOPUS;2010;Requirements reflection: Requirements as runtime entities;reflection,, requirements,, runtime,, self-adaptive systems;Computational reflection is a well-established technique that gives a program the ability to dynamically observe and possibly modify its behaviour. To date, however, reflection is mainly applied either to the software architecture or its implementation. We know of no approach that fully supports requirements reflection- that is, making requirements available as runtime objects. Although there is a body of literature on requirements monitoring, such work typically generates runtime artefacts from requirements and so the requirements themselves are not directly accessible at runtime. In this paper, we define requirements reflection and a set of research challenges. Requirements reflection is important because software systems of the future will be self-managing and will need to adapt continuously to changing environmental conditions. We argue requirements reflection can support such self-adaptive systems by making requirements first-class runtime entities, thus endowing software systems with the ability to reason about, understand, explain and modify requirements at runtime. © 2010 ACM.
SCOPUS;2010;Dynamically adaptive systems through automated model evolution using service compositions;;Runtime adaptability and delivered quality are two important concerns for every system. One way to achieve runtime adaptability is by specifying variants in the system model at design time to allow switching between runtime configurations. The fulfillment of system's quality requirements depends on parameters that can change at runtime. In order to meet its quality requirements, the system must be able to dynamically adapt to changes that affect the delivered quality. We outline our approach to enhance system adaptability through automatic evolution of the system model. Our approach periodically updates the model by re-evaluating the delivered quality based on runtime information. We use a service composition model to represent the system functional requirements and annotate it with delivered quality evaluations. We ensure system runtime adaptability by selecting the variant to execute at runtime based on the evolved model. © 2010 Springer-Verlag.
SCOPUS;2010;Live goals for adaptive service compositions;goals,, requirements,, service compositions,, supervision;Service compositions represent an important family of self-adaptive systems. Though many approaches for monitoring and adapting service compositions have already been proposed, a clear connection with the motivations for using such techniques is still missing. To this aim we address self-adaptation from requirements elicitation down to execution. In this paper, we propose to enrich existing goal models with adaptive goals, responsible for the actual evolution/adaptation of the goal model at runtime. We also translate the goal model with both conventional and adaptive goals, into the actual functionality provided by the system and the adaptation policies needed to make it self-adapt. © 2010 ACM.
SCOPUS;2010;RELAX: A language to address uncertainty in self-adaptive systems requirement;Adaptive systems,, Fuzzy logic,, Requirements engineering,, Smart homes,, Uncertainty;Self-adaptive systems have the capability to autonomously modify their behavior at run-time in response to changes in their environment. Self-adaptation is particularly necessary for applications that must run continuously, even under adverse conditions and changing requirements,, sample domains include automotive systems, telecommunications, and environmental monitoring systems. While a few techniques have been developed to support the monitoring and analysis of requirements for adaptive systems, limited attention has been paid to the actual creation and specification of requirements of self-adaptive systems. As a result, self-adaptivity is often constructed in an ad-hoc manner. In order to support the rigorous specification of adaptive systems requirements, this paper introduces RELAX, a new requirements language for self-adaptive systems that explicitly addresses uncertainty inherent in adaptive systems. We present the formal semantics for RELAX in terms of fuzzy logic, thus enabling a rigorous treatment of requirements that include uncertainty. RELAX enables developers to identify uncertainty in the requirements, thereby facilitating the design of systems that are, by definition, more flexible and amenable to adaptation in a systematic fashion. We illustrate the use of RELAX on smart home applications, including an adaptive assisted living system. © 2010 Springer-Verlag London Limited.
SCOPUS;2010;Hybrid automata, reachability, and Systems Biology;Reachability,, Semi-algebraic automata,, Systems Biology;Hybrid automata are a powerful formalism for the representation of systems evolving according to both discrete and continuous laws. Unfortunately, undecidability soon emerges when one tries to automatically verify hybrid automata properties. An important verification problem is the reachability one that demands to decide whether a set of points is reachable from a starting region. If we focus on semi-algebraic hybrid automata the reachability problem is semi-decidable. However, high computational costs have to be afforded to solve it. We analyse this problem by exploiting some existing tools and we show that even simple examples cannot be efficiently solved. It is necessary to introduce approximations to reduce the number of variables, since this is the main source of runtime requirements. We propose some standard approximation methods based on Taylor polynomials and ad hoc strategies. We implement our methods within the software SAHA-Tool and we show their effectiveness on two biological examples: the Repressilator and the Delta-Notch protein signaling. © 2010 Elsevier B.V. All rights reserved.
SCOPUS;2009;Robust on-line model-based object detection from range images;Object detection,, Point clouds,, Range images;A mobile robot that accomplishes high level tasks needs to be able to classify the objects in the environment and to determine their location. In this paper, we address the problem of online object detection in 3D laser range data. The object classes are represented by 3D point-clouds that can be obtained from a set of range scans. Our method relies on the extraction of point features from range images that are computed from the point-clouds. Compared to techniques that directly operate on a full 3D representation of the environment, our approach requires less computation time while retaining the robustness of full 3D matching. Experiments demonstrate that the proposed approach is even able to deal with partially occluded scenes and to fulfill the runtime requirements of online applications. © 2009 IEEE.
SCOPUS;2009;Model transformation of dependability-focused requirements models;;Recent research has focused on extending standard requirements elicitation processes to address potential abnormal situations that can interrupt normal system interaction at run-time. We proposed a process, DREP, that extends use case-driven modelling with elements that allow the modelling of system behaviour in exceptional situations. This paper discusses the challenge of using the notions of exceptional behaviour and outcomes defined in use cases within a MDE process. In order to create a more formal specification model with activity diagrams, the use cases have to be well-formed to begin with. We describe precise transformation rules to systematically create an activity diagram corresponding to each use case. Special stereotypes are introduced to document partial or degraded outcomes and handling activities. The model resulting from the transformation unambiguously specifies the system interactions required to satisfy the user, as well as exceptional interactions that can lead to degraded service provision. © 2009 IEEE.
SCOPUS;2009;RELAX: Incorporating uncertainty into the specification of self-adaptive systems;;Self-adaptive systems have the capability to autonomously modify their behaviour at run-time in response to changes in their environment. Self-adaptation is particularly necessary for applications that must run continuously, even under adverse conditions and changing requirements,, sample domains include automotive systems, telecommunications, and environmental monitoring systems. While a few techniques have been developed to support the monitoring and analysis of requirements for adaptive systems, limited attention has been paid to the actual creation and specification of requirements of self-adaptive systems. As a result, self-adaptivity is often constructed in an ad-hoc manner. In this paper, we argue that a more rigorous treatment of requirements explicitly relating to self-adaptivity is needed and that, in particular, requirements languages for self-adaptive systems should include explicit constructs for specifying and dealing with the uncertainty inherent in self-adaptive systems. We present RELAX, a new requirements language for selfadaptive systems and illustrate it using examples from the smart home domain. © 2009 IEEE.
SCOPUS;2009;Specifying requirements for real-time systems;Duration Calculus,, Electronic Medical Record (EMR),, Emergent faults,, Memory leakages,, Real-time systems,, Software and system requirement;Software or System Requirements often change during their operational lifetime[4]. These changes take place due to misconceived requirements perceived during the requirement and design stages. Any changes in requirements are mostly evident during the run time of the system and are known as emergent faults. These emergent faults are unforeseen by the requirement engineers and so there is no built-in fault tolerance solution for the components in this system. In this paper we consider a real-time multi-network system of Alegent Health, where their application servers are used for critical Electronic Medical Record (EMR) application. These application servers frequently blow up due to memory leakages thus resulting in kicking off the users (doctor and nurses) of the system, elevating concerns on patient safety and also causing loss of revenue. We attribute this problem to be a misconceived requirement and as an emergent runtime fault. We propose a health monitoring system which will keep track of the health conditions of the servers with the objective of increasing overall system throughput, decreasing system downtime, and minimizing loss of data. We have proposed the use of the concept of Duration Calculus to specify the requirements of the system in terms of continuous state variables and discrete state variables.
SCOPUS;2009;Reasoning on non-functional requirements for integrated services;;We focus on non-functional requirements for applications offered by service integrators,, i.e., software that delivers service by composing services, independently developed, managed, and evolved by other service providers. In particular, we focus on requirements expressed in a probabilistic manner, such as reliability or performance. We illustrate a unified approach-a method and its support tools - which facilitates reasoning about requirements satisfaction as the system evolves dynamically. The approach relies on run-time monitoring and uses the data collected by the probes to detect if the behavior of the open environment in which the application is situated, such as usage profile or the external services currently bound to the application, deviates from the initially stated assumptions and whether this can lead to a failure of the application. This is achieved by keeping a model of the application alive at run time, automatically updating its parameters to reflect changes in the external world, and using the model's predictive capabilities to anticipate future failures, thus enabling suitable recovery plans. © 2009 IEEE.
SCOPUS;2009;Monitoring probabilistic properties;Performance,, Probabilistic properties,, Reliability,, Runtime monitoring,, Safety,, Security,, Web services;Monitoring allows for checking if a system fulfils its requirements at runtime. This is required for quality assurance purposes. Currently several approaches exist to monitor standard and timing properties. However, a current challenge is to provide a comprehensive approach for monitoring probabilistic properties, as they are used to formulate performance, reliability, safety, and availability requirements. The main problem of these probabilistic properties is that there is no binary acceptance condition. To overcome this problem, this paper describes a monitoring approach called ProMo that is based on acceptance sampling and sequential hypothesis testing. This approach is validated based on several experiments that have been performed on an example system which provides medical assistance in remote areas. Copyright 2009 ACM.
SCOPUS;2009;Towards development of Web applications based on User Interface Services - A requirement analysis;;In modern Web applications Web services are often used to encapsulate business logic to enable their reuse. By contrast User Interfaces are usually hand-crafted because standardized models for dynamic integration or reuse of UI components do not exist. We address this issue with an approach providing the dynamic and context-aware composition of User Interfaces based on exchangeable and customizable User Interface Services (UIS), which encapsulate UI parts. The central contribution of this paper is the discussion of the requirements for a runtime environment, which provides the novel approach of UI composition based on UIS. © 2009: CriMiCo'2009 Organizing Committee.
SCOPUS;2009;Feature Interactions in Software and Communication Systems X;;The proceedings contain 21 papers. The topics discussed include: goals and conflicts in telephony,, online detection of feature interactions of CPL services,, feature interaction detection in the feature language extensions,, feature interaction testing: an industrial perspective,, detecting policy conflicts by model checking UML state machines,, feature interactions in aspect-oriented scenario models,, feature interactions in a software product line for e-voting,, feature diagrams for change-oriented programming,, feature interactions in object-oriented effect systems from a viewpoint program comprehension,, feature interaction as a context sharing problem,, software security vulnerabilities seen as feature interactions,, interactions among secrecy models,, problem-solution feature interactions as configuration knowledge in distributed runtime adaptations,, and semantic-based aspect interaction detection with goal models.
SCOPUS;2009;ASE2009 - 24th IEEE/ACM International Conference on Automated Software Engineering;;The proceedings contain 89 papers. The topics discussed include: a Petri net based debugging environment for QVT relations,, validating automotive control software using instrumentation-based verification,, semi-automated test planning for e-ID systems by using requirements clustering,, understanding the value of software engineering technologies,, evaluating the accuracy of fault localization techniques,, spectrum-based multiple fault localization,, clone-aware configuration management,, Looper: lightweight detection of infinite loops at runtime,, improving the efficiency of dependency analysis in logical decision models,, design rule hierarchies and parallelism in software development tasks,, a divergence-oriented approach to adaptive random testing of Java programs,, self-repair through reconfiguration: a requirements engineering approach,, and inferring resource specifications from natural language API documentation.
SCOPUS;2009;A comparison of metaheuristics on a practial staff scheduling problem;Evolution strategy,, Metaheuristic,, Particle swarm optimization,, Staff scheduling,, Sub-daily planning;A practical scenario from logistics is used in this paper to compare different variants of particle swarm optimisation (PSO) and the evolution strategy (ES). Rapid, sub-daily planning with metaheuristics, which is the focus of our research, can significantly add to the improvement of staff scheduling in practice. PSO outperformes ES on this problem. The superior performance must be attributed to the operators and parameters of PSO since the coding of PSO and ES are identical. Repairing solutions to reduce the violation of soft constraints significantly improves the quality of results for both metaheuristics, although the runtime requirements are approximately doubled.
SCOPUS;2009;Property analysis of visual behavior models to code transformation;Animation,, Metamodeling,, Runtime complexity,, Termination;Nowadays, when visual modeling is becoming more and more popular, it is still an open issue how to model the runtime behavior (animation) of visual languages. We are currently working on a complete solution to this issue, we have specified visual languages that can describe the behavior of arbitrary metamodeled visual language and we have also provided a graph-rewriting-based transformation which processes these 'animation' models and generates executable source code. This paper shortly introduces previous work, and focuses on the analysis of the runtime properties of the transformation. We performed termination analysis on the transformation, and examined the runtime requirements of the algorithm, based on the size of the input models. We have also verified that the transformation processes topologically correct models only. We present generic techniques which are applicable not only in connection with this concrete case, but with arbitrary other graph-rewriting based model transformations.
SCOPUS;2009;Frequent itemsets hiding: A performance evaluation framework;;Sensitive knowledge hiding is an essential requirement to prevent disclosure of any sensitive knowledge holding in shared databases. The security of a database may be risked when it is made public as is: because the data mining tools are so sophisticated that the sensitive knowledge can easily be surfaced by receivers. This gives rise to a sanitization process which transforms the original database into another database, the released one, which does not hold the sensitive knowledge but can substitute the original otherwise. In case the sensitive knowledge is of the form frequent itemsets, the resulting concrete problem is called frequent itemsets hiding. A number of algorithms, exploiting different approaches and techniques, for frequent itemsets hiding problem is proposed in the literature. Since finding optimal solutions is NP-Hard, algorithms resort to certain heuristics having different levels of sophistication, complexity, efficiency and effectiveness. This paper presents an evaluation framework which implements recent algorithms belonging to different approaches and a set of metrics to gauge the performance and problem difficulties. The current work also presents an experimental study and its results where four algorithms and seven datasets are involved. Our results indicate that data distortion levels and runtime requirements are quite high, especially for difficult problem instances. Our conclusion is that there are new rooms for more sophisticated and tuneable (w.r.t. effectiveness/efficiency tradeoff) algorithms. © 2009 IEEE.
SCOPUS;2009;Efficient large-scale model checking;;Model checking is a popular technique to systematically and automatically verify system properties. Unfortunately, the well-known state explosion problem often limits the extent to which it can be applied to realistic specifications, due to the huge resulting memory requirements. Distributedmemory model checkers exist, but have thus far only been evaluated on small-scale clusters, with mixed results. Weexamine one well-known distributed model checker, DiVinE, in detail, and show how a number of additional optimizations in its runtime system enable it to efficiently check very demanding problem instances on a large-scale, multicore compute cluster. We analyze the impact of the distributed algorithms employed, the problem instance characteristics and network overhead. Finally, we show that the model checker can even obtain good performance in a highbandwidth computational grid environment. © 2009 IEEE.
SCOPUS;2009;Engineering adaptive requirements;;Challenges in the engineering of self-adaptive software have been recently discussed and summarised in a seminal research road map. Following it, we focus on requirements engineering issues, with a two-fold, long term objective. The first objective is to support the system analyst to engineer adaptive requirements at requirements-time, the second is to make software able to reason on requirements at run-time in order to enable a goal-oriented adaptation. Along the first objective, in this position paper we propose a characterisation of adaptive requirements. Moreover, we investigate how available techniques aimed at eliciting and specifying domain properties, stakeholders' goals and preferences, can provide a practical support to the analyst while capturing adaptive requirements. © 2009 IEEE.
SCOPUS;2009;Proceedings of the 2009 ICSE Workshop on Aspect-Oriented Requirements Engineering and Architecture Design, EA 2009;;The proceedings contain 9 papers. The topics discussed include: automating the discovery of stable domain abstractions for reusable aspects,, runtime monitoring of cross-cutting policy,, support for aspectual modeling to multiagent system architecture,, heterogeneous pointcut expressions,, using tagging to identify and organize concerns during pre-requirements analysis,, concern tracing and change impact analysis: an exploratory study,, on modeling interactions of early aspects with goals,, promoting the software evolution in AOSD with early aspects: architecture-oriented model-based pointcuts,, and model driven development with non-functional aspects.
SCOPUS;2009;Runtime monitoring of cross-cutting policy;;In open systems, certain unfavorable situations due to unanticipated user behavior may be seen, which results in a violation of cross-cutting policy. This paper proposes a runtime monitoring method to check such problems. Since there is a large gap, a certain link is needed between the policy and runtime execution method. We employ a two-step checking approach,, an offline symptom checking and a runtime monitoring. The ingredient to tie the two steps is a Linear-time Temporal Logic formula for the cross-cutting policy to look at. © 2009 IEEE.
SCOPUS;2009;From requirements to embedded software - Formalising the key steps;;Failure of a design to satisfy a system's requirements can result in schedule and cost overruns. When using current approaches, ensuring requirements are satisfied is often delayed until late in the development process during a cycle of testing and debugging. This paper introduces a more rigorous approach to design using Behavior Engineering, which has previously been applied primarily to requirements analysis and specification development. To support design with Behavior Engineering we introduce the embedded Behavior Runtime Environment, a virtual machine created to execute a Behavior Engineering design on an embedded system. The result is a model-driven development approach that can create embedded system software that satisfies its requirements, as a result of applying the development process.
SCOPUS;2009;Situ: A situation-theoretic approach to context-aware service evolution;Context aware,, Hidden Markov chain,, Human intention,, Requirements,, Runtime,, Service,, Situation theoretic,, Smart home,, Software evolution;Evolvability is essential for computer systems to adapt to the dynamic and changing requirements in response to instant or delayed feedback from a service environment that nowadays is becoming more and more context aware,, however, current context-aware service-centric models largely lack the capability to continuously explore human intentions that often drive system evolution. To support service requirements analysis of real-world applications for services computing, this paper presents a situation-theoretic approach to human-intention-driven service evolution in context-aware service environments. In this study, we give situation a definition that is rich in semantics and useful for modeling and reasoning human intentions, whereas the definition of intention is based on the observations of situations. A novel computational framework is described that allows us to model and infer human intentions by detecting the desires of an individual as well as capturing the corresponding context values through observations. An inference process based on Hidden Markov Model makes instant definition of individualized services at runtime possible, and significantly, shortens service evolution cycle. We illustrate the possible applications of this framework through a smart home example a imed at supporting independent living of elderly people. © 2009 IEEE.
SCOPUS;2009;Critical infrastructures security modeling, enforcement and runtime checking;Access control enforcement,, Runtime model checking,, Security of critical infrastructures,, Security policies and models;This paper identifies the most relevant security requirements for critical infrastructures (CIs), and according to these requirements, proposes an access control framework. The latter supports the CI security policy modeling and enforcement. Then, it proposes a runtime model checker for the interactions between the organizations forming the CIs, to verify their compliance with previously signed contracts. In this respect, not only our security framework handles secure local and remote accesses, but also audits and verifies the different interactions. In particular, remote accesses are controlled, every deviation from the signed contracts triggers an alarm, the concerned parties are notified, and audits can be used as evidence for sanctioning the party responsible for the deviation. © 2009 Springer Berlin Heidelberg.
SCOPUS;2009;Requirements tracing to support change in dynamically adaptive systems;Adaptive Systems,, Requirements Evolution,, Traceability;[Context and motivation] All systems are susceptible to the need for change, with the desire to operate in changeable environments driving the need for software adaptation. A Dynamically Adaptive System (DAS) adjusts its behaviour autonomously at runtime in order to accommodate changes in its operating environment, which are anticipated in the system's requirements specification. [Question/Problem] In this paper, we argue that Dynamic Adaptive Systems' requirements specifications are more susceptible to change than those of traditional static systems. We propose an extension to i*strategic rationale models to aid in changing a DAS. [Principal Ideas/Results] By selecting some of the types of tracing proposed for the most complex systems and supporting them for DAS modelling, it becomes possible to handle change to a DAS' requirements efficiently, whilst still allowing artefacts to be stored in a Requirements Management tool to mitigate additional complexity. [Contribution] The paper identifies different classes of change that a DAS' requirements may be subjected to, and illustrates with a case study how additional tracing information can support the making of each class of change. © 2009 Springer Berlin Heidelberg.
SCOPUS;2009;Engineering an interoperable multimedia assessment authoring and run-time environment conforming to IMS QTI;Computer based assessment,, E-learning,, IMS QTI,, Innovation,, Interoperability,, Learning standards;In this paper we present the development of a multimedia assessment tool conforming to the latest version of IMS QTI learning standard. Initially, a presentation of the development of IMS QTI is given and relevant research papers are analysed. Then, the requirements analysis and the development decisions for the assessment tool are discussed. At the end of the paper the assessment tool is evaluated and the method of validating its output is presented. Copyright © 2009, Inderscience Publishers.
SCOPUS;2009;Automatic multiple-zone rigid-body refinement with a large convergence radius;Multiple-zone protocols,, Rigid-body refinement;Rigid-body refinement is the constrained coordinate refinement of one or more groups of atoms that each move (rotate and translate) as a single body. The goal of this work was to establish an automatic procedure for rigid-body refinement which implements a practical compromise between runtime requirements and convergence radius. This has been achieved by analysis of a large number of trial refinements for 12 classes of random rigid-body displacements (that differ in magnitude of introduced errors), using both least-squares and maximum-likelihood target functions. The results of these tests led to a multiple-zone protocol. The final parameterization of this protocol was optimized empirically on the basis of a second large set of test refinements. This multiple-zone protocol is implemented as part of the phenix.refine program. © 2009 International Union of Crystallography.
SCOPUS;2009;Analysis of problem frame and properties materiel requirements;Key artifacts,, Materiel requirements,, Materiel requirements reflection,, Problem frame,, Weapon materiel;In order to extropolate what are involved in materiel requirements, problem frame for materiel requirements is provided and substantive properties which belong to materiel requirements are rigorously analyzed based on key artifacts, which are distilled from the wide variations in the use of terms. Problem frame for materiel requirements focuses on the key artifacts, their attributes, and relationships at a general level. By using higher-order logical notational conventions, formal analyses are made, describing the relations that material requirements and their reflection must satisfy. Adequacy, precondition, feasibility, necessity, and hierarchy of materiel requirements are included. Adequacy, necessity, and guidablity of materiel requirements reflection are also included. Finally, an example about the presumed urban antimissile defense system shows that the approach is highly feasible, effective and promising.
SCOPUS;2009;Specifying and monitoring interactions and commitments in open business processes;Business processes,, Open software systems,, Requirements analysis,, Runtime monitoring,, Web services;Business processes are increasingly complex and open because they rely on services that are distributed geographically and across organizations. So, they're prone to several points of failure. Monitoring, therefore, remains an important concern. A new approach specifies and monitors interactions among heterogeneous services by tracking their commitments. This approach extends recent research that views business process design as a composition of interaction protocols. It specifies and monitors policies and commitments as a way to monitor service-level agreements and recover from process failures. © 2009 IEEE.
SCOPUS;2009;Requirements evolution and what (research) to do about it;Evolution,, Monitoring,, Requirements,, Satisfiability;Requirements evolution is a research problem that has received little attention hitherto, but deserves much more. For systems to survive in a volatile world, where business needs, government regulations and computing platforms keep changing, software systems must evolve too in order to survive. We discuss the state-of-the-art for research on the topic, and predict some of the research problems that will need to be addressed in the next decade. We conclude with a concrete proposal for a run-time monitoring framework based on (requirements) goal models. © Springer-Verlag Berlin Heidelberg 2009.
SCOPUS;2009;Operational semantics of goal models in adaptive agents;Agent programming,, Formal semantics,, Goal models,, Goals;Several agcnt-oricnted software engineering methodologies address the emerging challenges posed by the increasing need of adaptive software. A common denominator of such methodologies is the paramount importance of the concept of goal model in order to understand the requirements of a software system. Goal models consist of goal graphs representing AND/OR-dccomposition of abstract goals down to operationalisable leaf-level goals. Goal models are used primarily in the earlier phases of software engineering, for social modelling, requirements elicitation and analysis, to concretise abstract objectives, to detail them and to capture alternatives for their satisfaction. Although various agent programming languages incorporate the notion of (leaf-level) goal as a language construct, none of them natively support the definition of goal mod-els. However, the semantic gap between goal models used at design-time and the concept of goal used at implementation and execution time represent a limitation especially in the development of self-adaptive and fault-tolerant systems. In such systems, design-time knowledge on goals and variability becomes relevant at run-time, to take autonomous decisions for achieving high level objectives correctly. Recently, unifying operational semantics for (leaf) goals have been proposed [15]. We extend this work to define an operational semantics for the behaviour of goals in goal models, maintaining the flexibility of using different goal types and conditions. We use a simple example to illustrate how the proposed approach effectively deals with the semantic gap between design-time goal models and run-time agent implementations. Categories and Subject Descriptors 1.2.11 [Artificial intelligence]: Distributed Artificial Intelligence-Intelligent agents, Languages and structures,, D.2.1 [Software Engineering]: Requirements/Specifications,, D.3.1 [Programming Languages]: Formal Definitions and Theory General Terms. Copyright © 2009, International Foundation for Autonomous Agents and Multiagent Systems.
SCOPUS;2009;Software self-reconfiguration: A BDI-based approach;BDI,, Multi-agent systems,, Self-reconfiguration;Software self-reconfiguration is the capability of software systems to change autonomously their current configuration to a better one. This is a more and more requested feature, particularly for software systems that operate in critical domains when human intervention is not possible or not convenient. The Belief-Desire-Intention (BDI) architecture proposes a structured Monitor-Diagnose-Compensate cycle that partially meets self-reconfiguration requirements. We propose a realization of the abstract BDI control loop and we draw generic solutions to support the self-reconfiguration process. We aim at supporting traceability and runtime monitoring of requirements and we base our solution on Tro- pos goal models to structure agents' internal state. Copyright © 2009, International Foundation for Autonomous Agents and Multiagent Systems.
SCOPUS;2008;PEM fuel cells versus diesel generators which solution to pick?;;Hurricane Katrina and the pursuant ruling by the FCC has caused many Telecom operators to review and find solutions to provide extended runtime in critical areas of their network. In the past, lead acid batteries have dominated the backup power market for these sites and the power grid quality was considered robust. Katrina forced operators to look past this and it revealed that lead acid batteries typically only provide up to eight hours of backup for a site. For those sites where operators require extended runtime, a diesel generator was typically the only option. This paper will provide an overview of not only the diesel solution but also for a new solution that utilizes a Proton Exchange Membrane Fuel Cell (PEMFC) and premium lead acid batteries to provide extended runtime at the site Using a 5kW base station plant, we will examine the architecture and design solutions that will give operators well over 48 hours of backup and in some cases over 160 hours. The PEMFC solution can utilize advanced lead acid front terminal batteries to ease maintenance as well as a robust and cost effective PEMFC engine. Critical areas for the mainstream success of fuel cells include total solution costs, reliability, fuel supply, maintenance and Hydrogen storage. The comparative diesel will be the embedded 30kW solution that is normally found in sites throughout these applications. Issues associated with generator operation include noise, emissions, permitting, fuel supply and maintenance costs will all be examined. The goal will be to show a total cost of ownership for both solutions and educate operators about new approaches for their extended runtime requirements. ©2008 IEEE.
SCOPUS;2008;Agent based seamless context-aware service for moving learners in ubiquitous computing environment;;We are focusing on the applicability to agent based mechanism with context-awareness and requirement engineering approaches with u-learning service scenario, which has a basis on ubiquitous WPAN technologies. Our proposed approach intends to design application service model that provide users with customized learning contents stepwise through synchronizing participant's previous learning experience with a current device using agents. Furthermore, dynamic learning contents with various contexts was managed and refactored to the relevant cluster in viewing of reusability on the meta model at run time, so efficiency and effectiveness for U-learning at U-home/U-office could be implemented through the proposed service model. © 2008 IEEE.
SCOPUS;2008;Multi-level system integration based on autosar;Automatic Code-Generation,, Automotive,, Embedded Software,, Rapid Prototyping;The design of distributed embedded real-time system is a challenging task. Besides solving the control-engineering issues, one has to consider real-time scheduling, reliability and production requirements w.r.t. production cost of the electronic control unit (ECU). This has a considerable impact on the employed software design techniques. These design techniques are well known in the automotive software industry, but are applied with different flavors at each vehicle manufacturer and their suppliers. This situation has changed considerably with the results of the AUTOSAR development partnership, which unifies the flavors of automotive software design. Automotive software design is embedded in the so-called V-Cycle of embedded automotive system development [1]. It starts with the requirements analysis which results later on in a model of the control algorithm. The control algorithm is tested against a vehicle model and establishes the topmost level in system integration. The second system integration level is the adaptation of the control algorithm to be run on a rapid-prototyping system. The rapid-prototyping system is integrated into an existing E/E-architecture. The E/E-architecture consists of the ECUs connected by networks like CAN or FlexRay and gateways. Sensors- and actuators being used by several control algorithms are coupled to an ECU, which might propagate signals to other ECUs via a vehicle network. From the software point of view, the control-algorithm has now to respect real-time scheduling and the quantization of the sensor- and actuator signals, no matter whether these signals are generated on the rapid-prototyping system or exchanged via the bus with other ECUs. Further development steps in the V-cycle are the software implementation, the ECU- and the network integration. The integrated ECUs and networks are tested against vehicle models running on Hardware-in-the-loop (HiL) systems. If this works fine, the ECUs are integrated in the real vehicle for calibration. The software implementation and the ECU integration are deeply influenced by AUTOSAR. AUTOSAR is a development partnership of all stakeholders in the automotive software development (e.g. vehicle manufacturers and their suppliers) which unifies several software implementation techniques[2]. It describes a common ECU software architecture[3] consisting of configurable basic software modules (BSW), a runtime environment (RTE) and a software component description[4]. The software component description describes the interfaces for data-exchange as well as the access points for the RTE. The basic idea of AUTOSAR is to establish a so-called virtual functional bus (VFB) consisting of interconnected software components which are later mapped to an E/E-architecture. Currently, the VFB structure of AUTOSAR software architectures is mainly driven by the next generation E/E-architectures. The VFB structure forms the third system integration level. The next integration step into an AUTOSAR environment is the integration of the rapid-prototyping tested control algorithm to AUTOSAR software components. Since most VFB descriptions use fixed-point interfaces, the control algorithm has to be transformed to fixed-point arithmetic. If the control algorithm is modelled in tools like ASCET, this conversion can be achieved by code-generation. The same holds true for control algorithms already being used in E/E-architectures without an AUTOSAR software architecture. The VFB-description with the control-algorithms forms the fourth system integration level, which can be simulated with plant-models on a PC by tools like INTECRIO-VP. The fifth system integration level is given by the mapping process as defined in the AUTOSAR methodology[5] and requires the configuration of the RTE and the BSW modules for a single ECU. At this integration level, one can perform HiL testing and calibration in the same way as for non-AUTOSAR systems. Several evaluation projects, e.g. [6] and [7], have shown that the multi-level integration approach is feasible to guide the configuration capabilities of AUTOSAR software architectures.
SCOPUS;2008;Engineering pluripotent information systems;;A pluripotent information system is an open and distributed information system that (i) automatically adapts at runtime to changing operating conditions, and (ii) satisfies both the requirements anticipated at development time, and those unanticipated before but relevant at runtime. Engineering pluripotency into an information system therefore responds to two recurring critical issues: (i) the need for adaptability given the uncertainty in a system's operating environment, and (ii) the difficulty to fully anticipate and account for all possible stakeholders' requirements at development time and respond to the change of requirements at runtime. We draw on our group's research efforts over the last two years to show and discuss how pluripotency can be engineered into information systems.
SCOPUS;2008;GPU-MEME: Using graphics hardware to accelerate motif finding in DNA sequences;;Discovery of motifs that are repeated in groups of biological sequences is a major task in bioinformatics. Iterative methods such as expectation maximization (EM) are used as a common approach to find such patterns. However, corresponding algorithms are highly compute-intensive due to the small size and degenerate nature of biological motifs. Runtime requirements are likely to become even more severe due to the rapid growth of available gene transcription data. In this paper we present a novel approach to accelerate motif discovery based on commodity graphics hardware (GPUs). To derive an efficient mapping onto this type of architecture, we have formulated the compute-intensive parts of the popular MEME tool as streaming algorithms. Our experimental results show that a single GPU allows speedups of one order of magnitude with respect to the sequential MEME implementation. Furthermore, parallelization on a GPU-cluster even improves the speedup to two orders of magnitude. © 2008 Springer Berlin Heidelberg.
SCOPUS;2008;Secure workflow development from early requirements analysis;;Requirements engineering is being increasingly adopted as a key step in the software development process and so new challenges and possibilities emerge. Designing of web services and developing of business processes and workflows for web services is one of the most thought challenging issues in requirements engineering. The research on web services design is well under way, but the existing design methodologies for web services do not address the issue of developing secure web services, secure business processes and secure workflows. For the purpose of developing secure workflows based on the early requirements analysis, in this work, we propose a refinement methodology and a language that allows the workflow engine to automatically enforce trust and delegation requirements. Those workflows are then to be distributed,, the security aspects being enforced dynamically at runtime accordingly to the identified requirements. To make the discussion more concrete, we illustrate the proposal with an e-business banking case study. © 2008 IEEE.
SCOPUS;2008;Supporting the UML state machine diagrams at runtime;Model executability,, Statecharts,, UML;Input models that are not completely checked generate ill-formed output models in MDA transformation processes. Model executability is a means for, at development time, simulating/testing models and thus making them compliant with requirements. At runtime, persistent models bring added values like the monitoring and control of applications through the observation of the active states, the guards which hold true, the occurring events... This paper on purpose presents a Java-based execution engine for the UML State Machine Diagrams. In order to incorporate this UML interpreter into MDA tools, the execution semantics of the UML State Machine Diagrams is first analyzed and next disambiguated. Execution semantics choices are thus proposed and justified accordingly. © 2008 Springer-Verlag Berlin Heidelberg.
SCOPUS;2008;Requirements capture with RCAT;;NASA spends millions designing and building spacecraft for its missions. The dependence on software is growing as spacecraft become more complex. With the increasing dependence on software comes the risk that bugs can lead to the loss of a mission. At NASA's Jet Propulsion Laboratory new tools are being developed to address this problem. Logic model checking [9] and runtime verification [5] can increase the confidence in a design or an implementation. A barrier to the application of such property-based checks is the difficulty in mastering the requirements notations that are currently available. For these techniques to be easily usable, a simple but expressive requirement specification method is essential. This paper describes a requirements capture notation and supporting tool that graphically captures formal requirements and converts them into automata that can be used in model checking and for runtime verification. © 2008 IEEE.
SCOPUS;2008;Compliance of semantic constraints - A requirements analysis for process management systems;Compliance validation and enforcement,, Process management systems,, Semantic constraints,, Semantic process verification;Key to the use of process management systems (PrMS) in practice is their ability to facilitate the implementation, execution, and adaptation of business processes while still being able to ensure error-free process executions. Mechanisms have been developed to prevent errors at the syntactic level such as deadlocks. In many application domains, processes often have to comply with business level rules and policies (i.e., semantic constraints). Hence, in order to ensure error-free executions at the semantic level, PrMS need certain control mechanisms for validating and ensuring the compliance with semantic constraints throughout the process lifecycle. In this paper, we discuss fundamental requirements for a comprehensive support of semantic constraints in PrMS. Moreover, we provide a survey on existing approaches and discuss to what extent they meet the requirements and which challenges still have to be tackled. Finally, we show how the challenge of life time compliance can be dealt with by integrating design time and runtime process validation.
SCOPUS;2008;Building contingencies into specifications;;We propose an approach to runtime feature composition and conflict resolution that combines arbitration and contingencies. By arbitration we mean the resolution of conflicts between features using priorities. Contingency means having several specifications per feature, satisfying the same requirement, depending on the current state of the shared resource. Evaluation of our approach shows that combining arbitration and contingencies ensures that in the event of a conflict, requirements of the conflicting features are eventually satisfied. © 2008 IEEE.
SCOPUS;2008;Non-uniform yield optimization for integrated circuit layout considering global interactions;Critical area analysis (CAA),, Design for manufacturability (DFM),, Litho-friendly design (LFD),, Manufacturing yield,, Optical proximity correction (OPC),, Resolution enhancement techniques (RET),, Yield optimization;In a previous work we have shown a yield optimization metric and a technique that considers the effects of several types of yield enhancement methods for a given layout. Those findings suggested that it is important to consider two types of yield tradeoffs, local tradeoffs where addressing one yield loss mechanism degrades others in the immediate vicinity of the correction (local optimization window), and global tradeoffs where the net effect of the correction can be fully accounted only when considering neighboring optimization windows. Such conclusion was derived from the fact that the locally optimized layouts did not completely realize the theoretically optimal yield, which was obtained from the assumption that global tradeoffs could be fully resolved. This work focuses in the contribution that such global tradeoffs have on the final yield score when accounted properly during the optimization. While the previous work focused only in selecting the corrections that locally improved the yield score1, this work evaluates the global interactions before and after a change, and the correction is only accepted if it improves the global score. While the global optimization requires a more expensive computational process, the intention of this work is to determine how close the optimal layout can be from its theoretical limit. Since the optimization is performed and evaluated under four different types of processes in which the failure mechanisms vary in relative importance, it is possible to derive conclusions as to the need of considering global effects when trading off runtime requirements with quality of the correction. © 2008 SPIE.
SCOPUS;2008;Automated mapping from goal models to self-adaptive systems;;Self-adaptive systems should autonomously adapt at run time to changes in their operational environment, guided by the goals assigned by their stakeholders. We present a tool that supports goal-oriented modelling and generation of code for goal-directed, self-adaptive systems, supporting Tropos4AS, an extension of the software engineering methodology Tropos. © 2008 IEEE.
SCOPUS;2008;From monitoring templates to security monitoring and threat detection;Event-calculus,, Intrusion-detection,, Patterns,, Run-time monitoring,, Security;This paper presents our pattern-based approach to runtime requirements monitoring and threat detection being developed as part of an approach to build frameworks supporting the construction of secure and dependable systems for ambient intelligence. Our patterns infra-structure is based on templates. From templates we generate Event-Calculus formulas expressing security requirements to monitor at run-time. From these theories we generate attack signatures, describing threats or possible attacks to the system. At run-time, we evaluate the likelihood of threats from run-time observations using a probabilistic model based on Bayesian networks. © 2008 IEEE.
SCOPUS;2008;Accelerating molecular dynamics simulations using Graphics Processing Units with CUDA;Advanced computer architecture,, Graphics processing unit,, Molecular dynamics;Molecular dynamics is an important computational tool to simulate and understand biochemical processes at the atomic level. However, accurate simulation of processes such as protein folding requires a large number of both atoms and time steps. This in turn leads to huge runtime requirements. Hence, finding fast solutions is of highest importance to research. In this paper we present a new approach to accelerate molecular dynamics simulations with inexpensive commodity graphics hardware. To derive an efficient mapping onto this type of computer architecture, we have used the new Compute Unified Device Architecture programming interface to implement a new parallel algorithm. Our experimental results show that the graphics card based approach allows speedups of up to factor nineteen compared to the corresponding sequential implementation. © 2008 Elsevier B.V. All rights reserved.
SCOPUS;0;Integrating FPGA acceleration into HMMer;Accelerators,, Bioinformatics,, Hidden Markov models,, Reconfigurable computing,, Viterbi algorithm;HMMer is a commonly used package for biological sequence database searching with profile hidden Markov model (HMMs). It allows researchers to compare HMMs to sequence databases or sequences to HMM databases. However, such searches often take many hours on traditional computer architectures. These runtime requirements are likely to become even more severe due to the rapid growth in size of both sequence and model databases. We present a new reconfigurable architecture to accelerate the two HMMer database search procedures hmmsearch and hmmpfam. It is described how this leads to significant runtime savings on off-the-shelf field-programmable gate arrays (FPGAs). © 2008 Elsevier B.V. All rights reserved.
SCOPUS;2008;A human-machine dimensional inference ontology that weaves human intentions and requirements of context awareness systems;;Changing system requirements, especially for context awareness (CA) systems, often cause modifications in the software systems in order to adapt to dynamic environments. Since the requirements may become temporarily obsolete or contrary to human intentions, the CA systems need to be tuned to resolve the conflict. On the other hand, most CA design methods rely on pre-defined requirements and reasoning engine, thus, fail to address all the possible situations. Consequently, services provided by such a CA system are limited to accommodate some situations and unable to react as expected. Therefore, it is critical for CA systems to capture exceptions at runtime, infer changed human intentions, and adapt to these changes. This study focuses on inference of ever-changing human intentions and monitoring human intentions to handle system evolution. In this paper, we present an inference mechanism of human intentions via the Human-machine Dimensional Inference Ontology (HDIO). This ontology gives inference rules based on the BDI logic to deduce human intentions from contexts. Furthermore, the inference exercises of a healthcare system example shows how user intentions relate to system requirements and how they help improve self-adaptability of CA systems. © 2008 IEEE.
SCOPUS;2008;Dynamic grid scheduling using job runtime requirements and variable resource availability;;We describe a scheduling technique in which estimated job runtimes and estimated resource availability are used to efficiently distribute workloads across a homogeneous grid of resources with variable availability. The objective is to increase efficiency by minimizing job failure caused by resources becoming unavailable. Optimal scheduling will be accomplished by mapping jobs onto resources with sufficient availability. Both the scheduling technique and the implementation called PGS (Prediction based Grid Scheduling) are described in detail. Results are presented for a set of sleep jobs, and compared with a first come, first serve scheduling approach. © 2008 Springer-Verlag Berlin Heidelberg.
SCOPUS;2008;When to adapt? Identification of problem domains for adaptive systems;Adaptive systems,, Non-functional requirements;Dynamically adaptive systems (DASs) change behaviour at run-time to operate in volatile environments. As we learn how best to design and build systems with greater autonomy, we must also consider when to do so. Thus far, DASs have tended to showcase the benefits of adaptation infrastructures with little understanding of what characterizes the problem domains that require run-time adaptation. This position paper posits that context-dependent variation in the acceptable trade-offs between non-functional requirements is a key indicator of problems that require dynamically adaptive solutions. © 2008 Springer-Verlag Berlin Heidelberg.
SCOPUS;2008;Refining goal models by evaluating system behaviour;;Nowadays, information systems have to perform in complex, heterogeneous environments, considering a variety of system users with different needs and preferences. Software engineering methodologies need to cope with the complexity of requirements specification in such scenarios, where new requirements may emerge also at run-time and the system's goals are expected to evolve to meet new stakeholder needs. Following an agent-oriented approach, we are studying methods and techniques to design adaptive and evolvable information systems able to fulfill stakeholders' objectives. In a previous work we defined an Agent-Oriented framework to design and code system specifications in terms of goal models and we instantiated it in a tool supported process which exploits the Agent-Oriented Software Engineering methodology Tropos and the Multi-Agent Platform JADE/Jadex [11]. In this paper, we show how to use this framework to develop a system following an iterative process, where the system execution allows enriching the system specification given in terms of goal models. Experimental evaluation has been performed on a simple example and lead to the refinement of the designed goal model upon the analysis of the system's run-time behaviour. © 2008 Springer-Verlag Berlin Heidelberg.
SCOPUS;2008;A reflective framework for fine-grained adaptation of aspect-oriented compositions;;Dynamic Aspect Oriented Programming (AOP) technologies typically provide coarse-grained mechanisms for adapting aspects that cross-cut a system deployment,, i.e. whole aspect modules can be added and removed at runtime. However, in this paper we demonstrate that adaptation of the finer-grained elements of individual aspect modules is required in highly dynamic systems and applications. We present AspectOpenCOM, a principled, reflection-based component framework that provides a meta object protocol capable of fine-grained adaptation of deployed aspects. We then evaluate this solution by eliciting a set of requirements for dynamic fine-grained adaptation from a series of case studies, and illustrate how the framework successfully meets these criteria. We also investigate the performance gains of fine-grained adaptation versus a coarse-grained approach. © 2008 Springer-Verlag Berlin Heidelberg.
SCOPUS;2007;Setting and evaluation of flexible points on software user interface;;In order to adapt to user requirement changes at runtime, software provides adaptable operations through user interfaces to change software functionality. We suggest the FleXible Point (FXP), flexible changes, flexible degree, flexible force, and flexible distance to evaluate the effect of such user interfaces. An approach and a case are given to illustrate the evaluation and setting of the FXPs. Quantitative relationship between flexible point on user interface and software flexibility is discussed and expressed. The approach can be used as a guide to adjust, improve, and to compare the FXPs on user interfaces. It is a direction for managers to arrange different levels of manipulators to increase the FXP efficiency and bring user interface flexibility into play. © 2007 IEEE.
SCOPUS;2007;Adaptive agent model: An agent interaction and computation model;;Software systems must be capable of coping with continuous requirements changes and at the same time wisely make use of emerging components and services to remain useful in their environment. In this paper, the Adaptive Agent Model (AAM) approach is proposed. The AAM uses configurable interaction model to drive adaptive agent behaviour. The model captures user requirements and is maintained by experts at a high level of abstraction. The AAM interaction model has been discussed with regard to interaction specification and interaction coordination, in line with a coordination language for the OpenKnowledge project. A major benefit of using the approach is agents can dynamically choose disparate components and services already developed for computation via their interaction with each other at runtime, when a new interaction model has been configured for them towards an emerging business goal. A simple expert seeking scenario has been used to illustrate the approach. © 2007 IEEE.
SCOPUS;2007;Architectural adaptation addressing the criteria of multiple quality attributes in mission-critical systems;;Mission-critical software claims safe and robust adaptations that comply with rigorous criteria of multiple critical quality attributes. Existing adaptation approaches pay little attention to comprehensively capture mission goals and explicitly specify adaptation requirements. We propose an approach to using scenario-based analysis to elicit and specify the criteria of multiple quality attributes as adaptation invariants, and design corresponding architecture variants as facilities implementing adaptations. We also present how to make adaptation decisions at runtime. ©2007 IEEE.
SCOPUS;2007;Delay optimization for airspace capacity management with runtime and equity considerations;;En route air traffic management is difficult and can benefit greatly from decision support tools. This paper presents a study of the efficiency and effectiveness of two practical approaches to real-time scheduling algorithms: a simple greedy scheduler and a well-studied optimal scheduler. A subset (region) of the National Airspace System is isolated to perform optimization on a manageable portion of the airspace. The schedulers are tested on realistic data sets representing traffic and conditions in the corridor between Chicago and New York area airports. In particular, the optimal scheduling of flights with both origin and destination in that corridor is considered, while reserving sufficient airspace for other air traffic. In a majority of cases, the greedy method provides sufficient (often optimal) results, while under difficult traffic and weather conditions, the optimal scheduler is worth the runtime requirements due to the inability of the greedy version to find satisfactory solutions. Further benefits of an optimal scheduler are demonstrated by incorporating the concept of equity or 'fairness' into the scheduling decision. Design choices in implementing equity amongst the various airlines are discussed and results demonstrating the utility of these choices are provided. Equity is easily implemented for the optimal scheduler but not the greedy, and does not require significant additional run time. Ultimately, it is shown that an equity-aware decision support tool for delay optimization can be developed to run in real-time and can benefit from incorporating more than one approach depending on the complexity of the scenario.
SCOPUS;2007;Visualizing the analysis of dynamically adaptive systems using i* and DSLs;;Self-adaptation is emerging as a crucial enabling capability for many applications, particularly those deployed in dynamically changing environments. One key challenge posed by Dynamically Adaptive Systems (DASs) is the need to handle changes to the requirements und corresponding behavior of a DAS in response to varying environmental conditions. In this paper we propose a visual model-driven approach that uses the i* modeling language to represent goal models for the DAS requirements. Our approach applies a rigorous separation of concerns between the requirements for the DAS to operate in stable conditions and those that enable it to adapt at run-time to enable it to cope with changes in its environment. We further show how requirements derived from the i* modeling can be used by a domain-specific language to achieve requirements model-driven development. We describe our experiences with applying this approach to GridStix, an adaptive flood warning system, deployed on the River Ribble in North Yorkshire, England. © 2007 IEEE.
SCOPUS;2007;Middleware based runtime monitoring and analyzing framework;Constraint definition Language,, Description of constraint,, Middleware,, Monitoring framework,, Runtime monitoring and analysis;Runtime software monitoring and analyzing is not only the approach to improve the quality of software, but also the basis of adaptive software. This paper proposes a pattern-based declarative approach to specify constraints. Based on this approach we implemented a runtime monitoring and analyzing framework on J2EE middleware PKUAS. The most special points of the framework are the flexibility of deploying probes and business logic oriented monitoring. In the end, the paper describes the implementation details and evaluation result of the framework.
SCOPUS;2007;From a goal-oriented methodology to a BDI agent language: The case of Tropos and alan;;This approach aims at addressing crucial issues in complex distributed software such as capability of evolving and adaptivity. Within the area of goal-oriented software requirements engineering, we propose the use of goal models at different abstraction levels in engineering a Multi-Agent System (MAS), namely, not only at design time, but also as a part of the agent knowledge and choice strategy, at run-time. In this paper we briefly overview a mapping between Tropos concepts and Alan (an agent-object programming language) structures. Specifically, we focus on two advantages of our approach: first, Alan allows us to use in an integrated fashion both agent oriented and object oriented design principles. Second, Alan has a well defined semantics expressed by means of rewriting logic. This allows us to verify the properties of an agent both at design time and at run-time (when its knowledge and behavior can have been modified). © Springer-Verlag Berlin Heidelberg 2007.
SCOPUS;2007;Dynamic requirements specification for adaptable and open service systems;;The Dynamic Requirements Adaptation Method (DRAM) is suggested to assist existing RE methodologies in updating requirements specifications at runtime for adaptable and open service-oriented systems. Updates are needed because an adaptable and open system continually changes how and to what extent initial requirements are achieved. © 2007 IEEE.
SCOPUS;2007;Pedestrian navigation systems: A case study of deep personalization;;Requirements Engineering (RE) focuses on obtaining the user goals and environmental constraints of a proposed system. In traditional RE, users are treated as a consumer-class: what holds for one member is assumed to hold for the rest. Personalization comes through providing options that a user can set at run-time to tailor features of the system to his or her personal preferences. In our work, we take up the personalization issue in more detail. In particular, we believe that some systems require a "deep personalization" that includes knowledge of an individual user's skills and limitations. In some cases, these skills and limitations might not be self-aware, i.e., a user cannot accurately self-reflect on his or her skills and weaknesses. In this paper, we will demonstrate the notion of deep personalization in the domain of personal navigation systems. We find this an interesting domain for several reasons: (1) There is a domain theory of navigation skills that draws from both Cartography and Psychology. (2) There are individual differences in navigation skills. (3) An individual user may not be self-aware of his or her skills. (4) If a system is delivered that does not match the skills of the user, it may be less than effective, and at worst, abandoned. ©2007 IEEE.
SCOPUS;2007;Smart distribution of bio-signal processing tasks in M-health;;The past few years have witnessed a rapid advancement of mobile healthcare systems. However, in the mobile computing environment, the resource fluctuations, stringent application requirements and user mobility have severely hindered the performance and reliability of the healthcare service delivery. The current approaches to solve this resource supply and service demand mismatch problem either limit the adaptation to an isolated node or require significant user's involvement. Given the distributed processing paradigm of mhealth system, we propose that a new adaptation approach could be dynamically redistributing processing tasks across distributed nodes. This PhD research addresses two main issues to validate this approach: (1) computation of a suitable assignment of tasks at compile-time or run-time,, and (2) dynamic distribution of tasks across the nodes according to this new assignment at run-time. © Springer-Verlag Berlin Heidelberg 2007.
SCOPUS;2007;Informed evolution;;Ageless Software evolves, to meet new requirements, without reducing its efficiency or understandability. Here we introduce a methodology called Informed Evolution for supporting the construction and evolution of ageless software. This methodology integrates the software architecture (structure and constraints) and the system implementation (behaviour) within system execution. Evolution is effected by evolution patterns which are in turn guided by constraints specified in the software architecture. The availability of the software architecture and implementation at run-time ensures that changes are informed by design and implementation decisions, thus preserving efficiency and understandability. In this paper, we outline Informed Evolution, and describe how evolution patterns may be expressed for systems developed using this methodology. © Springer-Verlag Berlin Heidelberg 2007.
SCOPUS;2007;Implicit phasing for R6RS libraries;Binding phases,, Hygienic macros,, Libraries,, Macro expansion,, Scheme;The forthcoming Revised6 Report on Scheme differs from previous reports in that the language it describes is structured as a set of libraries. It also provides a syntax for defining new portable libraries. The same library may export both procedure and hygienic macro definitions, which allows procedures and syntax to be freely intermixed, hidden, and exported. This paper describes the design and implementation of a portable version of R6RS libraries that expands libraries into a core language compatible with existing R5RS implementations. Our implementation is characterized by its use of inference to determine when the bindings of an imported library are needed, e.g., run time or compile time, relieving programmers of the burden of declaring usage requirements explicitly. Copyright © 2007 ACM.
SCOPUS;2007;Secure service orchestration;;We present a framework for designing and composing services in a secure manner. Services can enforce security policies locally, and can invoke other services in a "call-by-contract" fashion. This mechanism offers a significant set of opportunities, each driving secure ways to compose services. We discuss how to correctly plan service orchestrations in some relevant classes of services and security properties. To this aim, we propose both a core functional calculus for services and a graphical design language. The core calculus is called λreg [10]. It features primitives for selecting and invoking services that respect given behavioural requirements. Critical code can be enclosed in security framings, with a possibly nested, local scope. These framings enforce safety properties on execution histories. A type and effect system over-approximates the actual run-time behaviour of services. Effects include the actions with possible security concerns, as well as information about which services may be selected at run-time. A verification step on these effects allows for detecting the viable plans that drive the selection of those services that match the security requirements on demand. © Springer-Verlag Berlin Heidelberg 2007.
SCOPUS;2007;Path-based error propagation analysis in composition of software services;;In Service-Oriented Architectures (SOA) composed services provide functionalities with certain non-functional properties that depend on the properties of the basic services. Models that represent dependencies among these properties are necessary to analyze non-functional properties of composed services. In this paper we focus on the reliability of a SOA. Most reliability models for software that is assembled from basic elements (e.g. objects, components or services) assume that the elements are independent, namely they do not take into account the dependencies that may exist between basic elements. We relax this assumption here and propose a reliability model for a SOA that embeds the "error propagation" property. We present a path-based model that generates the possible execution paths within a SOA from a set of scenarios. The reliability of the whole system is then obtained as a combination of the reliability of all generated paths. On the basis of our model, we show on an example that the error propagation analysis may be a key factor for a trustworthy prediction of the reliability of a SOA. Such a reliability model for a SOA may support, during the system development, the allocation of testing effort among services and, at run time, the selection of functionally equivalent services offered by different providers. © Springer-Verlag Berlin Heidelberg 2007.
SCOPUS;2007;A fault detection mechanism for fault-tolerant SOA-based applications;Artificial neural network,, Fault detection,, Performance requirement,, Probability change point analysis,, SOA;Fault tolerance is an important capability for SOA-based applications, since it ensures the dynamic composition of services and improves the dependability of SOA-based applications. Fault detection is the first step of fault detection, so this paper focuses on fault detection, and puts forward a fault detection mechanism, which is based on the theories of artificial neural network and probability change point analysis rather than static service description, to detect the services that fail to satisfy performance requirements at runtime. This paper also gives reference model of fault-tolerance control center of Enterprise Services Bus. © 2007 IEEE.
SCOPUS;2007;An automated approach to monitoring and diagnosing requirements;diagnostics,, requirements monitoring;Monitoring the satisfaction of software requirements and diagnosing what went wrong in case of failure is a hard problem that has received little attention in the Software and Requirement Engineering literature. To address this problem, we propose a framework adapted from artificial intelligence theories of action and diagnosis. Specifically, the framework monitors the satisfaction of software requirements and generates log data at a level of granularity that can be tuned adaptively at runtime depending on monitored feedback. When errors are found, the framework diagnoses the denial of the requirements and identifies problematic components. To support diagnostic reasoning, we transform the diagnostic problem into apropositional satisfiability (SAT) problem that can be solved by existing SAT solvers. We preprocess log data into a compact propositional encoding that better scales with problem size. The proposed theoretical framework has been implemented as a diagnosing component that will return sound and complete diagnoses accounting for observed aberrant system behaviors. Our solution is illustrated with two medium-sized publicly available case studies: a Web-based email client and an ATM simulation. Our experimental results demonstrate the feasibility of scaling our approach to medium-size software systems. Copyright 2007 ACM.
SCOPUS;2007;Runtime service discovery and reconfiguration using OWL-S based semantic web service;;In recent business environment where collaboration between organizations is essential, business organization must fast adapt to changing user and market demands. SOC (Service-Oriented Computing) is believed to be a prominent paradigm for efficient businesses application development. Semantic web enables users to locate, select, employ, compose, and monitor Web-based services automatically. OWL-S, ontology of service, helps this location, selection, and composition of services. In this paper we propose a framework, to support dynamic reconfiguration of service-oriented applications. This framework supports semantic discovery, ranking, and dynamic reconfiguration of services using OWL-S based ontology. A prototype is provided to show the validity of this framework. © 2007 IEEE.
SCOPUS;2007;Towards variability design as decision boundary placement;;Complex information systems have numerous design variables that are systematically decided upon during the design process. In high-variability systems, some of these decisions are left open and deferred to later stages. For example, in product line architectures, some decision variables are used to generate families of products with variations in features. In user-adaptive systems, the behavior of the system is determined at runtime, based on user characteristics and preferences. In this paper, we propose to characterize variability in terms of boundaries in design decision graphs which depict the space of alternatives. A design decision about variability, such as what choices should be left to the user and which ones should be fixed at which stage in the design process, is then a question of where to place that decision boundary along some path in the relevant decision graph.
SCOPUS;2007;Dynamic requirements specification for adaptable and open service-oriented systems;;It is not feasible to engineer requirements for adaptable and open service-oriented systems (AOSS) by specifying stakeholders' expectations in detail during system development. Openness and adaptability allow new services to appear at runtime so that ways in, and degrees to which the initial functional and nonfunctional requirements will be satisfied may vary at runtime. To remain relevant after deployment, the initial requirements specification ought to be continually updated to reflect such variation. Depending on the frequency of updates, this paper separates the requirements engineering (RE) of AOSS onto the RE for: individual services (Service RE), service coordination mechanisms (Coordination RE), and quality parameters and constraints guiding service composition (Client RE). To assist existing RE methodologies in dealing with Client RE, the Dynamic Requirements Adaptation Method (DRAM) is proposed. DRAM updates a requirements specification at runtime to reflect change due to adaptability and openness. © Springer-Verlag Berlin Heidelberg 2007.
SCOPUS;2007;Requirements-driven design and configuration management of business processes;;The success of a business process (BP) depends on whether it meets its business goal as well as non-functional requirements associated with it. BP specifications frequently need to accommodate changing business priorities, varying client preferences, etc. However, since business process goals and preferences are rarely captured explicitly in the dominant BP modeling approaches, adapting business processes proves difficult. We propose a systematic requirements-driven approach for BP design and configuration management that uses requirements goal models to capture alternative process configurations and provides the ability to tailor deployed processes to changing business priorities or customer preferences (i.e., non-functional constraints) by configuring their corresponding goal models at the goal level. A set of design time and runtime tools for configuring business processes implemented using WS-BPEL is provided, allowing to easily change the behaviour of deployed BP instances at a high level, based on business priorities and stakeholder preferences. © Springer-Verlag Berlin Heidelberg 2007.
SCOPUS;2007;The interface of VISTO, a new vector image search tool;CBIR,, Query formulation,, SVG format,, User interface,, Vector images;We discuss the interface of VISTO, a Content Based Image Retrieval system for vector images in SVG (Scalable Vector Graphics) format. The system includes different engines and one common graphical interface. In fact, to support the different requirements of different application domains, the system offers a variety of engines. Most notably, due to its modular architecture, the system allows users to add engines at runtime,, the interface provides support for newly added engines, including parameter tuning. The interface is designed for two classes of users: application domain users and researchers in the field of multimedia. Application domain users can use both query-by-sketch and query-by-example to search collections,, researcher users can test, tune, and compare engines, and they can design datasets to be used in batch mode. VISTO is an open source project developed in Java,, it uses advanced features of the language, such as the Core Reflection API, to dynamically adapt the interface to the available engines. The system is being validated in different application domains, including the production of 2D animation, the Sign Writing Language, and the BLISS Language. © Springer-Verlag Berlin Heidelberg 2007.
SCOPUS;2007;Aspectizing a web server for adaptation;;Web servers are exposed to extremely changing runtime requirements. Going offline to adjust policies and configuration parameters in order to cope with such requirements is not an available choice for long running web servers. Many of the policies that need to be adapted are crosscutting in nature. Aspect-Oriented Programming (AOP) provides mechanisms to encapsulate the crosscutting policies as aspects. This paper describes the integration of a statically configurable web server with our dynamic aspect weaving infrastructure. This integration transformed the server to a dynamically adaptable one that could adjust its policies and configuration parameters at runtime according to the changing requirements. This paper further provides a comprehensive analysis of the memory and runtime costs associated with this transformation, and explains how our dynamic aspect weaving infrastructure via its tailored support facilitates to minimise these costs. © 2007 IEEE.
SCOPUS;2007;SLA-based advance reservations with flexible and adaptive time QoS parameters;;Utility computing enables the use of computational resources and services by consumers with service obligations and expectations defined in Service Level Agreements (SLAs). Parallel applications and workflows can be executed across multiple sites to benefit from access to a wide range of resources and to respond to dynamic runtime requirements. A utility computing provider has the difficult role of ensuring that all current SLAs are provisioned, while concurrently forming new SLAs and providing multiple services to numerous consumers. Scheduling to satisfy SLAs can result in a low return from a provider's resources due to trading off Quality of Service (QoS) guarantees against utilisation. One technique is to employ advance reservations so that an SLA aware scheduler can properly manage and schedule its resources. To improve system utilisation we exploit the principle that some consumers will be more flexible than others in relation to the starting or completion time, and that we can juggle the execution schedule right up until each execution starts. In this paper we present a QoS scheduler that uses SLAs to efficiently schedule advance reservations for computation services based on their flexibility. In our SLA model users can reduce or increase the flexibility of their QoS requirements over time according to their needs and resource provider policies. We introduce our scheduling algorithms, and show experimentally that it is possible to use flexible advance reservations to meet specified QoS while improving resource utilisation. © Springer-Verlag Berlin Heidelberg 2007.
SCOPUS;2007;Physical configuration on-line visualization of Xilinx Virtex-II FPGAs;Dynamic reconfiguration,, FPGA,, On-line visualization;Xilinx Virtex-II / Virtex-II Pro FPGAs provide the possibility of partial and dynamic run-time reconfiguration. This feature can be used in adaptive systems providing the possibility to adapt to application requirements by exchanging parts of the hardware while other parts stay operative. This computing in time and space and many other fine grained adjustments within the architectures, opens new dimensions for electronic system design as well as for novel scheduling mechanisms based on well established graph-based algorithms in comparison to pure microprocessor based electronic systems. However, at the moment it is not possible to visualize the physical configuration of the chip and the manifold possibilities of manipulations on the device. This feature allows to demonstrate the system's behavior and helps to debug final integrated reconfigurable systems. This paper presents an approach to the system integration of an autonomously working on-line visualization stand alone IP-Core integrated on Xilinx Virtex-II and Virtex-II Pro FPGAs. © 2007 IEEE.
SCOPUS;2007;Combining model processing and middleware configuration for building distributed high-integrity systems;;Requirements of High Integrity systems now encompass distribution mechanisms along with strong junctional and non-functional features (run-time support for hardware, dependability, safety, analyzability). In this paper, we show how model processing help addressing such needs. We present a generic distribution model suitable for High Integrity systems, and demonstrate how a high-level modeling deployment view allows one to greatly reduce the model complexity. Finally, we conclude by assessing a case study. © 2007 IEEE.
SCOPUS;2007;An approach to automated agent deployment in service-based systems;;In service-based systems, services from various providers can be integrated following specific workflows to achieve users' goals. These workflows are often executed and coordinated by software agents, which invoke appropriate services based on situation changes. These agents need to be deployed on underlying platforms with respect to various requirements, such as access permission of agents, real-time requirements of workflows, and reliability of the overall system. Deploying these agents manually is often error-prone and time-consuming. Furthermore, agents need to migrate from hosts to hosts at runtime to satisfy deployment requirements. Hence, an automated agent deployment mechanism is needed. In this paper, an approach to automated agent deployment in service-base systems is presented. In this approach, the deployment requirements are represented as deployment policies, and techniques are developed for generating agent deployment plans by solving the constraints specified in deployment policies, and for generating executable code for runtime agent deployment and migration. © 2007 IEEE.
SCOPUS;2007;Runtime verification and monitoring of embedded systems;;Ensuring the correctness of software applications is a difficult task. The area of runtime verification, which combines the approaches of formal verification and testing, offers a practical but limited solution that can help in finding many errors in software. Runtime verification relies upon tools for monitoring software execution. There are particular difficulties with regard to monitoring embedded systems. The concerns for arranging non-intrusive monitoring of embedded systems in a way that is suitable for use in runtime verification methods are considered here. A number of existing runtime verification tools are referenced, highlighting their requirement for monitoring solutions. Established and emerging approaches for the monitoring of software execution using execution monitors are reviewed, with an emphasis on the approaches that are best suited for use with embedded systems. A suggested solution for non-intrusive monitoring of embedded systems is presented. The conclusions summarise the possibilities for arranging non-intrusive monitoring of embedded systems, and the potential for runtime verification to utilise such monitoring approaches. © The Institution of Engineering and Technology 2007.
SCOPUS;2007;Development and evaluation of a vehicle-infrastructure integration simulation architecture;Architecture,, Infrastructure,, Integrated systems,, Intelligent transportation systems,, Simulation,, Vehicles;Advances in information technology are making the close integration of vehicles and the infrastructure in the surface-transportation system feasible. In particular, one approach to this integration receiving significant attention and effort in the United States is referred to as vehicle-infrastructure integration (VII). While few would argue that VII possesses the potential to support applications that improve traffic safety and mobility, the level of benefits, as well as strengths/weaknesses of various design alternatives, cannot be effectively assessed without the availability of an appropriate simulation environment. The purpose of this research is to develop a simulator that can fully model the functional aspects of VII deployment. This paper identifies key requirements for development of the simulation architecture, and proposes an architecture for a VII simulation system after identifying several alternatives. The proposed architecture components include a commercial off-the-shelf (COTS) traffic simulator, database, VII interface module, and multiple applications. Finally, the paper presents a prototype application of the architecture to explore the computing efficacy of the architecture. The results indicate that simulation run times are linear with respect to the number of COTS traffic simulator interruptions required to extract vehicular data, and that database access can increase computational requirements at a ratio of 14:1-reasonable for analytical purposes. © 2007 ASCE.
SCOPUS;2007;Towards security monitoring patterns;Event calculus,, Runtime monitoring,, Security patterns;Runtime monitoring is performed during system execution to detect whether the system's behaviour deviates from that described by requirements. To support this activity we have developed a monitoring framework that expresses the requirements to be monitored in event calculus - a formal temporal first order language. Following an investigation of how this framework could be used to monitor security requirements, in this paper we propose patterns for expressing three basic types of such requirements, namely confidentiality, integrity and availability. These patterns aim to ease the task of specifying confidentiality, integrity and availability requirements in monitorable forms by non-expert users. The paper illustrates the use of these patterns using examples of an industrial case study. Copyright 2007 ACM.
SCOPUS;2007;Pervasive service composition in the home network;;The home environment becomes ready to host distributed devices dynamically adapting to service availability and reacting to user location and user activity. Sensors, high definition rendering systems, home gateways, wired and wireless controllable equipments are now available. Many protocols enable connectivity and interaction between devices. However, challenges remain: protocol heterogeneity, interface fragmentation and device composition static aspect make self-organization and dynamic reconfiguration hardly achievable. This paper describes attractive scenarios at home which lead to the definition of the pervasive service composition requirements. A software architecture facing the mentioned challenges is proposed over OSGi. It first enables developers to implement distributed plug-n-play applications like a local one. It also delivers a serviceoriented middleware allowing spontaneous distributed service composition to occur at runtime. © 2007 IEEE.
SCOPUS;2007;LENO: LEast rotation near-optimal cluster head rotation strategy in wireless sensor networks;;Cluster-based self-organization scheme is attracting tremendous research interest in the studies of the wireless sensor networks (WSN), because it meets the critical runtime requirement of the WSN based applications: working in self-organized and energy efficient way. Whereas, an important problem in the cluster scheme remains seldom studied, that the cluster heads depletes energy very fast and the rotation strategy of the cluster head is needed to prolong the system's lifetime. In this paper, the cluster head rotation problem is studied with the dynamic programming method. An energy first cluster head rotation strategy is proposed and is proved to be the optimal in the means of the cluster lifetime. Further, the upper bound and the lower bound of the cluster lifetime are derived based on the Law of Conservation of Energy. We show that the optimal strategy is not unique, which can be accomplished in different ways. Based on the analysis, a practical, LEast-rotation, Near-Optimal cluster head rotation algorithm (LENO) is proposed to practice the inner cluster rotation. The validity of LENO is verified with the node level simulation tool PowerTOSSIM. Near optimal cluster lifetime is obtained as desired, which is much better than the performances of Leach and EDAC etc. © 2007 IEEE.
SCOPUS;2007;A job pause service under LAM/MPI+BLCR for transparent fault tolerance;;Checkpoint/restart (C/R) has become a requirement for long-running jobs in large-scale clusters due to a meantime-to-failure (MTTF) in the order of hours. After a failure, C/R mechanisms generally require a complete restart of an MPI job from the last checkpoint. A complete restart, however, is unnecessary since all but one node are typically still alive. Furthermore, a restart may result in lengthy job requeuing even though the original job had not exceeded its time quantum. In this paper, we overcome these shortcomings. Instead of job restart, we have developed a transparent mechanism for job pause within LAM/MPI+BLCR. This mechanism allows live nodes to remain active and roll back to the last checkpoint while failed nodes are dynamically replaced by spares before resuming from the last checkpoint. Our methodology includes LAM/MPI enhancements in support of scalable group communication with fluctuating number of nodes, reuse of network connections, transparent coordinated checkpoint scheduling and a BLCR enhancement for job pause. Experiments in a cluster with the NAS Parallel Benchmark suite show that our overhead for job pause is comparable to that of a complete job restart. A minimal overhead of 5.6% is only incurred in case migration takes place while the regular checkpoint overhead remains unchanged. Yet, our approach alleviates the need to reboot the LAM run-time environment, which accounts for considerable overhead resulting in net savings of our scheme in the experiments. Our solution further provides full transparency and automation with the additional benefit of reusing existing resources. Executing continues after failures within the scheduled job, i.e., the application staging overhead is not incurred again in contrast to a restart. Our scheme offers additional potential for savings through incremental checkpointing and proactive diskless live migration, which we are currently working on. © 2007 IEEE.
SCOPUS;2007;High performance database searching with HMMer on FPGAs;;Profile Hidden Markov Models (profile HMMs) are used as a popular bioinformatics tool for sensitive database searching, e.g. a set of not annotated protein sequences is compared to a database of profile HMMs to detect functional similarities. HMMer is a commonly used package for profile HMM-based methods. However, searching large databases with HMMer suffers from long runtimes on traditional computer architectures. These runtime requirements are likely to become even more severe due to the rapid growth in size of both sequence and model databases. In this paper, we present a new reconfigurable architecture to accelerate HMMer database searching. It is described how this leads to significant runtime savings on off-the-shelf field-programmable gate arrays (FPGAs). © 2007 IEEE.
SCOPUS;2007;A process splitting transformation for Kahn process networks;;In this paper we present a process splitting transformation for Kahn process networks. Running applications written in this parallel program specification on a multiprocessor architecture does not guarantee that the runtime requirements are met. Therefore, it may be necessary to further analyze and optimize Kahn process networks. In this paper, we will present a four-step transformation that results in a functionally equivalent process network, but with a changed and optimized network structure. The class of networks that can be handled is not restricted to static networks. The novelty of this approach is that it can also handle processes with dynamic program statements. We will illustrate the transformation prototyped in GCC for a JPEG decoder, showing a 21% performance improvements. © 2007 EDAA.
SCOPUS;2007;A framework for supporting dynamic systems co-evolution;Adaptive software,, Co-evolution,, Incremental design,, Reflection,, Run-time evolution,, Software evolution;Businesses and their supporting software evolve to accommodate the constant revision and re-negotiation of commercial goals, and to intercept the potential of new technology. We have adopted the term co-evolution to describe the concept of the business and the software evolving sympathetically, but at potentially different rates. More generally, we extend co-evolution to accommodate wide-informatics systems, that are assembled from parts that co-evolve with each other and their environment, and whose behavior is potentially emergent. Typically these are long-lived systems in which dynamic co-evolution, whereby a system evolves as part of its own execution in reaction to both expected and unexpected events, is the only feasible option for change. Examples of such systems include continuously running business process models, sensor nets, grid applications, self-adapting/tuning systems, peer-to-peer routing systems, control systems, autonomic systems, and pervasive computing applications. The contribution of this paper comprises: a study of the intrinsic nature of dynamic co-evolving systems,, the derivation of a set of intrinsic requirements,, a description of a model and a set of technologies, new and extant, to meet these intrinsic requirements,, and illustrations of how these technologies may be implemented within an architecture description language (ArchWare ADL) and a conventional programming language (Java). The model and technologies address three topics: structuring for dynamic co-evolution, incremental design, and adapting dynamic co-evolving systems. The combination yields a framework that can describe the system's specification, the executing software and the reflective evolutionary mechanisms within a single computational domain in which all three may evolve in tandem. © 2007 Springer Science+Business Media, LLC.
SCOPUS;2007;The role of roles in supporting reconfigurability and fault localizations for open distributed and embedded systems;Actors,, Coordination,, Coordinators,, Open distributed embedded systems,, Roles;One of the main characteristics of open distributed embedded systems is that the involved entities are often very dynamic - -different individual entities may join or leave the systems frequently. Therefore, systems built of these dynamic entities must be runtime reconfigurable. In addition, large classes of open embedded systems often have high availability and dependability requirements. However, the openness makes these requirements more difficult to achieve and the system more vulnerable to attacks. This article presents a coordination model, the Actor, Role and Coordinator (ARC) model, that aims to support reconfigurability and fault localization for open distributed embedded software systems. In particular, the actor model is used to model concurrent embedded entities, while the system's reconfigurability and dependability requirements are encapsulated within coordination objects: roles and coordinators, and are achieved through coordination among the actors. Roles, as a key thrust in the ARC model not only represent an abstraction for a set of behaviors shared by a group of actors so that reconfiguration within the roles becomes transparent to entities outside the roles, but also assume coordination responsibilities among the member actors. The article also argues from both analytical and empirical perspectives that with the support of the role, faults can be localized within actors, and actor level reconfiguration becomes transparent to the system. © 2007 ACM.
SCOPUS;2007;Preemption threshold scheduling: Stack optimality, enhancements and analysis;;Using preemption threshold scheduling (PTS) in a multi-threaded real-time embedded system reduces system preemptions and hence reduces run-time overhead while still ensuring real-time constraints are met. However, PTS offers other valuable benefits. In this paper we investigate the use of PTS for hard real-time system with limited RAM. Our primary contribution is to prove the optimality of PTS among all preemptionlimiting methods for minimizing a system's total stack memory requirements. We then discuss characteristics of PTS and show how to reduce average worst-case response times. We also introduce a unified framework for using PTS with existing fixed-priority (e.g. rate-or deadline-monotonic), or dynamic-priority scheduling algorithms (e.g. earliest-deadline first). We evaluate the performance of PTS and our improvements using synthetic workloads and a real-time workload. We show PTS is extremely effective at reducing stack memory requirements. Our enhancements to PTS improve worst-case response-times as well. © 2007 IEEE.
SCOPUS;2007;A factory to design and buid tailorable and verifiable middleware;;Heterogeneous non-functional requirements of Distributed Real-Time Embedded (DRE) system put a limit on middleware engineering: the middleware must reflect application requirements, with limited runtime impact. Thus, building an application-tailored middleware is both a requirement and a challenge. In this paper, we provide an overview of our work on the construction of middleware. We focus on two complementary projects: the definition of middleware that provides strong support for both tailorability and verification of its internals,, the definition of a methodology that enables the automatizing of key steps of middleware construction. We illustrate how our current work on PolyORB, Ocarina and the use of Petri Nets allows designer to build the middleware that precisely matches its application requirements and comes with precise proof of its properties. © Springer-Verlag Berlin Heidelberg 2007.
SCOPUS;2007;Towards agent-oriented model-driven architecture;Adaptive agent model,, Agent-oriented model-driven architecture,, Business knowledge model,, Model-driven architecture,, Multi-agent system,, Software adaptivity;Model-Driven Architecture (MDA) supports the transformation from reusable models to executable software. Business representations, however, cannot be fully and explicitly represented in such models for direct transformation into running systems. Thus, once business needs change, the language abstractions used by MDA (e.g. object constraint language/action semantics), being low level, have to be edited directly. We therefore describe an agent-oriented MDA (AMDA) that uses a set of business models under continuous maintenance by business people, reflecting the current business needs and being associated with adaptive agents that interpret the captured knowledge to behave dynamically. Three contributions of the AMDA approach are identified: (1) to Agent-oriented Software Engineering, a method of building adaptive Multi-Agent Systems,, (2) to MDA, a means of abstracting high-level business-oriented models to align executable systems with their requirements at runtime,, (3) to distributed systems, the interoperability of disparate components and services via the agent abstraction. © 2007 Operational Research Society Ltd. All rights reserved.
SCOPUS;2007;Enhancing residential gateways: OSGi service composition;BPEL,, OSGi,, Residential gateways,, Semantic reasoning,, Service Composition;We propose a schema to support the composition of OSGi services as result of orchestrating atomic services at run-time. With this proposal the OSGi potentiality increases because of the huge amount of new services that can be offered as a result of appropriate combinations. To specify the service composition we propose a totally transparent BPEL-style solution which does not break the OSGi standard. However, the syntactic matchmaking provided by the OSGi standard does not enable a flexible enough automatic service composition. Thus, we propose to define a Semantic OSGi platform, which combined with the BPEL-style solution fits all the OSGi service composition requirements. © 2007 IEEE.
SCOPUS;2007;Research on MDA based simulation model development and integration methodology;MDA,, Simulation Composition,, Simulation meta model,, Simulation model design time component,, Simulation model runtime component;An MDA-based simulation model development and integration methodology were discussed. The benefit using MDA in simulation model reusability and composibility was discussed, e.g. the consistency through simulation scenario, experiment, analysis and model integration can be achieved', reusability and composibility of simulation model can be improved,, an engineering development approach for simulation requirement, model design and model implementation can be supported by MDA,, the views for architecture framework for SOS can be more easily mapped to simulation model,, flexible and high configurable simulation can be more easily designed and implemented by MDA. So a simulation model integration framework and a simulation model development and integration process based on MDA were illustrated. The framework composed of simulation elements corresponding to PIM and PSM (simulation model design time component and simulation model runtime component), and simulation model development and integration process consists of simulation model design, simulation model development, simulation model composition, and simulation model execution.
SCOPUS;2007;Using Microcomponents and Design Patterns to Build Evolutionary Transaction Services;CBSE,, design pattern,, Evolution,, Fractal component model,, microcomponent,, transaction service;The evolution of existing transaction services is limited because they are tightly coupled to a given transaction standard, implement a dedicated commit protocol, and support a fixed kind of applicative participants. The next challenge for transaction services will be to deal with evolution concerns. This evolution should allow developers to tune the transaction service depending on the transaction standard or the application requirements either at design time or at runtime. The contribution of this paper is to introduce the common approach that we have defined to build various evolutionary transaction services. This common approach is based on the use of microcomponents and design patterns, whose flexibility properties allow transaction services to be adapted to various execution contexts. This approach is applied in our GoTM framework that supports the construction of transaction services implementing several transaction standards and commit protocols. We argue that using fine-grained components and design patterns to build transaction services is an efficient solution to the evolution problem and our past experiences confirm that this approach does not impact the transaction service efficiency. © 2006 Elsevier B.V. All rights reserved.
SCOPUS;2006;A reconfigurable ethernet switch for self-optimizing communication systems;;Self-optimization is a promising approach to cope with the increasing complexity of today's automation networks. The high complexity is mainly caused by a rising amount of network nodes and increasing real-time requirements. Dynamic hardware reconfiguration is a key technology for self-optimizing systems, enabling, e.g., Real-Time Communication Systems (RCOS) that adapt to varying requirements at runtime. Concerning dynamic reconfiguration of an RCOS, an important requirement is to maintain connections and to support time-constrained communication during reconfiguration. We have developed a dynamically reconfigurable Ethernet switch, which is the main building block of a prototypic implementation of an RCOS network node. Three methods for reconfiguring the Ethernet switch without packet loss are presented. A prototypical implementation of one method is described and analyzed in respect to performance and resource efficiency. © 2006 International Federation for Information Processing.
SCOPUS;2006;FPGA implementation of dynamic run-time behavior reconfiguration in robots;;Using a single robot for multiple operations has been a significant problem for researchers in Robotics since available space, cost, and power consumption are constraints to the increasing number of behaviors on a robot. An efficient technique to implement multiple behaviors in robots using FPGA based partial reconfigurable hardware is presented in this paper. Robots built using reconfigurable FPGAs can have their functionalities modified at run-time without completely taking the robot off-line. Resource efficiency increases because only the modules corresponding to the current behavior are implemented in the FPGA while inactive modules are stored in an external memory. The approach is validated through a case study where teams of robots are configured to meet application specific requirements and the run-time behaviors of these robots are modified dynamically using Xilinx Virtex-II Pro FPGAs. © 2006 IEEE.
SCOPUS;2006;Dynamic weaving of security aspects in service composition;;Web Service composition is to construct complex service through combining available services components as request. Service composition often has to handle the security risk that can not be predicted when the service components are developed. This paper presents an Aspect-Oriented (AO) approach to enhance the security of service composition that can not only realize flexible security policies but also accomplish it with very little run-time overhead. The security control is separated from other functional requirements and encapsuled into service extension aspect. And the composition can be extended by weaving the extension at runtime. It also gives the service composer a chance to unify security policy in composed service by specifying appropriate security extension himself. A Web Service extension Environment (WSXE) is devised to demonstrate the approach. Finally, an application of performing user-defined access control dynamically at runtime is given to exemplify the dynamic extension to service composition. © 2006 IEEE.
SCOPUS;2006;Extended web services framework to meet non-functional requirements;Framework,, Non-functional requirements,, Web services;Non-functional requirements/characteristics are important for providing effectively every kind of services including web services. A realistic web service must meet both functional and non-functional requirements of its consumers. Therefore, it is important that a web services framework is augmented so that non-functional characteristics of a web service can be determined at run-time and consumers are bound to a service that best meet their functional as well as non-functional requirements. In this paper, we propose an extension of the existing web services framework that enables a collection of functional and non-functional service characteristics at run-time, and usage of the collected data in discovery, binding, and execution of web services. Descriptions of new and enhanced components of the proposed framework are also presented. Typical publishing and usage scenarios in the proposed framework are also described.
SCOPUS;2006;Proceedings - SBAC-PAD 2006 18TH International Symposium on Computer Architecture and High Performance Computing;;The proceedings contain 22 papers. The topics discussed include: towards production code effective portability among vector machines and microprocessors-based architectures,, data segmentation management infrastructure in a database grid,, detecting malicious manipulation in grid environments,, policy-based resource allocation in hierarchical virtual organizations for global grids,, a speculative trace reuse architecture with reduced hardware requirements,, controlling the power and area of neural branch predictors for practical implementation in high-performance processors,, a run-time system for efficient execution of scientific workflows on distributed environments,, dual-thread speculation: two threads in the machine are worth eight in the bush,, characterizing the performance of data management systems on hyper-threaded architectures,, and ultra-fast CPU performance prediction: extending the monte carlo approach.
SCOPUS;2006;Policy-driven model for autonomic management of Web services using mas;Multi-agent system,, Ontology,, Policy,, Web services;Web service is an important technology in open distributed system. But because of the dynamical and heterogeneous nature of web services, it is difficult to be managed directly. On the other hand, multi-agent system provides a model for automatic discovery, negotiation and cooperation. It can be designed as a container of web services for autonomic management. This paper provides a policy-driven model base on multi-agent system that encapsulates the web service as an agent. It can supervise the dynamical agents and web services that may join in or leave out at run time according to high-level policies or business requirements. © 2006 IEEE.
SCOPUS;2006;The MAIS approach to web service design;;This chapter presents a first attempt to realize a methodological framework supporting the most relevant phases of the design of a value-added service. A value-added service is defined as a functionality of an adaptive and multichannel information system obtained by composing services offered by different providers. The framework has been developed as part of the multichannel adaptive information systems (MAIS) project. The MAIS framework focuses on the following phases of service life cycle: requirements analysis, design, deployment, and run-time use and negotiation. In the first phase, the designer elicits, validates, and negotiates service requirements according to social and business goals. The design phase is in charge of modeling services with an enhanced version of UML, augmented with new features developed within the MAIS project. The deployment phase considers the network infrastructure and, in particular, provides an approach to implement and coordinate the execution of services from different providers. In the run-time use and negotiation phase, the MAIS methodology provides support to the optimal selection and quality renegotiation of services and to the dynamic evaluation of management costs. The chapter describes the MAIS methodological tools available for different phases of service life cycle and discusses the main guidelines driving the implementation of a service management architecture called reflective architecture that complies with the MAIS methodological approach. © 2006, Idea Group Inc.
SCOPUS;2006;Using product line techniques to build adaptive systems;;Adaptive systems are able to adapt their properties and resource requirements at runtime in response to dynamically varying user needs and resource constraints. With the emergence of mobile and service oriented computing, such variation is becoming increasingly common, and the need for adaptivity is increasing accordingly. Software product line engineering has proved itself as an efficient way to deal with varying user needs and resource constraints. In this paper we present an approach to building adaptive systems based on product line oriented techniques such as variability modeling and component based architectures. By representing the product line architecture at runtime, we are able to delegate much of the complexity of adaptation to a reusable adaptation platform. To validate our approach we have built a prototype adaptation platform and developed a few pilot applications exploiting the platform to achieve adaptivity. © 2006 IEEE.
SCOPUS;2006;Elevating interaction requirements for web service composition;;Web services are increasingly utilized to create integrated applications from existing components However, incompatible interaction expectations between Web service interfaces and/or non-Web service interfaces participating in the overall application can inhibit the integration Frequently, these conflicts are resolved on a case by case basis Advantages can be gained by understanding integration conflict resolution as formal interaction requirements This approach enables evaluation of recurring integration conflicts during requirements analysis (rather than as a testing step), thus providing a more complete resolution that is abstracted from specific runtime interaction instances.
SCOPUS;2006;Requirements-driven design of autonomic application software;;Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying the objectives of the system, analyzing alternative ways of how these objectives can be met, and designing a system that supports all or some of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and demonstrates this on an example. © 2006 Alexei Lapouchnian, Yijun Yu, Sotirios Liaskos, John Mylopoulos.
SCOPUS;2006;Efficient trace monitoring;;A wealth of recent research involves generating program monitors from declarative specifications. Doing this efficiently has proved challenging, and available implementations often produce infeasibly slow monitors. We demonstrate how to dramatically improve performance - typically reducing overheads to within an order of magnitude of the program's normal runtime.
SCOPUS;2006;Keeping track of crosscutting requirements in UML models via context-based constraints;AOP,, AOSD,, Constraints,, Modelling,, Requirements engineering,, UML;One crosscutting requirement (also called aspect) affects several parts of a software system. Handling aspects is well understood at source-code level or at runtime. However, only a few aspect-oriented approaches handle other software artefact types, like UML models, configuration files, or database schema definitions. Instead of re-writing the same aspect newly for each artefact type, this paper suggests to write down an aspect once independent of artefact types.But, wait a minute: Which places does such an aspect affect? Where do we weave in an aspect if its pointcut doesn't refer to artefact details? This paper suggests expressing aspects via Context-Based Constraints (CoCons). They select their constrained system elements according to the element's context. For instance, CoCons affect all system elements used in a certain department, workflow, or location. CoCons are easy to grasp for users and customers because they express business requirements without referring to technical details. This paper focuses on how to express and monitor crosscutting requirements in UML models via CoCons. Moreover, it reveals that CoCons are a new notion of constrains by comparing CoCons to the Object Constraint Language OCL.
SCOPUS;2006;Preprocessing the MAP problem;;The MAP problem for Bayesian networks is the problem of finding for a set of variables an instantiation of highest posterior probability given the available evidence. The problem is known to be computationally infeasible in general. In this paper, we present a method for preprocessing the MAP problem with the aim of reducing the runtime requirements for its solution. Our method exploits the concepts of Markov and MAP blanket for deriving partial information about a solution to the problem. We investigate the practicability of our preprocessing method in combination with an exact algorithm for solving the MAP problem for some real Bayesian networks.
SCOPUS;2006;Selective code/data migration for reducing communication energy in embedded MpSoC architectures;Energy,, Migration,, MPSoC;Proliferation of embedded on-chip multiprocessor architectures (MpSoC) motivates researchers from both academia and industry to consider optimization techniques for such architectures. While many proposals on code/data partitioning on parallel architectures try to minimize interprocessor communication requirements at runtime, many applications still have significant runtime communication requirements. This paper proposes a novel task/data migration scheme that decides whether to migrate task or data in order to satisfy a given communication requirement. The choice between the two options is made at runtime based on the statistics collected off-line through profiling. An important characteristic of the approach proposed in this paper is that it takes into account future uses of tasks and data and tries to make a decision that is globally optimal (i.e., when considering multiple communications not just the current one). Our results collected so far are very encouraging and indicate that the proposed selective migration strategy is very successful in practice, reducing communication energy and total energy by 38.6% (resp. 18.9%) and 13.8% (resp. 6.8%) on an average, as compared to task (resp. data) migration only, across ten embedded applications tested. Copyright 2006 ACM.
SCOPUS;2006;Generic architecture and mechanisms for protocol reconfiguration;Binding,, Protocol component,, Protocol stack,, Reconfiguration;The next generation of wireless mobile communications termed beyond 3G (or 4G) will be based on a heterogeneous infrastructure that comprises different wireless networks in a complementary manner. Beyond 3G will introduce reconfiguration capabilities to flexibly and dynamically (i.e. during operation) adapt the wireless protocol stacks to better meet the ever-changing service requirements. For the dynamic reconfiguration of protocol stacks during runtime operation to become a practical capability of mobile communication systems it is necessary to establish a software architecture that functionally supports reconfiguration. In the present paper a generic architecture and respective mechanisms to achieve protocol stack and component based protocol layer reconfiguration are proposed. © Springer Science + Business Media LLC 2006.
SCOPUS;2006;Proceedings 2006 Australian Software Engineering Conference;;The proceedings contain 44 papers. The topics discussed include: an agile approach to support incremental development of requirements specifications,, requirements capture and specifications for enterprise applications: a UML based attempt,, compatibility test for coordination aspects of software components,, a framework for checking behavioral compatibility for component composition,, a runtime monitoring and validation framework for web service interactions,, checking conformance between business processes and web service contract in service oriented applications,, a coordinated architecture for the agent-based service level agreement negotiation of web service composition,, optimizing web services performance with table driven XML,, design of agent-oriented pattern templates,, a framework for evaluating alternative architectures and its application to financial business processes,, and a service-oriented architecture for software process technology,, the transparent adaptation approach to the development of awareness mechanisms for groupware.
SCOPUS;2006;Comparative evaluation of dependability characteristics for peer-to-peer architectural styles by simulation;Architectural style,, Dependability,, Peer-to-peer system;An important concern for the successful deployment of a dependable system is its quality of service (QoS), which is significantly influenced by its architectural style. We propose the comparative evaluation of architectural styles by simulation. Our approach integrates architectural styles and concrete architectures to enable early design-space exploration in order to predict the QoS of peer-to-peer systems. We illustrate the approach via two case studies where availability of resources and performance of peer-to-peer search methods are evaluated. Based on our experience with these simulation environments, we sketch tool support for simulating architectural changes at runtime. © 2006 Elsevier Inc. All rights reserved.
SCOPUS;2006;Non-intrusive monitoring of service-based systems;Runtime requirements monitoring,, Service-based systems;This paper presents a framework for monitoring the compliance of systems composed of Web-services with requirements set for them at runtime. This framework assumes systems composed of Web-services which are co-coordinated by a service composition process expressed in BPEL and uses event calculus to specify the requirements to be monitored. These requirements may include behavioral properties of a system which are automatically extracted from the specification of its composition process in BPEL and/or assumptions that system providers can specify in terms of events extracted from this specification. © World Scientific Publishing Company.
SCOPUS;2006;Research on architecture of support environment for distributed simulation;Architecture,, Runtime infrastructure,, Simulation management,, Simulation repository,, Support environment;Support environment for distributed simulation that integrates various simulation/modeling tools is the main trend in simulation domain. It can be applied in full life cycle of simulation system from requirement analysis to maintenance and training. The architecture is one of the most important factors that influences the performance of support environment. A hierarchical architecture was devised which includes the layer of simulation repository, the layer of runtime infrastructure and the layer of simulation management. The components of each layer were analyzed, and the relationship between two different layers was discussed.
SCOPUS;2006;Impact of virtual execution environments on processor energy consumption and hardware adaptation;Energy Efficiency,, Hardware Adaptation,, Power Dissipation;During recent years, microprocessor energy consumption has been surging and efforts to reduce power and energy have received a lot of attention. At the same time, virtual execution environments (VEEs), such as Java virtual machines, have grown in popularity. Hence, it is important to evaluate the impact of virtual execution environments on microprocessor energy consumption. This paper characterizes the energy and power impact of two important components of VEEs, Just-in-time (JIT) optimization and garbage collection. We find that by reducing instruction counts, JIT optimization significantly reduces energy consumption, while garbage collection incurs runtime overhead that consumes more energy. Importantly, both JIT optimization and garbage collection decrease the average power dissipated by a program. Detailed analysis reveals that both JIT optimizer and JIT optimized code dissipate less power than un-optimized code. On the other hand, being memory bound and with low ILP, the garbage collector dissipates less power than the application code, but rarely affects the average power of the latter. Adaptive microarchitectures are another recent trend for energy reduction where microarchitectural resources can be dynamically tuned to match program runtime requirements. This research reveals that both JIT optimization and garbage collection alter a program's behavior and runtime requirements, which considerably affects the adaptation of configurable hardware units, and influences the overall energy consumption. This work also demonstrates that the adaptation preferences of the two VEE services differ substantially from those of the application code. Both VEE services prefer a simple core for high energy reduction. On the other hand, the JIT optimizer usually requires larger data caches, while the garbage collector rarely benefits from large data caches. The insights gained in this paper point to novel techniques that can further reduce microprocessor energy consumption. Copyright © 2006 ACM.
SCOPUS;2006;A genuine design manufacturability check for designers;;The design of integrated circuits (ICs) has been made possible by a simple contract between design and manufacturing: Manufacturing teams encapsulated their process capabilities into a set of design rules such as minimum width and spacing or overlap for each layer, and designers complied with these design rules to get a manufacturable IC. However, since the advent of 90nm technology, designers have to play by the new rules of sub-90nm technologies. The simple design rules have evolved into extremely complex, context-dependent rules. Minimum design rules have been augmented with many levels of yield-driven recommended guidelines. One of the main drivers behind these complex rules is the increase in optical proximity effects that are directly impacting systematic and parametric yields for sub-90nm designs. The design's sensitivity to optical proximity effects increases as features get smaller, however design engineers do not have visibility into the manufacturability of these features. A genuine design for manufacturing (DFM) solution for designers should provide a fast, easy-to-use and cost-effective solution that accurately predicts the designs sensitivity to shape variations throughout the design process. It should identify and reduce design sensitivity by predicting and reducing shape variations. The interface between manufacturing and design must provide designers with the right information to allow them to maximize the manufacturability of their design while shielding them from the effects of resolution enhancement technologies (RET) and manufacturing complexity. This solution should also protect the manufacturing know-how in the case of a fabless foundry flow. Currently, the interface between manufacturing and design solely relies on design rules that do not provide these capabilities. A common proposition for design engineers in predicting shape variation is to move the entire RET/OPC/ORC into the hands of the designer. However, this approach has several major practicality issues that make it unfeasible, even as a "service" offered to designers: • Cost associated with replicating the flow on designer's desktop. • The ability of designers to understand RET/OPC and perform lithographic judgments. • Confidentiality of the recipes and lithographic settings, especially when working with a foundry. • The level of confidence the fab/foundry side has in accepting the resulting RET/OPC. • Runtime and data volume explosion. • The logistics of reflecting RET/OPC and manufacturing changes. • The ability to tie this capability to EDA optimization tools. In this paper we present a new technique and methodology that overcomes these hurdles and meets both the design and manufacturing requirements by providing a genuine DFM solution to designers. We outline a new manufacturing-to-design interface that has evolved from rule-based to model-based, and provides the required visibility to the designer on their design manufacturability. This approach is similar to other EDA approaches which have been used to successfully capture complex behavior by using a formulation that has a higher level of abstraction (for example, SPICE for transistor behavior). We will present how this unique approach uses this abstracted model to provide very accurate prediction of shape variations and at the same time, meet the runtime requirements for a smooth integration into the design flow at 90nm and below. This DFM technology enables designers to improve their design manufacturability, which reduces RET complexity, mask cost and time to volume, and increases the process window and yield.
SCOPUS;2006;An agent based synchronization scheme for multimedia applications;Agents,, Delay estimation,, IBM Aglets,, Multimedia,, Playout,, Stream synchronization;Synchronization of multimedia streams is one of the important issue in multimedia communications. In this paper, we propose an adaptive synchronization agency for synchronization of streams by using an agent based approach. The synchronization agency triggers one of the three synchronization mechanisms, point synchronization or real-time continuous or adaptive synchronization in order to adapt to the run-time and life-time presentation requirements of an application. The scheme or agency employs static and mobile agents for the following purpose: to estimate the network delays in real-time based on sustainable stream loss, to compute the skew, to monitor the loss and estimate the playout times of the presentation units. We have experimentally evaluated the scheme by using IBM Aglets and verified its functioning in terms of synchronization loss and mean buffering delays. The benefits of this agent based scheme are: asynchronous and autonomous delay estimation, flexibility, adaptability, software re-usability and maintainability. © 2005 Elsevier Inc. All rights reserved.
SCOPUS;2006;Abstract machine and structural availability;Metric,, Software availability,, Software quality;Abstract machine is the theoretical foundation of software programming. In previous research of software quality on abstract machine, because abstract machine always works correctly without even a small mistake encountered, software availability is concerned without relevant with hardware availability and environmental factors. However, software availability is defined as the probability that software is operating according to requirements at a given point in time. Hardware error, fault or failure surely has negative impact on software operation and decreases software availability. Software runtime environment, such as Operation System and/or some platforms, also have availability lower than 100%. Users tend to consider unrepeatable and unobvious hardware or environmental errors as application errors, and decrease their satisfaction with application. Therefore, such factors should be considered in software availability. Thus, an Abstract Machine with Hardware Reliability (AM-HR) and an Abstract Machine with Environment Reliability (AM-ER) is proposed in this paper to extend the AM theory to enable the investigation of software structural availability. Though structural availability isn't mature enough now, its utility in Change Management is shown to exhibit the promising prospect.
SCOPUS;2006;ANIS: A negotiated integration of services in distributed environments;Distributed systems,, Integration,, Life cycle,, Negotiation,, OSGi,, Services;The development of highly dynamic distributed environments modifies the runtime behavior of applications. Applications tend to use services available everywhere in the environment and would like to, whenever it is possible and/or needed, integrate services offered by the local environment. In particular, if no single service can satisfy the functionality required by the application, combining existing services together should be a possibility in order to fulfill the request. In this article, we propose ANIS: A Negotiated Integration System. Our system provides a framework including a set of integration management interfaces - Integrable, Negotiable, IntegrationLifeCycle - and the tools implementing these interfaces. These tools offer different techniques of integration (local/remote composition, local/remote weaving, deployment by downloading/uploading), negotiation by contracts and the capability to manage the life cycle of the integration. A prototype based on Java platform and OSGi technology is implemented as a proof-of-concept to demonstrate the potential of ANIS1. © Springer-Verlag Berlin Heidelberg 2006.
SCOPUS;2006;A comparison of nearest neighbor search algorithms for generic object recognition;;The nearest neighbor (NN) classifier is well suited for generic object recognition. However, it requires storing the complete training data, and classification time is linear in the amount of data. There are several approaches to improve runtime and/or memory requirements of nearest neighbor methods: Thinning methods select and store only part of the training data for the classifier. Efficient query structures reduce query times. In this paper, we present an experimental comparison and analysis of such methods using the ETH-80 database. We evaluate the following algorithms. Thinning: condensed nearest neighbor, reduced nearest neighbor, Baram's algorithm, the Baram-RNN hybrid algorithm, Gabriel and GSASH thinning. Query structures: kd-tree and approximate nearest neighbor. For the first four thinning algorithms, we also present an extension to k-NN which allows tuning the trade-off between data reduction and classifier degradation. The experiments show that most of the above methods are well suited for generic object recognition. © Springer-Verlag Berlin Heidelberg 2006.
SCOPUS;2006;Improving exception handling by discovering change dependencies in adaptive process management systems;;Process-aware information systems should enable the flexible alignment of business processes to new requirements by supporting deviations from the predefined process model at runtime. To facilitate such dynamic process changes we have adopted techniques from case-based reasoning (CBR). In particular, our existing approach allows to capture the semantics of ad-hoc changes, to support their memorization, and to enable their reuse in upcoming exceptional situations. To further improve change reuse this paper presents an approach for discovering dependencies between ad-hoc modifications from change history. Based on this information better user assistance can be provided when dynamic process changes have to be made. © Springer-Verlag Berlin Heidelberg 2006.
SCOPUS;2005;Business processes management based on stratified grid;Business process design,, Business Process Management,, Collaborative Business Process,, Stratified grid;In the context of current day collaborative business processes, seamlessly integrating the applications residing in different enterprises becomes necessary. Process centered BPM provides the capability for business partners to carry out such processes by providing a platform for cross enterprise applications to take part in the process. However, current business processes management systems based on services can not composite services automatically in order to meet user's requirements and lack the ability of dynamically configuration of services and resources. Roughly, existing methods can be classified into run-time binding based on task decomposition and automatic composition based on bottom-up method, but the former method needs decomposition of complex processes in advance while the later method is more complex. In addition, both methods are not goal-oriented. This paper introduces a stratified grid concept, and based on its characteristics, proposes a goal-oriented business processes management system. The method can search business processes, optimize the composite processes and configure resources in distributed stratified grid environment based on requirements and goals of users. Especially, the method can provide the user best global services and simplify the design of business process.
SCOPUS;2005;Run-time monitoring of requirements for systems composed of Web-services: Initial implementation and evaluation experience;;This paper describes a framework supporting the run-time monitoring of requirements for systems implemented as compositions of web-services specified in BPEL. The requirements that can be monitored are specified in event calculus. The paper presents an overview of the framework and describes the architecture and implementation of a tool that we have developed to ope rationalise it. It also presents the results of a preliminary experimental evaluation of the framework.
SCOPUS;2005;Compile-time stack requirements analysis with GCC motivation, development, and experiments results;;Stack overflows are a major threat to many computer applications and run-time recovery techniques are not always available or appropriate. In such situations, it is of utmost value to prevent overflows from occurring in the first place, which requires evaluating the worst case stack space requirements of an application prior to operational execution time. More generally, as run-time stack areas need memory resources, information on the stack allocation patterns in software components is always of interest. We believe that specialized compiler outputs can be of great value in a stack usage analysis framework and have developed GCC extensions for this purpose. In this paper, we first expand on the motivations for this work, then describe what it consists of so far, future directions envisioned, and experiments results.
SCOPUS;2005;The MAIS approach to web service design;;This paper presents a first attempt to realize a methodological framework supporting the most relevant phases of the design of a value-added service. A value-added service is defined as a functionality of an adaptive and multi-channel information system obtained by composing services offered by different providers. The framework has been developed as part of the MAIS project. The MAIS framework focuses on the following phases of service life cycle: requirements analysis, design, deployment, run time use and negotiation. In the first phase, the designer elicits, validates and negotiates service requirements according to social and business goals. The design phase is in charge of modelling services with an enhanced version of UML, augmented with new features developed within the MAIS project. The deployment phase considers the network infrastructure and, in particular, provides an approach to implement and coordinate the execution of services from different providers. In the run time use and negotiation phase, the MAIS methodology provides support to the optimal selection and quality renegotiation of services and to the dynamic evaluation of management costs. The paper describes the MAIS methodological tools available for different phases of service life cycle and discusses the main guidelines driving the implementation of a service management architecture, called reflective architecture, that complies with the MAIS methodological approach.
SCOPUS;2005;Component-based system development;;The development of an adequate technology for component-based development faces many challenges. This is in particular true for real-time and embedded systems. Based on the exposition in this section, we structure the issues into several groups Component specification: in the context of embedded systems, it is obvious that interface specifications of components must go beyond syntactic information and include functional and extra-functional characteristics and requirements. For real-time systems the temporal attributes of components and systems are of main interest. For embedded systems the properties specifying the resources and the properties related to dependability are important. However, there is still no consensus about how components for real-time systems should be specified. Prediction of system properties from component properties: Even if we assume that we can specify all the relevant properties of components, it is not necessarily known how they will determine the corresponding properties of systems of which they are composed. Moreover, existing component models do not provide support for predictable composition. In this, one should aim for interfaces providing full functional and extra-functional specifications of components are essential. Managing the interplay between achievable system requirements and component specifications: is complex, as the possible candidate components usually lack one or more required features. Further, the relations between the system requirements and component requirements are complex. Architecture specification: the use of components has an impact on the choice of the system architecture, as it must take into account not only the requirements, but also the available components. Component models: Component models for real-time systems are still in the very early phase of development. In general, existing component models do not support the specification of functional and extra-functional properties, in particular timing and QoS properties. Component evaluation and verification (possibly for certification): the trustworthiness of a component, which is the reliability of component in relation to its interface specification, is an important issue. The issue is difficult since the trend is to deliver components in binary form and the component development process is outside the control of component users. Protocols for component certification are of great interest Component repositories: which address the issues of how to store and retrieve components, how to index components in a component library, and how to find "similar" components. Managing changes in component requirements: an important issue are changes to components over time and possible conflicts arising from different coexisting versions of a component within the same system. A precise interface specification should allow clarifying this issue. Update and replacement of components at run-time is useful for many real-time systems. In the context of design-time composition, it is a challenge to combine this feature with design-time optimization across component boundaries. For all areas, it is evident that appropriate tools are essential for a successful component-based development. In non real-time domains there exists various tools supporting model based and component-based development and they have proved to be successful, but in the real-time domains there is a lack of such tools. There is thus a unique opportunity for transferring essential results from research into industry through development of tool suites. © Springer-Verlag Berlin Heidelberg 2005.
SCOPUS;2005;Efficient response time predictions by exploiting application and resource state similarities;;In large-scale Grids with many possible resources (clusters of computing elements) to run applications, it is useful that the resources can provide predictions of job response times so users or resource brokers can make better scheduling decisions. Two metrics need to be estimated for response time predictions: one is how long a job executes on the resource (application run time), the other is how long the job waits in the queue before starting (queue wait time). In this paper we propose an Instance Based Learning technique to predict these two metrics by mining historical workloads. The novelty of our approach is to introduce policy attributes in representing and comparing resource states, which is defined as the pool of running and queued jobs on the resource at the time to make a prediction. The policy attributes reflect the local resource scheduling policies and they can be automatically discovered using a genetic search algorithm. The main advantages of this approach compared with scheduler simulation are twofolds: Firstly, it has a better performance to meet the real time requirement of Grid resource brokering,, secondly, it is more general because the scheduling policies are learned from past observations. Our experimental results on the NIKHEF LCG production cluster show that acceptable prediction accuracy can be obtained, where the relative prediction errors for response times are between 0.35 and 0.70. © 2005 IEEE.
SCOPUS;2005;Dynamic preemption threshold scheduling for specific real-time control systems;Dynamic preemption threshold,, Real-time systems,, Scheduling algorithm;Application specific operating systems (ASOS) are developing quickly as a new trend in real-time control systems development. It often belongs to system on chip. The scheduling for ASOS should satisfy two basic demands (a) Context switching overheads are not significant,, (b) The scheduling should use small amount of RAM memory. According to characteristics of ASOS, we present a novel scheduling algorithm, named dynamic preemption threshold (DPT) scheduling, which integrates preemption threshold scheduling into the EDF (earliest deadline first). The scheduling can achieve greater processor utilization, theoretically even up to all of a processor capacity. Meanwhile, the preemption times between tasks can be effectively decreased using DPT scheduling by two ways: 1) threads allocating,, 2) dynamic thresholds regularly adjusting at runtime. With the reduction of task preemptions, memory requirements are also decreased. In addition, the DPT gives an approach to transform a static model to dynamic model seamlessly. The DPT algorithm can perfectly schedule a mixed task set with preemptive and non-preemptive tasks, and subsumes both as special cases. Thus it remains the scheduling flexibility and also decreases unnecessary context switching and memory requirements at runtime. © 2005 IEEE.
SCOPUS;2005;Monitoring with behavior view diagrams for debugging;;UML sequence diagrams are widely used during requirements analysis and design for specifying the expected message exchanges among a set of objects in various scenarios for the program to perform a certain task. In this paper, we present the behavior view diagrams, a type of extended sequence diagrams, to facilitate execution monitoring during debugging. Using a behavior view diagram, software developers can precisely specify the runtime objects whose behaviors will be monitored during debugging. Software developers can also specify the important message exchanges to be observed among these objects during the progress of various scenarios, and may further define the monitoring actions to be performed for inspecting the program state when a message exchange is observed. We also present a debugger that can automatically monitor the program execution using the information specified in a behavior view diagram. Through this monitoring, the debugger can not only check whether the scenarios are progressed as intended, but also check whether the actions performed by the program have the the desired effects on the program states. Therefore, it will be useful for detecting and localizing bugs. © 2005 IEEE.
SCOPUS;2005;Engineering runtime requirements-monitoring systems using MDA technologies;;The Model-Driven Architecture (MDA) technology toolset includes a language for describing the structure of meta-data, the MOF, and a language for describing consistency properties that data must exhibit, the OCL. Off-the-shelf tools can generate meta-data repositories and perform consistency checking over the data they contain. In this paper we describe how these tools can be used to implement runtime requirements monitoring of systems by modelling the required behaviour of the system, implementing a meta-data repository to collect system data, and consistency checking the repository to discover violations. We evaluate the approach by implementing a contract checker for the SLAng service-level agreement language, a language defined using a MOF meta-model, and integrating the checker into an Enterprise JavaBeans application. We discuss scalability issues resulting from immaturities in the applied technologies, leading to recommendations for their future development. © Springer-Verlag Berlin Heidelberg 2005.
SCOPUS;2005;A PES for use in highly safety-critical control;;The programmable electronic systems currently employed in safety-critical control implement either strictly periodical or task-based operation. Here, a concept combining the advantages of both principles is presented. Its most essential characteristics are task execution without the use of asynchronous interrupts, and a tightly integrated hardware scheme to detect processing failures, for non-intrusive monitoring, and forward recovery at runtime. The architecture builds up on physical separation of task execution and task administration, which is implemented in form of a digital circuitry. Time is quantised into execution intervals, and tasks are partitioned into execution blocks matching these intervals. This concept lowers the complexity of both hardware and temporal behaviour and, thus, conforms particularly well with the requirements of the safety standard IEC 61508. © 2005 IEEE.
SCOPUS;2005;A note on an L-approach for solving the manufacturer's pallet loading problem;Cutting and packing,, Implementation,, Pallet and container loading,, Recursive algorithm;An L-approach for packing (l,w)-rectangles into an (L,W)-rectangle was introduced in an earlier work by Lins, Lins and Morabito. They conjecture that the L-approach is exact and point out its runtime requirements as the main drawback. In this note it is shown that, by simply using a different data structure, the runtime is considerably reduced in spite of larger (but affordable) memory requirements. This reduction is important for practical purposes since it makes the algorithm much more acceptable for supporting actual decisions in pallet loading. Intensive numerical experiments showing the efficiency and effectiveness of the algorithm are presented © 2005 Operational Research Society Ltd. All rights reserved.
SCOPUS;2005;Effective adaptive computing environment management via dynamic optimization;;To minimize the surging power consumption of microprocessors, adaptive computing environments (ACEs) where microarchitectural resources can be dynamically tuned to match a program's runtime requirement and characteristics are becoming increasingly common. Adaptive computing environments usually have multiple configurable hardware units, necessitating exploration of a large number of combinatorial configurations in order to identify the most energy-efficient configuration. In this paper, we propose a scheme for efficient management of multiple configurable units, utilizing the inherent capabilities of dynamic optimization systems. Most dynamic optimizers typically detect dominant code regions (hotspots). We develop an ACE management scheme where hotpot boundaries are used for phase detection and adaptation. Since hotspots are of variable sizes and are often nested, program phase behavior which is hierarchical in nature is automatically captured in this technique. To demonstrate the usefulness and effectiveness of our framework, we use the proposed framework to dynamically adapt the sizes of L1 data and L2 caches that have different reconfiguration latencies and overheads. Our technique reduces L1D and L2 cache energy consumption by 47% and 58%, while a popular previously proposed technique only achieves reduction of 32% and 52% respectively.
SCOPUS;2005;Construction of weighted finite state transducers for very wide context-dependent acoustic models;;A previous paper by the authors described an algorithm for efficient construction of Weighted Finite State Transducers for speech recognition when high-order context-dependent models of order K > 3 (triphones) with tied state observation distributions are used, and showed practical application of the algorithm up to K = 5 (quinphones). In this paper we give additional details of the improved implementation and analyze the algorithm's practical runtime requirements and memory footprint for context-orders up to K = 13 (+/-6 phones context) when building fully cross-word capable WFSTs for large vocabulary speech recognition tasks. We show that for typical systems it is possible to use any practical context-order K ≤ 13 without having to fear an exponential explosion of the search space, since the necessary state ID to phone transducer (resembling a phone-loop observing all possible K-phone constraints) can be built in a few minutes at most. The paper also gives some implementation details of how we efficiently collect context statistics and build phonetic decision trees for very wide context-dependent acoustic models. © 2005 IEEE.
SCOPUS;2005;Semantic and syntactic modeling of component-based services for context-aware pervasive systems using OWL-s;;In this paper, we present a service design methodology and specification as a basis for a pervasive context-aware service infrastructure. The service specification is based on the OWL-s specification, a standard proposed to add a semantic layer on top of WSDL web service descriptions. We have defined a set of OWL-s concepts that make it possible to express various pervasive service related properties, including service adaptation, relocation, personalization, deployment and runtime requirements. Though not straightforward to parse within strict resource boundaries, OWL-s provides an open, flexible specification language for expressing syntactic and semantic pervasive service characteristics and it increases service interoperability.
SCOPUS;2005;Tracing cross-cutting requirements via context-based constraints;;In complex systems, it is difficult to identify which system element is involved in which requirement. In this article, we present a new approach for expressing and validating a requirement even if we don't precisely know which system elements are involved: a context-based constraint (CoCon) can identify the involved elements according to their context. CoCons support checking the system for compliance with requirements during (re-)design, during (re-)configuration or at runtime because they specify requirements on an abstract level independent of the monitored artefact type. They facilitate handling cross-cutting requirements for possibly large, overlapping or dynamically changing sets of system elements - even across different artefact types or platforms. Besides defining CoCons, we discuss algorithms for detecting violated or contradicting CoCons. © 2005 IEEE.
SCOPUS;2005;Correlating features and code using a compact two-sided trace analysis approach;Dynamic Analysis,, Feature,, Feature-Traces,, Software Evolution,, Software Metrics;Software developers are constantly required to modify and adapt application features in response to changing requirements. The problem is that just by reading the source code, it is difficult to determine how classes and methods contribute to the runtime behavior of features. Moreover, dependencies between system features are not obvious, consequently software maintenance operations often result in unintended side effects. To tackle these problems, we propose a compact feature-driven approach (i.e., summarized trace information) based on dynamic analysis to characterize features and computational units of an application. We extract execution traces to achieve an explicit mapping between features and classes using two complementary perspectives. We apply our approach to two case studies and we report our findings. © 2005 IEEE.
SCOPUS;2005;A CORBA-based dynamic reconfigurable middleware;;The widespread Internet and mobile applications demand increasing requirements for easy and flexible to reconfigure a deployed system during run time. The middleware proposed to help programmer developing distributed application automatically inherits these demands and requirements. In his paper we present a CORBA based middleware system that adopts our technology of Routing Based Workflow (RBW). RBW has modeled the execution environment of cooperative components. Within RBW component instances are temporally bound to routing for their functionality execution. It is the temporal binding makes the dynamic reconfiguration of software components easy to realize and greatly simplify the hard problems of preserving consistency. © Springer-Verlag Berlin Heidelberg 2005.
SCOPUS;2005;Autonomic runtime system for large scale parallel and distributed applications;;The development of efficient parallel algorithms for large scale wildfire simulations is a challenging research problem because the factors that determine wildfire behavior are complex,, they include fuel characteristics and configurations, chemical reactions, balances between different modes of heat transfer, topography, and fire/atmosphere interactions. These factors make static parallel algorithms inefficient, especially when large number of processors are used because we cannot predict accurately the propagation of the fire and its computational requirements at runtime. In this paper, we present an Autonomic Runtime Manager (ARM) to dynamically exploit the physics properties of the fire simulation and use them as the basis of our self-optimization algorithm. At each step of the wildfire simulation, the ARM decomposes the computational domain into several natural regions (e.g., burning, unburned, burned) where each region has the same temporal and special characteristics. The number of burning, unburned and burned cells determines the current state of the fire simulation and can then be used to accurately predict the computational power required for each region. By regularly monitoring the state of the simulation and analyzing it, and use that to drive the runtime optimization, we can achieve significant performance gains because we can efficiently balance the computational load on each processor. Our experimental results show that the performance of the fire simulation has been improved by 45% when compared with a static portioning algorithm that does not take into considerations the state of the computations. © Springer-Verlag Berlin Heidelberg 2005.
SCOPUS;2005;The application of compile-time reflection to software fault tolerance using Ada 95;Ada,, Atomic actions,, Backward error recovery,, Conversations,, Recovery blocks,, Reflection,, Software fault tolerance;Transparent system support for software fault tolerance reduces performance in general and precludes application-specific optimizations in particular. In contrast, explicit support - especially at the language level - allows application-specific tailoring. However, current techniques that extend languages to support software fault tolerance lead to interwoven code addressing functional and non-functional requirements. Reflection promises both significant separation of concerns and a malleability allowing the user to customize the language toward the optimum point in a language design space. To explore this potential we compare common software fault tolerance scenarios implemented in both standard and reflective Ada. Specifically, in addition to backward error recovery and recovery blocks, we explore the application of reflection to atomic actions and conversations. We then compare the implementations in terms of expressive power, portability, and performance. © Springer-Verlag Berlin Heidelberg 2005.
SCOPUS;2005;Self-optimization of large scale wildfire simulations;;The development of efficient parallel algorithms for large scale wildfire simulations is a challenging research problem because the factors that determine wildfire behavior are complex. These factors make static parallel algorithms inefficient, especially when large number of processors is used because we cannot predict accurately the propagation of the fire and its computational requirements at runtime. In this paper, we propose an Autonomic Runtime Manager (ARM) to dynamically exploit the physics properties of the fire simulation and use them as the basis of our self-optimization algorithm. At each step of the wildfire simulation, the ARM decomposes the computational domain into several natural regions (e.g., burning, unburned, burned) where each region has the same temporal and special characteristics. The number of burning, unburned and burned cells determines the current state of the fire simulation and can then be used to accurately predict the computational power required for each region. By regularly monitoring and analyzing the state of the simulation, and using that to drive the runtime optimization, we can achieve significant performance gains because we can efficiently balance the computational load on each processor. Our experimental results show that the performance of the fire simulation has been improved by 45% when compared with a static portioning algorithm. © Springer-Verlag Berlin Heidelberg 2005.
SCOPUS;2005;Frameworks for model-driven software architecture;;On every new development new issues are taken into account to cope with either environmental or the stakeholders needs that are evolving over time. Several approaches have faced this problem. Some of them exhibit this evolution ability by using static design/compile-time techniques whereas others introduce this ability in the system at run-time. Nevertheless, in both cases evolving requirements give rise to the need of adaptability which is inherent to every software development. This paper sketches our work in this field in which we are concerned about the MDSAD (Model Driven Software Architecture Development) methodology to guide the reflexive development of architectures from the software requirements. In particular, we are detailing the first step of this methodology, i.e., the definition of the goals model whose constituents are the fundamental basis for the overall process defined in MDSAD proving its suitability for obtaining traceable architectural models. It provides our work either to its ability to specify and manage positive and negative interactions among goals or to its capability to trace low-level details back to high-level concerns. © Springer-Verlag Berlin Heidelberg 2005.
SCOPUS;2005;Study on support environment of distributed complex system;Distributed complex system,, Interoperability,, Model standardization,, Support environment;The foundation of support environment of distributed complex system has great importance for resource sharing, efficiency improvement and cost-efficiency. Support environment, including R&D (Research and Design) environment and Runtime environment, can be applied in full life cycle of system, from requirement analysis to maintenance and training. New simulation requirement to support environment was proposed. Study and application results in AST_LAB was shown, and key technologies was addressed including model standardization,, interoperability,, spatial-temporal consistency and visualization. At last, new study and research was brought up.
SCOPUS;2005;The Eclipse 3.0 platform: Adopting OSGi technology;;From its inception Eclipse was mainly designed to be a tooling platform, but with Version 3.0, Eclipse is now evolving toward a Rich Client Platform (RCP). This change, driven by the open-source community, brought a whole set of new requirements and challenges for the Eclipse platform, such as dynamic plug-in management, services, security, and improved performance. This paper describes the path from the proprietary Eclipse 2.1 runtime to the new Eclipse 3.0 runtime based on OSGi™ specifications. It details the motivation for such a change and discusses the challenges this change presented. © Copyright 2005 by International Business Machines Corporation.
SCOPUS;2005;Early verification and validation of mission critical systems;Acceptance Tests,, Animation,, Early Verification,, Goal-orientation,, Monitoring,, Requirements Engineering,, Validation;Our world is increasingly relying on complex software and systems. In a growing number of fields such as transportation, finance, telecommunications, medical devices, they now play a critical role and require high assurance. To achieve this, it is imperative to produce high quality requirements. The KAOS goal-oriented requirements engineering methodology provides a rich framework for requirements elicitation and management of such systems. This paper demonstrates the practical industrial application of that methodology. The non-critical parts are modelled semi-formally using a graphical language for goal-oriented requirements engineering. When and where needed (ie. for critical parts of a system) the model can be specified at formal level using a real-time temporal logic. That formal level seamlessly extends the semi-formal level which can also help hide the formality for the non-specialist. To ensure at an early stage that the right system is being built and that the requirements model is right, validation and verification tools are applied on that model. Early verification checks help to discover missing requirements, overlooked assumptions or incorrect goal refinements. State machines generation from operations provides an executable model useful for validation purposes or for deriving an initial design. Acceptance test cases and runtime behavior monitors can also be derived from the model. The process is supported by an integrated toolbox implementing the above tools by a roundtrip mapping of KAOS requirements level notations to the languages of formal technology tools such as model-checkers, SAT engines or constraint solvers. A graphical visualization framework also significantly helps validation using domain-based representations. © 2005 Elsevier B.V. All rights reserved.
SCOPUS;2005;Memory Requirements of Java Bytecode Verification on Limited Devices;Bytecode Verification,, Java Card,, Limited Devices,, Proof Carrying Code;Bytecode verification forms the corner stone of the Java security model that ensures the integrity of the runtime environment even in the presence of untrusted code. Limited devices, like Java smart cards, lack the necessary amount of memory to verify the type-safety of Java bytecode on their own. Proof carrying code techniques compute, outside the device, tamper-proof certificates which simplify bytecode verification and pass them along with the code. Rose has developed such an approach for a small subset of the Java bytecode language. In this paper, we extend this approach to real world Java software and develop a precise model of the memory requirements on the device. We use a variant of interval graphs to model liveness of memory regions in the checking step. Based on this model, memory-optimal checking strategies are computed outside the device and attached to the certificate. The underlying type system of the bytecode verifier has been augmented with multi-dimensional arrays and recognizes references to uninitialized Java objects. Our detailed measurements, based on real world Java libraries, demonstrate that the approach offers a substantial improvement in size of certificate over the similar approach taken by the KVM verifier. Worst case memory consumption on the device is examined as well and it turns out that the refinements based on our model save a significant amount of memory. © 2005 Published by Elsevier B.V.
SCOPUS;2005;Dynamic software assembly for automatic deployment-oriented adaptation;Deployment-oriented adaptation,, Dynamic software assembly,, Software adaptability;The notion of software adaptation considered in this paper relates to the capability of making software systems adjustable to varying deployment requirements. In this context we seek for the necessary runtime infrastructure to allow software systems adapt on the fly to the particular execution requirements. The primary assumption is that the constituent components of a software system may have to be provided with alternative incarnations, each potentially addressing varying deployment needs. In this context, adaptation is treated as a runtime function of the system itself, realising a component and assembly process, since the deployment-specific parameters are only known upon execution start-up. © 2005 Elsevier B.V.
SCOPUS;2004;Twelfth ACM SIGSOFT International Symposium on the Foundations of Software Engineering, SIGSOFT 2004/FSE-12;;The proceedings contain 28 papers from the Twelfth ACM SIGSOFT International Symposium on the Foundations of Software Engineering, SIGSOFT 2004/FSE-12. The topics discussed include: resolving uncertainties during trace analysis,, automating comprehensive safety analysis of concurrent programs using VeriSoft and TXL,, efficient incremental algorithms for dynamic detection of likely invariants,, system architecture: the context for scenario-based model synthesis,, merging partial behavioural models,, reasoning about partial goal satisfaction for requirements and design engineering,, and the usability problem for home appliances: engineers caused it, engineers can fix it.
SCOPUS;2004;A propositional logic-based method for verification of feature models;;The feature model is a domain/product-line oriented requirements model based on hierarchical structure and explicit variability modeling, and has been adopted by several important software reuse methods. However, with respect to the problem of verification of constraints on features and verification of partially customized feature models, these methods tend to be semi-formal and offer little formal assistance. In this paper, we propose a propositional logic-based method for the verification of feature models at different binding times. The most important characteristic of this method is that it integrates the logical verification with binding times, which makes it can be used to verify any partially customized feature models at any binding time (except run-time). In this method, constraints on features are formalized by logical sentences. Thus, the verification of feature models is converted into satisfaction problems in the logic. With this formal method, verification problems such as the detection of inconsistent constraints or the detection of conflicting or unnecessary binding resolutions can be automatically revealed. © Springer-Verlag Berlin Heidelberg 2004.
SCOPUS;2004;Goal and scenario driven product line development;;Product line development has proven a successful approach to achieve strategic and large-grained reuse and hence time-to-market and productivity. A key to successful software product lines is to identify and anlyze the right functionality for reusable implementation, and thus perform delated requirements analysis for product lines to exploit commonality and variability (C&V) within a family of related systems. In this paper, we describe the goal and scenario driven approach for developing software product lines, which elicits product line requirements and analyzes C&V in products of a product line, as well as supports developing a paricluar product in the product line. We also discuss our ultimate goal that is to develop a dynamic software product line, which can produce new products at runtime by dynamic reconfiguration of the product line based on goals and scenarios. © 2004 IEEE.
SCOPUS;2004;Strategies for handling the activity problem in runtime software evolution by reducing activity;;Different strategies to reduce the activity problem in runtime software evolutions, are presented. The circumstances under which those strategies are viable were also analyzed. A set of three properties that distinguish between the methods that are highly active for different reasons were proposed. A tool was created that automatically analyzes a running system and find the properties for any given methods. The main strategies for reducing the activity problem include outline parts at development time, outline non-blocking code at development time, outline updated part by runtime refractoring, and pause service of a method.
SCOPUS;2004;A dot placement approach to stochastic screening using bitmasks;;FM or stochastic screening is a popular approach to halftoning for many applications. The error diffusion algorithm delivers extremely good screen quality but at the price of a computationally-intensive runtime process. Point processes, using either dither arrays or bitmask sets, have efficient runtime requirements but often produce halftones of much lower quality. The generation of such screens usually involves starting with a random pattern and applying some simulated annealing process to gradually improve its characteristics. This paper proposes a method for generating stochastic patterns that employs a dot placement algorithm in which each dot is placed in a position "appropriate" for producing good stochastic output. The algorithm is then enhanced by applying a smoothing step at the end of each halftone pattern generation to adjust any dots that, due to the placement of later dots, are now in sub-optimal positions. Although the algorithm can be used to produce dither arrays, it is primarily aimed at generating bitmasks where the additional degree pattern freedom is exploited to improve pattern smoothness. The algorithm also permits second order stochastic patterns for use with imprecise print devices such as electro-photographic printers.
SCOPUS;2004;Scaling SDI systems via query clustering and aggregation;;XML-based Selective Dissemination of Information (SDI) systems aims to quickly deliver useful information to the users based on their profiles or user subscriptions. These subscriptions are specified in the form of XML queries. This paper investigates how clustering and aggregation of user queries can help scale SDI systems by reducing the number of document-subscription matchings required. We design a new distance function to measure the similarity of query patterns, and develop a filtering technique called YFilter* that is based on YFilter. Experiment results show that the proposed approach is able to achieve high precision, high recall, while reducing runtime requirement. © Springer-Verlag 2004.
SCOPUS;2004;A configurable XForms implementation;;XForms is a new language for defining dynamic forms and user interfaces for the World Wide Web. In order to take advantage of the user interaction related features in the language, a client side processor is needed. This paper describes a configurable open source software implementation of XForms. The main goal of the implementation is to conform to the World Wide Web Consortium's XForms Recommendation. The other goals are external to the XForms specification and are related to the portability and configurability of the processor. The important questions are related to implementing an XForms processor for diverse environments, and the integration of XForms and other XML languages with different layout models. In the paper, more detailed requirements are gathered from these goals. Also, the design and implementation are presented in detail, in order to give insight to the more difficult and non-obvious parts of the software. The results of the paper cover the runtime requirements of the XForms processor. © 2004 IEEE.
SCOPUS;2004;Study on integrated simulation environment for air defense combat;Air defense combat,, Architecture,, Integrated simulation environment,, Run-time framework;Building Integrated Simulation Environment (ISE) is the effective method for air defense combat simulation. The architecture and functions of ISE of air defense combat are presented on the basis of requirements analysis in this paper, and the run-time framework of air defense combat simulation based on such ISE is discussed in detail. ISE provides strong tools for the development of air defense combat simulation application with reusability and creditability.
SCOPUS;2004;Model-driven business process integration and management: A case study with the Bank SinoPac regional service platform;;Business process integration and management (BPIM) is a critical element in enterprise business transformation. Small and medium-sized businesses have their own requirements for BPIM solutions: The engagement methodology should be fast and efficient,, a reusable and robust framework is required to reduce cost,, and the whole platform should be lightweight so that one can easily revise, develop, and execute solutions. We believe that model-driven technologies are the key to solving all of the challenges mentioned above. Model Blue, a set of model-driven business integration and management methods, frameworks, supporting tools, and a runtime environment, was developed by the IBM China Research Laboratory (CRL) in Beijing to study the efficacy of model-driven BPIM. To verify the technology and methodology, Model Blue was deployed with Bank SinoPac, a mid-sized bank headquartered in Taiwan. A lightweight BPIM solution platform was delivered for Bank SinoPac to design, develop, and deploy its business logic and processes. During the eight-month life span of the project, IBM teams developed four major solutions for Bank SinoPac, which also developed one solution independently. In spite of the remote working environment and the outbreak of the Severe Acute Respiratory Syndrome illness, the project was completed successfully on schedule and within budget, with up to 30% efficiency improvement compared with similar projects. Bank SinoPac was satisfied with the technology and methodology, and awarded IBM other projects. In this paper, we illustrate how each key business process integration and solution development phase was carried out and guided by business process modeling, together with major experiences gained. The following technical aspects are discussed in detail: a two-dimensional business process modeling view to integrate flow modeling and data modeling,, a lightweight processing logic automation environment with tooling support,, and the end-to-end BPIM methodology, with models and documents successfully integrated as part of (or replacement for) the deliverables defined in the existing servicing methodologies and software engineering approaches. © Copyright 2004 by International Business Machines Corporation.
SCOPUS;2004;A debugging strategy based on the requirements of testing;Automated debugging,, Data-flow testing,, Debugging tool,, Dynamic testing information,, Fault localization;Testing and debugging activities consume a significant amount of the software development and maintenance budget. To reduce this cost, the use of testing information for debugging purposes has been advocated. In general, heuristics are used to select structural testing requirements (nodes, branches and definition-use associations) more closely related to the manifestation of a failure, which are then mapped into a piece of code. The intuition is that the selected piece of code is likely to contain the fault. However, this approach has its drawbacks. Heuristics that select a manageable piece of code are less likely to hit the fault and the piece of code itself does not provide enough guidance for program understanding - a major factor in program debugging. These problems occur because this approach relies only on static Information - a fragment of code. We introduce a strategy for fault localization that addresses these problems. The strategy - called the debugging strategy based on the requirements of testing (DRT) - is based on the investigation of indications (or hints) provided at run-time by data-flow testing requirements (definition-use associations). Our claim is that the selected definition-use associations may fail to hit the fault site, but still provide information useful for fault localization. The strategy's novelty and attractiveness are threefold: (i) the focus on dynamic information related to testing data,, (ii) implementation in state-of-the-practice symbolic debuggers with a low overhead,, and (iii) the use of algorithms which consume constant memory and are linear on the number of branches in the program. A case study shows that our claim is valid (for the subject program) and a prototype tool implements the strategy. Copyright © 2004 John Wiley & Sons, Ltd.
SCOPUS;2004;Policy-based dynamic reconfiguration of mobile-code applications;;Policy-Enabled Mobile Applications (Poema) is a policy-based approach to mobility programming that expresses and controls reconfiguration strategies at a high level of abstraction, separate from the application's functionality. It provides an integrated environment for developing applications that can change both their functionality and layout at runtime in response to environment conditions.
SCOPUS;2004;Rapid computation of dynamic stability derivatives;;A new technique for rapid computation of dynamic stability derivatives with steady Euler CFD codes is presented. The approach is analogous to that used in linear methods. It treats the rotational velocity as a perturbation on the steady-state solution. Standard thermodynamic relations from compressible, inviscid fluid dynamics are used to derive the modified forces and moments resulting from the generalized rotation. The coefficient of the linear term in the rotation rate is related to the desired stability derivative. Additional terms necessary to alter the center of rotation and the moment reference are also derived. All calculations are performed at the same time as other force and moment accounting. Thus, there is little impact on overall runtime requirements. Results of this model are compared with time-accurate rotating solutions, linear methods, and experimental data. The method produces results that can be characterized as having accuracy similar to that claimed for the traditional linear methods. Such accuracy is satisfactory for most design efforts.
SCOPUS;2004;A method to develop feasible requirements for java mobile code application;Access control,, Anti-requirements,, Goal oriented requirements analysis,, Java mobile codes,, Security policy;We propose a method for analyzing trade-off between an environment where a Java mobile code application is running and requirements for the application. In particular, we focus on the security-related problems that originate in low-level security policy of the code-centric style of the access control in Java runtime. As the result of this method, we get feasible requirements with respect to security issues of mobile codes. This method will help requirements analysts to compromise the differences between customers' goals and realizable solutions. Customers will agree to the results of the analysis by this method because they can clearly trace the reasons why some goals are achieved but others are not. We can clarify which functions can be performed under the environment systematically. We also clarify which functions in mobile codes are needed so as to meet the goals of users by goal oriented requirements analysis(GORA). By comparing functions derived from the environment and functions from the goals, we can find conflicts between the environments and the goals, and also find vagueness of the requirements. By resolving the conflicts and by clarifying the vagueness, we can develop bases for the requirements specification.
SCOPUS;2004;16th International Conference on Advanced Information Systems Engineering, CAiSE 2004;;The proceedings contain 41 papers. The special focus in this conference is on Enterprise Modelling, Data Integration, Conceptual Modelling, Workflows, Methodologies for Is Development and Databases. The topics include: Aligning organizational performance to IT development and integration,, model driven architectures for enterprise information systems,, simple and minimum-cost satisfiability for goal models,, a BAV data integration system for heterogeneous data sources,, adding agent-oriented concepts derived from Gaia to agent OPEN,, an ontologically well-founded profile for UML conceptual models,, measuring expressiveness in conceptual modeling,, design and implementation of the YAWL system,, multiple instantiation in a dynamic workflow environment,, towards a meta tool for change-centric method engineering,, two-hemisphere model driven approach,, an analysis of Clark-Wilson model in a database environment,, optimizing DOM programs on xml views over existing relational databases,, applicability of ERP systems for knowledge management in the context of quality management,, a combined runtime environment and web-based development environment for web application engineering,, enabling personalized composition and adaptive provisioning of web services,, synchronising models in an air traffic management case study,, facing document-provider heterogeneity in knowledge portals,, adaptive web-based courseware development using metadata standards and ontologies,, on the transparent management of persistent objects,, analysing slices of data warehouses to detect structural modifications,, empirical validation of metrics for conceptual models of data warehouses,, data warehouse methodology and cooperation of processes through message level agreement.
SCOPUS;2004;On the advantages of approximate vs. complete verification: Bigger models, faster, less memory, usually accurate;;We have been exploring LURCH, an approximate (not necessarily complete) alternative to traditional model checking based on a randomized search algorithm. Randomized algorithms like LURCH have been known to outperform their deterministic counterparts for search problems representing a wide range of applications. The cost of an approximate strategy is the potential for inaccuracy. If complete algorithms terminate, they find all the features they are searching for. On the other hand, by its very nature, randomized search can miss important features. Our experiments suggest that this inaccuracy problem is not too serious. In the case studies presented here and elsewhere, LURCHS random search usually found the correct results. Also, these case studies strongly suggest that LURCH can scale to much larger models than standard model checkers like NuSMV and SPIN. The two case studies presented in this paper are selected for their simplicity and their complexity. The simple problem of the dining philosophers has been widely studied. By making the dinner more crowded, we can compare the memory and runtimes of standard methods (SPIN) and LURCH. When hundreds of philosophers sit down to eat, both LURCH and SPIN can find the deadlock case. However, SPINS memory and runtime requirements can grow exponentially while LURCHS requirements stay quite low. Success with highly symmetric, automatically generated problems says little about the generality of a technique. Hence, our second example is far more complex: a real-world flight guidance system from Rockwell Collins. Compared to NuSMV, LURCH performed very well on this model. Our random search finds the vast majority of faults (close to 90%),, runs much faster (seconds and minutes as opposed to hours),, and uses very little memory (single digits to 10s of megabytes as opposed to 10s to 100s of megabytes). The rest of this paper is structured as follows. We begin with a theoretical rationale for why random search methods like LURCH can be incomplete, yet still successful. Next, we note that for a class of problems, the complete search of standard model checkers can be overkill. LURCH is then briefly introduced and our two case studies are presented. © 2004 IEEE.
SCOPUS;2003;Architecting Adaptable Software Using COTS: An NFR Approach;Adaptability,, COTS,, Non-functional requirements,, Software architecture;The use of Commercial-Off-The-Shelf (COTS) components presents a great promise, as well as challenges and risks. In this paper, we describe an Adaptable COTS-Aware Software Architecting (ACASA) framework that addresses the concerns of the various stakeholders of the proposed system, in the presence of COTS components, with a special emphasis on adaptability as a non-functional requirement. In particular, we describe a two-phase matching-and-selection scheme: one phase by use of functionality, and the other by use of adaptability. The ACASA framework is illustrated by way of a telepresence system example.
SCOPUS;2003;Quality of interactive models;;Interactive models have been proposed as a general technique for increasing the flexibility of computerised information systems. Interactive models are first made during development, but are also available for manipulation by the users at run-time, and the model contents influence the behaviour of the system. Such models are more immersed in day-to-day work activities than the models conventionally developed during software development. Consequently, they face stronger requirements, particularly regarding comprehensibility, simplicity and flexibility. A comprehensive overview and classification of these requirements is currently lacking in the literature on interactive models. We have earlier developed a framework for understanding and assessing the quality of models in general, with emphasis on conceptual models. The framework has earlier been specialised in several directions, but primarily for passive models such as enterprise and requirements models. In this paper we extend our quality framework towards assessing interactive models. These extensions are based on our experiences from implementing interactive modelling languages and support systems. Whereas parts of the framework can be used as originally defined, other areas give quite different results due to the much tighter interplay between model changes and domain changes than what is found when using traditional modelling and system development approaches. This results in a useful deepening of our framework, and improvement of its practical applicability for understanding the quality of interactive models. © Springer-Verlag Berlin Heidelberg 2003.
SCOPUS;2003;Compiler-decided dynamic memory allocation for scratch-pad based embedded systems;Compiler,, Embedded Systems,, Memory Allocation,, Scratch-Pad;This paper presents a highly predictable, low overhead and yet dynamic, memory allocation strategy for embedded systems with scratch-pad memory. A scratch-pad is a fast compiler-managed SRAM memory that replaces the hardware-managed cache. It is motivated by its better real-time guarantees vs cache and by its significantly lower overheads in energy consumption, area and overall runtime, even with a simple allocation scheme [4]. Existing scratch-pad allocation methods are of two types. First, software-caching schemes emulate the workings of a hardware cache in software. Instructions are inserted before each load/store to check the software-maintained cache tags. Such methods in-cur large overheads in runtime, code size, energy consumption and SRAM space for tags and deliver poor real-time guarantees just like hardware caches. A second category of algorithms partitions variables at compile-time into the two banks. For example, our previous work in [3] derives a provably optimal static allocation for global and stack variables and achieves a speedup over all earlier methods. However, a drawback of such static allocation schemes is that they do not account for dynamic program behavior. It is easy to see why a data allocation that never changes at runtime cannot achieve the full locality benefits of a cache. In this paper we present a dynamic allocation method for global and stack data that for the first time, (i) accounts for changing program requirements at runtime (ii) has no software-caching tags (iii) requires no run-time checks (iv) has extremely low overheads, and (v) yields 100% predictable memory access times. In this method data that is about to be accessed frequently is copied into the SRAM using compiler-inserted code at fixed and infrequent points in the program. Earlier data is evicted if necessary. When compared to a provably optimal static allocation our results show runtime reductions ranging from 11% to 38%, averaging 31.2%, using no additional hardware support. With hardware support for pseudo-DMA and full DMA, which is already provided in some commercial systems, the runtime reductions increase to 33.4% and 34.2% respectively. Copyright 2002 ACM.
SCOPUS;2003;Programming at runtime: Requirements & paradigms for nonprogrammer web application development;;We investigate the femibiliy of nonprogramnier web application development and propose the creation of end-user programming tools that address the issue at a high level of abstraction. The results of three related empirical studies and one protoping effort are reported. We surveyed nonprogrammers' needs for web applications and studied how nonprogrammers would naturally approach web development. To express what a tool should provide we summarize high-level components and concepts employed by web applications. To express how a tool may provide its functionality, we propose "Programming-at-Runtime" - a programming paradigm that is in its core similar to the automatic recalculation in spreadsheets. Finally, we introduce "FlashLight" - a protoype web development tool for nonprogrammers. © 2003 IEEE.
SCOPUS;2003;Cyclone: A broadcast-free dynamic instruction scheduler with selective replay;;To achieve high instruction throughput, instruction schedulers must be capable of producing high-quality schedules that maximize functional unit utilization while at the same time enabling fast instruction issue logic. Many solutions exist to the scheduling problem, ranging from compile-time to run-time approaches. Compile-time solutions feature fast and simple hardware, but at the expense of conservative schedules. Dynamic schedulers produce high-quality schedules that incorporate run-time information and dependence speculation, but implementing these schedulers requires complex circuits that can slow processor clock speeds. In this paper, we present the Cyclone scheduler, a novel design that captures the benefits of both compile- and run-time scheduling. Our approach utilizes a list-based single-pass instruction scheduling algorithm, implemented by hardware at run-time in the front end of the processor pipeline. Once scheduled, instructions are injected into a timed queue that orchestrates their entry into execution. To accommodate branch and load/store dependence speculation, the Cyclone scheduler supports a simple selective replay mechanism. We implement this technique by overloading instruction register forwarding to also detect instructions dependent on incorrectly scheduled operations. Detailed simulation analyses suggest that with sufficient queue width, the Cyclone scheduler can rival the instruction throughput of similarly wide monolithic dynamic schedulers. Furthermore, the circuit complexity of the Cyclone scheduler is much more favorable than a broadcast-based scheduler, as our approach requires no global control signals.
SCOPUS;2003;Acquiring and incorporating state-dependent timing requirements;Computer science,, Condition monitoring,, Control systems,, Design methodology,, Job design,, Real time systems,, Road accidents,, Runtime,, Time factors,, Timing;Some real-time systems are designed to deliver services to objects that are controlled by external sources. Their services must be delivered on a timely basis, and the system fails when some services are delivered too late. Such a system may fail if the timing requirements, which it is designed to meet are erroneous. It may under-utilize resources and, consequently, be costly or unreliable if the requirements are too stringent. In general, the timing requirements of the system may change when the states of the objects monitored by the system change. Hence, one must identify how changes in object states call for changes in system requirements and how these changes should be incorporated in the design and implementation of the system. We first describe a methodology to determine timing requirements and to take into account of requirement changes at runtime. The method is based on several timing requirement determination schemes. Simulation data show that these schemes are effective for applications such as mobile IP hand-offs. We then discuss how to incorporate this methodology in the design of such systems and in the development process. © 2003 IEEE.
SCOPUS;2002;Supporting objects in run-time bytecode specialization;Object-oriented paradigm,, Partial evaluation,, Program transformation,, Specialization;This paper describes a run-time specialization system for the Java language. One of the main difficulties of supporting the full Java language resides in a sound yet effective management of references to objects. This is because the specialization process may share references with the running application that executes the residual code, and because side-effects through those references by the specialization process could easily break the semantics of the running application. To cope with these difficulties, we elaborate requirements that ensure sound run-time specialization. Based on them, we design and implement a run-time specialization system for the Java language, which exhibits, for instance, approximately 20-25% speed-up factor for a ray-tracing application.
SCOPUS;2002;Using indexed data structures for program specialization;Automated software engineering,, Optimization,, Partial evaluation,, Program specialization,, Staged computation;Given a program and values of static (fixed) inputs, program specialization generates an optimized version of the program that only requires dynamic (run-time) inputs. It has been an useful tool for such areas as operating systems, multimedia applications, and scientific applications. However, the size of specialized code may grow up exponentially which makes program specialization impractical for many applications. In this paper, we present a mechanism to address this problem by using indexed data structures. Unlike traditional program specialization, which encodes the result of specialization only into run-time code, our method encodes the values of multi-valued static expressions into indexed data structures and single-valued static expressions into run-time code. Because the sizes of the indexed data structures are much smaller than that of program code, we can overcome the size problem of program specialization. With a preliminary implementation for Java, we achieved improvement in performance up to a factor of 3 with very low memory and space requirements and overheads.
SCOPUS;2002;Using reflection as a mechanism for enforcing security policies on compiled code;;Securing application resources or defining finer-grained access control for system resources using the Java security architecture requires manual changes to source code. This is error-prone and cannot be done if only compiled code is present. We show how behavioural reflection can be used to enforce security policies on compiled code. Other authors have implemented code rewriting toolkits that achieve the same effect but they either require policies to be expressed in terms of low level abstractions or require the use of new high level policy languages. Our approach allows reuseable policies to be implemented as metaobjects in a high level object oriented language (Java), and then bound to application objects at loadtime. The binding between metaobjects and objects is implemented through bytecode rewriting under the control of a declarative binding specification. We have implemented this approach using Kava which is a portable reflective Java implementation. Kava allows customisation of a rich range of runtime behaviour, and provides a non-bypassable meta level suitable for implementing security enforcement. We discuss how we have used Kava to show how to secure a third-party application, how we prevent Kava being bypassed, and compare its performance with non-reflective security enforcement.
SCOPUS;2002;SIMOO-RT - An object-oriented framework for the development of real-time industrial automation systems;Object-oriented methods,, Real-time systems,, Software tools,, System analysis and design;This paper presents SIMOO-RT, an object-oriented framework designed to support the whole development cycle of real-time industrial automation systems. It is based on the concept of distributed active objects, which are autonomous execution entities that have their own thread of control, and that interact with each other by means of remote methods invocation. SIMOO-RT covers most of the development phases, from requirements engineering to implementation. It starts with the construction of an object model of the technical plant to be automated, on which user and problem-domain requirements are captured. Here, emphasis on modeling timing constraints is given. The technical details involved in the process of mapping problem-domain objects to design specific entities as well as the automatic code generation for the runtime application are discussed in the paper. Furthermore, details are given on how to monitor the runtime applications and to evaluate its timing restrictions.
SCOPUS;2002;Using execution trace data to improve distributed systems;CORBA,, Distributed systems,, Execution visualization,, Tracing;One of the most challenging problems facing today's software engineer is to understand and modify distributed systems. One reason is that in actual use systems frequently behave differently than the developer intended. In order to cope with this challenge, we have developed a three-step method to study the run-time behavior of a distributed system. First, remote procedure calls are traced using CORBA interceptors. Next, the trace data is parsed to construct RPC call-return sequences, and summary statistics are generated. Finally, a visualization tool is used to study the statistics and look for anomalous behavior. We have been using this method on a large distributed system (more than 500000 lines of code) with data collected during both system testing and operation at a customer's site. Despite the fact that the distributed system had been in operation for over three years, the method has uncovered system configuration and efficiency problems. Using these discoveries, the system support group has been able to improve product performance and their own product maintenance procedures.
SCOPUS;2002;eModel: Addressing the need for a flexible modeling framework in autonomic computing;;The paper describes a novel, flexible framework, eModel, designed to address the runtime requirements of autonomic computing: on-line workload measurement, analysis, and prediction. The eModel architecture has been developed using platform independent technology (XML and Java) to allow for maximum portability while also allowing for ease-of-integration with existing measurement and system management tools. The eModel toolkit consists of a GUI based model builder tool, a data base deployment tool, a runtime tool, and an analysis tool. In addition to the toolkit, the eModel design provides a runtime architecture which can be deployed directly without using any interaction with the GUI. The architecture is flexible enough to allow for incorporation with models of various complexity, including modeling techniques that require a hierarchical approach to attain reasonable accuracy based upon on-line, measured data. We present examples that illustrate eModel as a capacity planning tool as well as an augmentation to autonomic system management in an effort to highlight the technological gaps that the eModel framework is capable of bridging. © 2002 IEEE.
SCOPUS;2002;Reflective event service middleware for distributed component based applications;;An Event Service is needed for providing event delivery occurring distributed component-based applications such as multimedia communication, electronic commerce, and traffic control system. It supports asynchronous communication between multiple suppliers and consumers required by the distributed applications. However, the event service specification lacks important features for user requirements reflection as follows,, reflective event filtering, user quality of service (UQoS) and component management. Thus, this paper proposes a Reflective Event Service (RES) Middleware framework for distributed component-based applications. The RES middleware based on CORBA Component Model (CCM) and includes the following components,, reflective event filtering component, event monitor component, and UQoS management Component. Especially, this paper concentrates on providing suitable reflective event filtering component for UQoS service. © Springer-Verlag Berlin Heidelberg 2002.
SCOPUS;2001;An architecture for re-engineering of client/server applications;;Netsiel's architectural approach to re-engineering client-server application is presented. The real case of a re-platform project for application of large dimensions composed from 5,000 Java classes and 10,000 Cobol programs.
SCOPUS;2001;QoS-aware middleware for ubiquitous and heterogeneous environments;;Middleware systems have emerged in recent years to support applications in heterogeneous and ubiquitous computing environments. Specifically, future middleware platforms are expected to provide quality of service support, which is required by a new generation of QoS-sensitive applications such as media streaming and e-commerce. This article presents four key aspects of a QoS-aware middleware system: QoS specification to allow description of application behavior and QoS parameters,, QoS translation and compilation to translate specified application behavior into candidate application configurations for different resource conditions,, QoS setup to appropriately select and instantiate a particular configuration,, and finally, QoS adaptation to adapt to runtime resource fluctuations. We also provide a comparison of existing QoS-aware middleware systems in these four aspects.
SCOPUS;2001;Evaluating meta-programming mechanisms for ORB middleware;;Distributed object computing middleware, such as CORBA, COM+, and Java RMI, shields developers from many tedious and error-prone aspects of programming distributed applications. It is hard to evolve distributed applications after they are deployed, however, without adequate middleware support for meta-programming mechanisms, such as smart proxies, interceptors, and pluggable protocols. These mechanisms can help improve the adaptability of distributed applications by allowing their behavior to be modified without changing their existing software designs and implementations significantly. This article examines and compares common meta-programming mechanisms supported by DOC middleware. These mechanisms allow applications to adapt more readily to changes in requirements and runtime environments throughout their lifecycles. Some of these meta-programming mechanisms are relatively new, whereas others have existed for decades. Until recently, however, DOC middleware has not provided all these mechanisms in a single integrated framework, so researchers and developers may not be familiar with the breadth of metaprogramming mechanisms available today. This article provides a systematic evaluation of these mechanisms to help researchers and developers determine which are best suited to their application needs.
SCOPUS;2001;2nd international workshop on living with inconsistency;;In software engineering, there has long been a recognition that inconsistency is a fact of life. Evolving descriptions of software artefacts are frequently inconsistent, and tolerating this inconsistency is important if flexible collaborative working is to be supported. This workshop will focus on reasoning in the presence of inconsistency, for a wide range of software engineering activities, such as building and exploring requirements models, validating specifications, verifying correctness of implementations, monitoring runtime behaviour, and analyzing development processes. A particular interest is on how existing automated approaches such as model checking, theorem proving, logic programming, and model-based reasoning can still be applied in the presence of inconsistency.
SCOPUS;2001;EJVM: an economic Java run-time environment for embedded devices;;As network-enabled embedded devices and Java grow in their popularity, embedded system researchers start seeking ways to make devices Java-enabled. However, it is a challenge to apply Java technology to these devices due to their shortage of resources. In this paper, we propose EJVM (Economic Java Virtual Machine), an economic way to run Java programs on network-enabled and resource-limited embedded devices. Espousing the architecture proposed by distributed JVM, we store all Java codes on the server to reduce the storage needs of the client devices. In addition, we use two novel techniques to reduce the client-side memory footprints: server-side class representation conversion and on-demand bytecode loading. Finally, we maintain client-side caches and provide performance evaluation on different caching policies. We implement EJVM by modifying a freely available JVM implementation, Kaffe. From the experiment results, we show that EJVM can reduce Java heap requirements by about 20-50% and achieve 90% of the original performance.
SCOPUS;2001;Extensibility via a meta-level architecture;;Meta-level architectures are recognized as a means to achieve run-time extensibility, and have been applied as such in existing hypermedia systems. Yet, designing a good meta-level architecture is notoriously hard and remains an art rather than a science. This paper shows how to derive a meta-level architecture for hypermedia navigation, thereby providing a way to control how third-party components interact with the linking engine. This extra level of control allows for a better and safer integration between an extensible system and the third-party components extending it.
SCOPUS;2001;Towards automatically configurable multimedia applications;;We describe and illustrate an approach to the automatic configuration of component-based multimedia applications. The approach is based on the deployment of a run-time application model that mirrors the active application components, enabling changes to the configuration to be applied and evaluated in the model before they are deployed. The model employs a composite component structure, enabling complexity to be concealed except when detail is required. Constraints and QoS specifications are embedded in the model.
SCOPUS;2001;Towards support for ad-hoc multimedia bindings;;Multimedia applications of tomorrow face new challenges. As we move towards ubiquitous computing systems, users will require that the multimedia applications adopt to behave well in this new setting. This will require that developers of such applications are equipped with new development tools and abstractions to help construct these new applications. In this paper we investigate techniques to better support dynamical construction of multimedia bindings. Two alternatives are considered. The first lets the application choose the bindings it requires at run-time, from a pool of existing bindings. The second approach aims at helping the application dynamically construct the required binding. An evaluation of each of the approaches is given.
SCOPUS;2001;Residual requirements and architectural residues;;Monitoring running systems is a useful technique available to requirements engineers, to ensure that systems meet their requirements and in some cases to ensure that they obey the assumptions under which they were created. This report studies relationships between the original requirements and the monitoring infrastructure. Here we postulate that the monitored requirements are in fact just compilations of original requirements, called "residual" requirements. Dynamic architectural models have become important tools for expressing requirements on modern distributed systems. Monitoring residual requirements will be seen to involve "architectural residues," skeletal run-time images of the original logical architecture. An example sales support system is used to illustrate the issues involved, employing modest extensions to the Acme architecture description language to reason about architectural dynamism.
SCOPUS;2001;6th Ada-Europe International Conference on Reliable Software Technologies, Ada Europe 2001;;The proceedings contain 32 papers. The special focus in this conference is on Program Analysis, Distributed Systems, Real-Time Systems, Language and Dependable Systems. The topics include: Building formal requirements models for reliable software,, using Ada in interactive digital television systems,, reliable communication in distributed computer controlled systems,, building robust applications by reusing non-robust legacy software,, new developments in Ada 95 run-time profile definitions and language refinements,, a design pattern for state machines and concurrent activities,, using the spark toolset for showing the absence of run-time errors in safety-critical software,, test suite reduction and fault detecting effectiveness,, object-oriented stable storage based on mirroring,, an Ada kernel for real-time embedded applications,, defining new non-preemptive dispatching and locking policies for Ada,, ship system 2000, a stable architecture under continuous evolution and an application case for ravenscar technology.
SCOPUS;2001;Semantic models for knowledge management;Algorithm design and analysis,, Application software,, Educational institutions,, Information analysis,, Knowledge management,, Portals,, Prototypes,, Runtime,, Semantic Web,, Software prototyping;We explore the use of a semantic model to support a group of strategic business analysts in their daily work. In particular, we present a set of modeling constructs for representing goals, events and actors that are relevant to the work of analysts. We also describe a qualitative goal analysis procedure which makes it possible to reason about a goal model under different assumptions. The paper also reports on an incremental document classification scheme that can be used to classify relevant documents with respect to the concepts constituting the semantic model. © 2001 IEEE.
SCOPUS;2001;Constructing adaptive software in distributed systems;;Adaptive software that can react to changes in the execution environment or user requirements by switching algorithms at runtime is powerful yet difficult to implement, especially in distributed systems. This paper describes a software architecture for constructing such adaptive software and a graceful adaptation protocol that allows adaptations to be made in a coordinated manner across hosts transparently to the application. A realization of the architecture based on Cactus, a system for constructing highly configurable distributed services and protocols, is also presented. The approach is illustrated by outlining examples of adaptive components from a group communication service.
SCOPUS;2001;Requirements-based dynamic metrics in object-oriented systems;;Because early design decisions can have a major long-term impact on the performance of a system, early evaluation of the high-level architecture can be an important risk mitigation technique. This paper proposes a technique for predicting the volume of data that will flow across a network in a distributed system. The prediction is based upon anticipated execution of scenarios and can be applied at an extremely early stage of the design. It is driven by requirements specifications and captures dynamic metrics by defining typical usage patterns in terms of scenarios. Scenarios are then mapped to architectural components, and dataflow across inter-partition links is estimated. The feasibility of the approach is demonstrated through an experiment in which predicted metrics are compared to runtime measurements.
SCOPUS;2001;Software implementation of synchronous programs;;Synchronous languages allow a high level, concurrent, and deterministic description of the behavior of reactive systems. Thus, they can be used advantageously for the programming of embedded control systems. The runtime requirements of synchronous code are light, but several critical properties must be fulfilled. In this paper, we address the problem of the software implementation of synchronous programs. After a brief introduction to reactive systems, this paper formalizes the notion of "execution machine" for synchronous code. Then, a generic architecture for centralized execution machines is introduced. Finally, several effective implementations are presented. © 2001 IEEE.
SCOPUS;2000;Hierarchical error detection in a software implemented fault tolerance (SIFT) environment;;This paper proposes a hierarchical error detection framework for a Software Implemented Fault Tolerance (SIFT) layer of a distributed system. A four-level error detection hierarchy is proposed in the context of Chameleon, a software environment for providing adaptive fault-tolerance in an environment of commercial off-the-shelf (COTS) system components and software. The design and implementation of a software-based distributed signature monitoring scheme, which is central to the proposed four-level hierarchy, is described. Both intralevel and interlevel optimizations that minimize the overhead of detection and are capable of adapting to runtime requirements are proposed. The paper presents results from a prototype implementation of two levels of the error detection hierarchy and results of a detailed simulation of the overall environment. The results indicate a substantial increase in availability due to the detection framework and help in understanding the trade-offs between overhead and coverage for different combinations of techniques.
SCOPUS;2000;Field analysis: Getting useful and low-cost interprocedural information;;We present a new limited form of interprocedural analysis called field analysis that can be used by a compiler to reduce the costs of modern language features such as object-oriented programming, automatic memory management, and run-time checks required for type safety. Unlike many previous interprocedural analyses, our analysis is cheap, and does not require access to the entire program. Field analysis exploits the declared access restrictions placed on fields in a modular language (e.g. field access modifiers in Java) in order to determine useful properties of fields of an object. We describe our implementation of field analysis in the Swift optimizing compiler for Java, as well a set of optimizations that exploit the results of field analysis. These optimizations include removal of run-time tests, compile-time resolution of method calls, object inlining, removal of unncessary synchronization, and stack allocation. Our results demonstrate that field analysis is efficient and effective. Speedups average 7% on a wide range of applications, with some times reduced by up to 27%. Compile time overhead of field analysis is about 10%.
SCOPUS;2000;ABCD: Eliminating array bounds checks on demand;;To guarantee typesafe execution, Java and other strongly typed languages require bounds checking of array accesses. Because array-bounds checks may raise exceptions, they block code motion of instructions with side effects, thus preventing many useful code optimizations, such as partial redundancy elimination or instruction scheduling of memory operations. Furthermore, because it is not expressible at bytecode level, the elimination of bounds checks can only be performed at run time, after the bytecode program is loaded. Using existing powerful bounds-check optimizers at run time is not feasible, however, because they are too heavyweight for the dynamic compilation setting. ABCD is a light-weight algorithm for elimination of Array Bounds Checks on Demand. Its design emphasizes simplicity and efficiency. In essence, ABCD works by adding a few edges to the SSA value graph and performing a simple traversal of the graph. Despite its simplicity, ABCD is surprisingly powerful. On our benchmarks, ABCD removes on average 45% of dynamic bound check instructions, sometimes achieving near-ideal optimization. The efficiency of ABCD stems from two factors. First, ABCD works on a sparse representation. As a result, it requires on average fewer than 10 simple analysis steps per bounds check. Second, ABCD is demand-driven. It can be applied to a set of frequently executed (hot) bounds checks, which makes it suitable for the dynamic-compilation setting, in which compile-time cost is constrained but hot statements are known.
SCOPUS;2000;Unified compiler framework for control and data speculation;;Control speculation refers to the execution of instructions before it has been determined that they would be executed in the normal flow of execution. Data speculation refers to the execution of instructions with potentially incorrect operand values, and a typical example is to execute a load before its preceding aliasing stores. Both types of speculation are effective techniques to enrich instruction level parallelism, but the research work for these two types of speculation have remained largely independent so far and the required compiler support has not been well studied. This paper proposes a unified compiler framework to exploit both control and data speculation and provides an in-depth discussion of various compilation issues. The adopted recovery mechanism guarantees the original program semantics including exceptions fully recoverable from a mis-speculation. Cascaded speculation and predication are also addressed. We demonstrate the effectiveness of the compiler optimization techniques for control and data speculation in terms of run-time performance improvements and code size increases through experimental results.
SCOPUS;2000;An initial comparison of National Combustor Code simulations using various chemistry modules with experimental gas turbine Combustor Data;;The National Combustion Code (NCC) is a state of the art computational fluid dynamics code specifically designed to simulate gas turbine combustors. The NCC was given the geometry and boundary conditions from an Allied Signal gas turbine combustor. The Allied Signal combustor was an annular, mixed flow, swirler type configuration that was fueled by natural gas. The NCC was run to convergence using a variety of combustion chemistry modules that used. The chemistry models that were used were the Magnussen eddy dissipation model and the intrinsic low.dimensional manifolds (ILDM) model. The computational results were compared to experimental results for the Allied Signal combustor. For each chemistry model used, NCC computer runtime requirements and ease of use issues are discussed.
SCOPUS;2000;Changing class behaviors at run-time in MRP systems;;This paper presents an architecture that can be used to develop and maintain class behaviors in object-oriented material requirements planning (MRP) systems at run-time. The architecture provides this capability through a user-interface, and does not require knowledge of any programming language. The architecture is based on the concept of software reuse, it utilizes a library of fine-grained pre-compiled objects to develop and maintain class behaviors. The architecture is a dynamic-object application builder implemented on top of an object-oriented run-time environment.
SCOPUS;1999;Dynamic precision management for loop computations on reconfigurable architectures;;Reconfigurable architectures promise significant performance benefits by customizing the configurations to suit the computations. Variable precision for computations is one important method of customization for which reconfigurable architectures are well suited. The precision of the operations can be modified dynamically at run-time to match the precision of the operands. Though the advantages of reconfigurable architectures for dynamic precision have been discussed before, we are not aware of any work which analyzes the qualitative and quantitative benefits which can be achieved. This paper develops a formal methodology for dynamic precision management. We show how the precision requirements can be analyzed for typical computations in loops by computing the precision variation curve. We develop algorithms to generate optimal schedules of configurations using the precision variation curves. Using our approach, we demonstrate 25%-37% improvement in the total execution time of an example loop computation on the XC6200 device.
SCOPUS;1999;A virtual hardware handler for RTR systems;;The design of a Virtual Hardware Handler for run-time reconfiguration is presented. A windows-based system that works with the VCC Hotworks board has been implemented and results are presented.
SCOPUS;1999;Integrative approach to requirements modeling;;An approach to requirements modeling based on synergy between multiple views is presented. It integrates natural-language descriptions, visual executable models with run-time behavior monitoring and codegeneration facilities, and interactive graphical user-interface prototypes, through the use of dynamic scenarios. A conceptual design of an integrative requirements modeling system (currently implemented as research prototype) is described. Partial modeling techniques are used to preclude excessive redundancy and permit flexible balancing of emphasis between views. The general objective is to allow industrial teams to flexibly combine the advantages of informal and formal methods, to smoothly shift balance between them with incremental investment in tools and training, and to preserve continuity between old and new projects.
SCOPUS;1998;Use case maps as architectural entities for complex systems;Design,, Requirements,, Scenarios,, Software architecture,, System behavior,, Use case maps,, Use cases;This paper presents a novel, scenario-based notation called Use Case Maps (UCMs) for describing, in a high-level way, how the organizational structure of a complex system and the emergent behavior of the system are intertwined. The notation is not a behavior specification technique in the ordinary sense, but a notation for helping a person to visualize, think about, and explain the big picture. UCMs are presented as "architectural entities" that help a person stand back from the details during all phases of system development. The notation has been thoroughly exercised on systems of industrial scale and complexity and the distilled essence of what has been found to work in practice is summarized in this paper. Examples are presented that confront difficult complex-system issues directly: decentralized control, concurrency, failure, diversity, elusiveness and fluidity of runtime views of software, selfmodification of system makeup, difficulty of seeing large-scale units of emergent behavior cutting across systems as coherent entities (and of seeing how such entities arise from the collective efforts of components), and large scale. ©1998 IEEE.
SCOPUS;1998;A sequential detailed router for huge grid graphs;;Sequential routing algorithms using maze-running are very suitable for general over-the-cell-routing but suffer often from the high memory or runtime requirements of the underlying path search routine. A new algorithm for this subproblem is presented that computes shortest paths in a rectangular grid with respect to euclidean distance. It achieves performance and memory requirements similar to fast line-search algorithms while still being optimal. An additional application for the computation of minimal rip-up sets is presented. Computational results are shown for a detailed router based on these algorithms that is used for the design of high performance CMOS processors at IBM. © 1998 IEEE.
SCOPUS;1998;Proceedings of the 1998 International Conference on Software Engineering;;The proceedings contains 64 papers from the 1998 IEEE 20th International Conference on Software Engineering. Topics discussed include: experiences with software process improvement,, formal modeling,, reverse engineering,, object orientation,, Internet,, mobile codes,, security of data,, object-oriented technology,, large scale and complex system development,, computer supported cooperative work,, project estimation and simulation,, project and workflow management,, data analysis,, cost estimation,, and requirements engineering.
SCOPUS;1998;Proceedings of the 1998 20th International Conference on Software Engineering;;The proceedings contains 64 papers from the 20th International Conference on Software Engineering. Topics discussed include: agile software processes,, software process modeling,, software process improvement activities,, virtual reality systems,, object request broker,, goal-driven requirements engineering,, conceptual module querying,, reuse-driven interprocedural slicing,, three dimensional software modeling,, the Internet,, architecture-based runtime software evolution,, regression test selection techniques,, form-based visual programs,, integrating architecture description languages,, automated validation systems,, object-oriented software,, distributed systems,, non-intrusive object introspection,, and object oriented reuse.
SCOPUS;1996;RKB/PL: the persistence extension of C++;;RKB/PL (requirement - engineering knowledge base/PL) is a persistence extension of the C++ programming language, while adhering to the style of C++. In order to support persistence in RKB/PL, constraint declaration is introduced in the C++ class declaration to facilitate user's monitoring of object state,, the notion of cluster is introduced to capture the 'set - of - objects' conception of class,, set iterator, cluster iterator, cluster closure iterator are also introduced to support object query. RKB/PL run -time system facilitating these new language mechanisms is composed of a set of build - in class hierarchies, type information base interface functions, system service functions and also a set of system state tables. RKB/PL has been used to implement the RKB (requirement - engineering knowledge base) of the 'software requirements assistant FRA' system.
SCOPUS;1993;Run-time requirement tracing;;This work extends previous work on run-time tracing of the design activity in order to handle dependencies between design data in a uniform way, including requirements and deliverables. Each requirement is stored in a card,, several cards are organized into linear hierarchical stacks. The system offers: requirement tracing, impact analysis, support for requirement analysis, automatic retracing. The contributions are: 1) the run-time property, which assures a detailed representation of dependencies, 2) the linear organization of the stacks of cards, and 3) the uniformity with which all design data, from requirements to deliverables, are treated. A prototype has been used in a real design.
SCOPUS;1975;PARSING ALGORITHMS ANALYSIS.;;Two bottom-up shift-and-reduce parsing algorithms, simple precedence and LR(k) (k equals 1,2), are analyzed. These two are coded into MIXAL programs. (MIXAL is the assembly language for an imaginary computer MIX). Memory and runtime requirements are measured. Runtime estimation is made in two different ways, and the two algorithms are compared.
Web of Science;2015;A programming-level approach for elasticizing parallel scientific applications;;Elasticity is considered one of the fundamental properties of cloud computing. Several mechanisms to provide the feature are offered by public cloud providers and in some academic works. We argue these solutions are inefficient in providing elasticity for scientific applications, since they cannot consider the internal structure and behavior of applications. In this paper we present an approach for exploring the elasticity in scientific applications, in which the elasticity control is embedded in application source code and constructed using elasticity primitives. This approach enables the application itself to request or to release its own resources, taking into account the execution flow and runtime requirements. To support the construction of elastic applications using the presented approach, we developed the Cloudine framework. Cloudine provides all components necessary to construct and execute elastic scientific applications. The Cloudine effectiveness is demonstrated in the experiments where the platform is successfully used to include new features to existing applications, to extend functionalities of other elasticity frameworks and to add elasticity support to parallel programming libraries. (c) 2015 Elsevier Inc. All rights reserved.
Web of Science;2015;Rationalism with a dose of empiricism: combining goal reasoning and case-based reasoning for self-adaptive software systems;;Requirements-driven approaches provide an effective mechanism for self-adaptive systems by reasoning over their runtime requirements models to make adaptation decisions. However, such approaches usually assume that the relations among alternative system configurations, environmental parameters and requirements are clearly understood, which is often not true. Moreover, they do not consider the influence of the current configuration of an executing system on adaptation decisions. In this paper, we propose an improved requirements-driven self-adaptation approach that combines goal reasoning and case-based reasoning. In the approach, past experiences of successful adaptations are retained as adaptation cases, which are described by not only requirements violations and contexts, but also currently deployed system configurations. The approach does not depend on a set of original adaptation cases, but employs goal reasoning to provide adaptation solutions when no similar cases are available. Case-based reasoning is used to provide more precise adaptation decisions that better reflect the complex relations among requirements violations, contexts, and current system configurations by utilizing past experiences. To prevent case-based reasoning from getting trapped in suboptimal adaptation solutions, an additional case mutation mechanism is introduced to mutate existing adaptation solutions when necessary. We conduct an experimental study with an online shopping benchmark to evaluate the effectiveness of our approach. The results show that our approach outperforms both a requirements-driven approach and a case-based approach in terms of satisfaction level of quality constraints. The results also confirm the effectiveness of case mutation for producing better adaptation solutions. In addition, we empirically investigate the evolution process of adaptation solutions. The evolution analysis reveals some general evolution trends of adaptation solutions such as different evolution phases.
Web of Science;2015;Modeling and verification of Functional and Non-Functional Requirements of ambient Self-Adaptive Systems;;Self-Adaptive Systems modify their behavior at run-time in response to changing environmental conditions. For these systems, Non-Functional Requirements play an important role, and one has to identify as early as possible the requirements that are adaptable. We propose an integrated approach for modeling and verifying the requirements of Self-Adaptive Systems using Model Driven Engineering techniques. For this, we use RELAx, which is a Requirements Engineering language which introduces flexibility in Non-Functional Requirements. We then use the concepts of Goal-Oriented Requirements Engineering for eliciting and modeling the requirements of Self-Adaptive Systems. For properties verification, we use OMEGA2/IFx profile and toolset. We illustrate our proposed approach by applying it on an academic case study. (C) 2015 Elsevier Inc. All rights reserved.
Web of Science;2015;Comparative assessment of methods for the computational inference of transcript isoform abundance from RNA-seq data;;Background: Understanding the regulation of gene expression, including transcription start site usage, alternative splicing, and polyadenylation, requires accurate quantification of expression levels down to the level of individual transcript isoforms. To comparatively evaluate the accuracy of the many methods that have been proposed for estimating transcript isoform abundance from RNA sequencing data, we have used both synthetic data as well as an independent experimental method for quantifying the abundance of transcript ends at the genome-wide level.Results: We found that many tools have good accuracy and yield better estimates of gene-level expression compared to commonly used count-based approaches, but they vary widely in memory and runtime requirements. Nucleotide composition and intron/exon structure have comparatively little influence on the accuracy of expression estimates, which correlates most strongly with transcript/gene expression levels. To facilitate the reproduction and further extension of our study, we provide datasets, source code, and an online analysis tool on a companion website, where developers can upload expression estimates obtained with their own tool to compare them to those inferred by the methods assessed here.Conclusions: As many methods for quantifying isoform abundance with comparable accuracy are available, a user's choice will likely be determined by factors such as the memory and runtime requirements, as well as the availability of methods for downstream analyses. Sequencing-based methods to quantify the abundance of specific transcript regions could complement validation schemes based on synthetic data and quantitative PCR in future or ongoing assessments of RNA-seq analysis methods.
Web of Science;2015;Instruction-Cache Locking for Improving Embedded Systems Performance;;Cache memories in embedded systems play an important role in reducing the execution time of applications. Various kinds of extensions have been added to cache hardware to enable software involvement in replacement decisions, improving the runtime over a purely hardware-managed cache. Novel embedded systems, such as Intel's XScale and ARM Cortex processors, facilitate locking one or more lines in cache,, this feature is called cache locking. We present a method in for instruction-cache locking that is able to reduce the average-case runtime of a program. We demonstrate that the optimal solution for instruction cache locking can be obtained in polynomial time. However, a fundamental lack of correlation between cache hardware and software program points renders such optimal solutions impractical.Instead, we propose two practical heuristics-based approaches to achieve cache locking. First, we present a static mechanism for locking the cache, in which the locked contents of the cache are kept fixed over the execution of the program. Next, we present a dynamic mechanism that accounts for changing program requirements at runtime. We devise a cost-benefit model to discover the memory addresses that should be locked in the cache. We implement our scheme inside a binary rewriter, widening the applicability of our scheme to binaries compiled using any compiler.Results obtained on a suite of MiBench benchmarks show that our static mechanism results in 20% improvement in the instruction-cache miss rate on average and up to 18% improvement in the execution time on average for applications having instruction accesses as a bottleneck, compared to no cache locking. The dynamic mechanism improves the cache miss rate by 35% on average and execution time by 32% on instruction-cache-constrained applications.
Web of Science;2015;Designing an adaptive computer-aided ambulance dispatch system with Zanshin: an experience report;;We have been witnessing growing interest in systems that can adapt their behavior to deal with deviations between their performance and their requirements at run-time. Such adaptive systems usually need to support some form of a feedback loop that monitors the system's output for problems and carries out adaptation actions when necessary. Being an important feature, adaptivity needs to be considered in early stages of development. Therefore, adopting a requirements engineering perspective, we have proposed an approach and a framework (both called Zanshin) for the engineering of adaptive systems based on a feedback loop architecture. As part of our framework's evaluation, we have applied the Zanshin approach to the design of an adaptive computer-aided ambulance dispatch system, whose requirements were based on a well-known case study from the literature. In this paper, we report on the application of Zanshin for the design of an adaptive computer-aided ambulance dispatch system, presenting elements of the design, as well as the results from simulations of run-time scenarios. Copyright (c) 2013 John Wiley & Sons, Ltd.
Web of Science;2015;Iterative solutions to the steady-state density matrix for optomechanical systems;;We present a sparse matrix permutation from graph theory that gives stable incomplete lower-upper preconditioners necessary for iterative solutions to the steady-state density matrix for quantum optomechanical systems. This reordering is efficient, adding little overhead to the computation, and results in a marked reduction in both memory and runtime requirements compared to other solution methods, with performance gains increasing with system size. Either of these benchmarks can be tuned via the preconditioner accuracy and solution tolerance. This reordering optimizes the condition number of the approximate inverse and is the only method found to be stable at large Hilbert space dimensions. This allows for steady-state solutions to otherwise intractable quantum optomechanical systems.
Web of Science;2015;On the Efficiency of Nature-Inspired Algorithms for Generation of Fault-Tolerant graphs;;In this study several algorithms for the generation of inexpensive and fault-tolerant graphs are evaluated with respect to the quality of the found graphs and to the runtime requirements. A special focus lies on the properties of the algorithm that basically is a simulation of the foraging of the slime mold Physarum polycephalum, since in many other works the deployment of this algorithm does not go beyond the conclusion, that the algorithm is capable to generate a graph, while quality of the graph and runtime requirements of the algorithm are not reported. Our results show that the slime mold algorithm has some interesting features, however it is not the best means to construct highly efficient graphs out of large sets of nodes.
Web of Science;2015;An Open-Source Proactive Security Infrastructure For Business Process Management;;Business Process Management Systems (BPMS) have emerged in the IT arena as cornerstone in the automation and orchestration of complex services for organizations. These systems manage critical information that is crucial for the organizations. The potential cost and consequences of security threats could produce information loss for the reputation of organizations. Therefore, the early response regarding to the non-compliance of security requirement is a real necessity overall during the business process execution. Currently, an active response requires a human intervention with high know-how and expertise in both business process management and security. In this paper, we propose an initial work which presents an open-source proactive infrastructure for the automatic continuous monitoring and checking compliance of security requirements at runtime of business processes.
Web of Science;2015;Towards an Ontology-Based Approach to Safety Management in Cooperative Intelligent Transportation Systems;;The expected increase in transports of people and goods across Europe will aggravate the problems related to traffic congestion, accidents and pollution. As new road infrastructure alone would not solve such problems, Intelligent Transportation Systems (ITS) has been considered as new initiatives. Due to the complexity of behaviors, novel methods and tools for the requirements engineering, correct-by-construction design, dependability, product variability and lifecycle management become also necessary. This chapter presents an ontology-based approach to safety management in Cooperative ITS (C-ITS), primarily in an automotive context. This approach is supposed to lay the way for all aspects of ITS safety management, from simulation and design, over run-time risk assessment and diagnostics. It provides the support for ontology driven ITS development and its formal information model. Results of approach validation in CarMaker are also given in this Chapter. The approach is a result of research activities made in the framework of Swedish research initiative, referred to as SARMITS (Systematic Approach to Risk Management in ITS Context).
